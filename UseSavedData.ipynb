{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import dropbox\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "import math\n",
    "import collections\n",
    "from PIL import Image\n",
    "from dropbox import DropboxOAuth2FlowNoRedirect\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pypyodbc\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import BatchNormalization, concatenate, Input, Dropout, Flatten, Dense\n",
    "from keras.utils.np_utils import to_categorical  #?\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(open('train_tar_2.npy', 'rb'))\n",
    "y_test = np.load(open('valid_tar_2.npy', 'rb'))\n",
    "X_test = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "test_codes = np.load(open('valid_codes.npy', 'rb'))\n",
    "X_train = np.load(open('main_pic1.npy', 'rb'))\n",
    "train_codes = np.load(open('main_codes.npy', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = Input(shape=(2048,))\n",
    "\n",
    "model = Dense(1024, activation='relu')(first)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(512, activation='relu')(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(256, activation='relu')(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(10, activation='softmax')(model)\n",
    "\n",
    "model_ = Model(inputs=[first], outputs=model)\n",
    "\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model_.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41259 samples, validate on 4594 samples\n",
      "Epoch 1/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 1.8915 - acc: 0.3920 - val_loss: 1.7751 - val_acc: 0.4279\n",
      "Epoch 2/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.7518 - acc: 0.4280 - val_loss: 1.7675 - val_acc: 0.4279\n",
      "Epoch 3/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.7266 - acc: 0.4325 - val_loss: 1.7570 - val_acc: 0.4279\n",
      "Epoch 4/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.7116 - acc: 0.4336 - val_loss: 1.7652 - val_acc: 0.4279\n",
      "Epoch 5/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.7057 - acc: 0.4339 - val_loss: 1.7624 - val_acc: 0.4279\n",
      "Epoch 6/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.6955 - acc: 0.4343 - val_loss: 1.7605 - val_acc: 0.4279\n",
      "Epoch 7/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6861 - acc: 0.4343 - val_loss: 1.7670 - val_acc: 0.4279\n",
      "Epoch 8/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.6814 - acc: 0.4344 - val_loss: 1.7697 - val_acc: 0.4279\n",
      "Epoch 9/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6752 - acc: 0.4341 - val_loss: 1.7887 - val_acc: 0.4279\n",
      "Epoch 10/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6699 - acc: 0.4344 - val_loss: 1.8252 - val_acc: 0.4279\n",
      "Epoch 11/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6647 - acc: 0.4344 - val_loss: 1.8255 - val_acc: 0.4279\n",
      "Epoch 12/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6571 - acc: 0.4344 - val_loss: 1.8338 - val_acc: 0.4279\n",
      "Epoch 13/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6527 - acc: 0.4343 - val_loss: 1.8051 - val_acc: 0.4279\n",
      "Epoch 14/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6469 - acc: 0.4342 - val_loss: 1.8420 - val_acc: 0.4279\n",
      "Epoch 15/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6438 - acc: 0.4346 - val_loss: 1.8995 - val_acc: 0.4279\n",
      "Epoch 16/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.6399 - acc: 0.4340 - val_loss: 1.8991 - val_acc: 0.4279\n",
      "Epoch 17/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.6341 - acc: 0.4346 - val_loss: 1.9091 - val_acc: 0.4279\n",
      "Epoch 18/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6291 - acc: 0.4345 - val_loss: 1.9574 - val_acc: 0.4279\n",
      "Epoch 19/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.6259 - acc: 0.4350 - val_loss: 1.9834 - val_acc: 0.4279\n",
      "Epoch 20/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6202 - acc: 0.4351 - val_loss: 2.0157 - val_acc: 0.4279\n",
      "Epoch 21/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6152 - acc: 0.4362 - val_loss: 2.0174 - val_acc: 0.4279\n",
      "Epoch 22/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6098 - acc: 0.4372 - val_loss: 2.1247 - val_acc: 0.4279\n",
      "Epoch 23/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.6034 - acc: 0.4383 - val_loss: 2.0666 - val_acc: 0.4279\n",
      "Epoch 24/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5988 - acc: 0.4402 - val_loss: 2.1208 - val_acc: 0.4279\n",
      "Epoch 25/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5977 - acc: 0.4400 - val_loss: 2.1796 - val_acc: 0.4279\n",
      "Epoch 26/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5907 - acc: 0.4411 - val_loss: 2.1573 - val_acc: 0.4279\n",
      "Epoch 27/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5857 - acc: 0.4418 - val_loss: 2.2120 - val_acc: 0.4279\n",
      "Epoch 28/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5800 - acc: 0.4433 - val_loss: 2.2790 - val_acc: 0.4279\n",
      "Epoch 29/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5749 - acc: 0.4445 - val_loss: 2.2278 - val_acc: 0.4279\n",
      "Epoch 30/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5693 - acc: 0.4461 - val_loss: 2.3601 - val_acc: 0.4279\n",
      "Epoch 31/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5642 - acc: 0.4461 - val_loss: 2.3488 - val_acc: 0.4279\n",
      "Epoch 32/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5580 - acc: 0.4481 - val_loss: 2.3031 - val_acc: 0.4279\n",
      "Epoch 33/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5557 - acc: 0.4483 - val_loss: 2.4937 - val_acc: 0.4279\n",
      "Epoch 34/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5476 - acc: 0.4500 - val_loss: 2.6473 - val_acc: 0.4279\n",
      "Epoch 35/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5442 - acc: 0.4501 - val_loss: 2.8098 - val_acc: 0.4279\n",
      "Epoch 36/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5394 - acc: 0.4507 - val_loss: 2.8329 - val_acc: 0.4277\n",
      "Epoch 37/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5342 - acc: 0.4523 - val_loss: 2.9311 - val_acc: 0.4277\n",
      "Epoch 38/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5288 - acc: 0.4534 - val_loss: 3.1896 - val_acc: 0.4279\n",
      "Epoch 39/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5209 - acc: 0.4553 - val_loss: 3.2487 - val_acc: 0.4279\n",
      "Epoch 40/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5171 - acc: 0.4555 - val_loss: 3.3403 - val_acc: 0.4273\n",
      "Epoch 41/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.5128 - acc: 0.4563 - val_loss: 3.6318 - val_acc: 0.4275\n",
      "Epoch 42/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5079 - acc: 0.4575 - val_loss: 3.5783 - val_acc: 0.4275\n",
      "Epoch 43/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.5024 - acc: 0.4591 - val_loss: 3.8652 - val_acc: 0.4279\n",
      "Epoch 44/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.4997 - acc: 0.4596 - val_loss: 3.8156 - val_acc: 0.4275\n",
      "Epoch 45/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4920 - acc: 0.4613 - val_loss: 3.7790 - val_acc: 0.4271\n",
      "Epoch 46/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.4831 - acc: 0.4608 - val_loss: 4.1364 - val_acc: 0.4275\n",
      "Epoch 47/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.4868 - acc: 0.4612 - val_loss: 4.4703 - val_acc: 0.4279\n",
      "Epoch 48/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4800 - acc: 0.4640 - val_loss: 4.1231 - val_acc: 0.4266\n",
      "Epoch 49/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.4703 - acc: 0.4659 - val_loss: 4.3527 - val_acc: 0.4277\n",
      "Epoch 50/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4650 - acc: 0.4657 - val_loss: 4.1421 - val_acc: 0.4271\n",
      "Epoch 51/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.4648 - acc: 0.4673 - val_loss: 4.3148 - val_acc: 0.4275\n",
      "Epoch 52/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4590 - acc: 0.4681 - val_loss: 4.6891 - val_acc: 0.4269\n",
      "Epoch 53/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4559 - acc: 0.4673 - val_loss: 5.1777 - val_acc: 0.4275\n",
      "Epoch 54/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.4498 - acc: 0.4664 - val_loss: 5.2773 - val_acc: 0.4277\n",
      "Epoch 55/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.4454 - acc: 0.4692 - val_loss: 5.0315 - val_acc: 0.4271\n",
      "Epoch 56/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.4404 - acc: 0.4708 - val_loss: 5.2432 - val_acc: 0.4266\n",
      "Epoch 57/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.4377 - acc: 0.4729 - val_loss: 5.8407 - val_acc: 0.4275\n",
      "Epoch 58/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4357 - acc: 0.4735 - val_loss: 5.5336 - val_acc: 0.4273\n",
      "Epoch 59/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4288 - acc: 0.4737 - val_loss: 5.7061 - val_acc: 0.4271\n",
      "Epoch 60/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4256 - acc: 0.4738 - val_loss: 5.3801 - val_acc: 0.4260\n",
      "Epoch 61/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4212 - acc: 0.4769 - val_loss: 5.3768 - val_acc: 0.4262\n",
      "Epoch 62/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4170 - acc: 0.4764 - val_loss: 5.6268 - val_acc: 0.4271\n",
      "Epoch 63/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.4104 - acc: 0.4776 - val_loss: 5.8777 - val_acc: 0.4260\n",
      "Epoch 64/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4074 - acc: 0.4787 - val_loss: 5.9546 - val_acc: 0.4260\n",
      "Epoch 65/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4025 - acc: 0.4777 - val_loss: 6.1265 - val_acc: 0.4271\n",
      "Epoch 66/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3986 - acc: 0.4796 - val_loss: 5.9915 - val_acc: 0.4236\n",
      "Epoch 67/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.4001 - acc: 0.4795 - val_loss: 5.8834 - val_acc: 0.4240\n",
      "Epoch 68/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3941 - acc: 0.4806 - val_loss: 6.3581 - val_acc: 0.4269\n",
      "Epoch 69/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.3929 - acc: 0.4808 - val_loss: 6.0615 - val_acc: 0.4247\n",
      "Epoch 70/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3843 - acc: 0.4825 - val_loss: 6.2122 - val_acc: 0.4262\n",
      "Epoch 71/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3795 - acc: 0.4858 - val_loss: 6.1996 - val_acc: 0.4229\n",
      "Epoch 72/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3802 - acc: 0.4848 - val_loss: 6.4521 - val_acc: 0.4242\n",
      "Epoch 73/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3729 - acc: 0.4857 - val_loss: 6.4835 - val_acc: 0.4240\n",
      "Epoch 74/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3695 - acc: 0.4874 - val_loss: 6.9466 - val_acc: 0.4256\n",
      "Epoch 75/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3655 - acc: 0.4866 - val_loss: 6.9505 - val_acc: 0.4262\n",
      "Epoch 76/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3653 - acc: 0.4873 - val_loss: 6.6983 - val_acc: 0.4229\n",
      "Epoch 77/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3570 - acc: 0.4881 - val_loss: 6.9095 - val_acc: 0.4253\n",
      "Epoch 78/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3530 - acc: 0.4893 - val_loss: 7.0317 - val_acc: 0.4240\n",
      "Epoch 79/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3496 - acc: 0.4907 - val_loss: 7.0723 - val_acc: 0.4253\n",
      "Epoch 80/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3484 - acc: 0.4894 - val_loss: 7.0878 - val_acc: 0.4256\n",
      "Epoch 81/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.3415 - acc: 0.4901 - val_loss: 7.3246 - val_acc: 0.4262\n",
      "Epoch 82/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3416 - acc: 0.4924 - val_loss: 7.5495 - val_acc: 0.4266\n",
      "Epoch 83/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.3346 - acc: 0.4950 - val_loss: 7.2776 - val_acc: 0.4240\n",
      "Epoch 84/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3344 - acc: 0.4969 - val_loss: 7.5274 - val_acc: 0.4271\n",
      "Epoch 85/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3301 - acc: 0.4968 - val_loss: 7.4495 - val_acc: 0.4260\n",
      "Epoch 86/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3318 - acc: 0.4953 - val_loss: 7.5689 - val_acc: 0.4249\n",
      "Epoch 87/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.3259 - acc: 0.4960 - val_loss: 7.5631 - val_acc: 0.4251\n",
      "Epoch 88/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.3206 - acc: 0.4969 - val_loss: 7.4599 - val_acc: 0.4251\n",
      "Epoch 89/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.3133 - acc: 0.5010 - val_loss: 7.6908 - val_acc: 0.4247\n",
      "Epoch 90/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3153 - acc: 0.4985 - val_loss: 7.9251 - val_acc: 0.4262\n",
      "Epoch 91/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3180 - acc: 0.4996 - val_loss: 7.6915 - val_acc: 0.4251\n",
      "Epoch 92/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3145 - acc: 0.4993 - val_loss: 8.0492 - val_acc: 0.4262\n",
      "Epoch 93/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3025 - acc: 0.5024 - val_loss: 8.0876 - val_acc: 0.4256\n",
      "Epoch 94/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.3031 - acc: 0.5032 - val_loss: 8.2043 - val_acc: 0.4262\n",
      "Epoch 95/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2959 - acc: 0.5017 - val_loss: 8.0066 - val_acc: 0.4219\n",
      "Epoch 96/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2983 - acc: 0.5021 - val_loss: 7.8921 - val_acc: 0.4227\n",
      "Epoch 97/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2916 - acc: 0.5045 - val_loss: 8.0894 - val_acc: 0.4216\n",
      "Epoch 98/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2878 - acc: 0.5079 - val_loss: 8.2462 - val_acc: 0.4256\n",
      "Epoch 99/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2878 - acc: 0.5073 - val_loss: 8.0798 - val_acc: 0.4251\n",
      "Epoch 100/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2850 - acc: 0.5042 - val_loss: 8.1025 - val_acc: 0.4256\n",
      "Epoch 101/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2797 - acc: 0.5077 - val_loss: 8.1519 - val_acc: 0.4245\n",
      "Epoch 102/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2802 - acc: 0.5057 - val_loss: 8.1461 - val_acc: 0.4229\n",
      "Epoch 103/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.2765 - acc: 0.5068 - val_loss: 8.3486 - val_acc: 0.4253\n",
      "Epoch 104/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.2777 - acc: 0.5055 - val_loss: 8.2092 - val_acc: 0.4240\n",
      "Epoch 105/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.2684 - acc: 0.5104 - val_loss: 8.1932 - val_acc: 0.4253\n",
      "Epoch 106/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2759 - acc: 0.5079 - val_loss: 8.2183 - val_acc: 0.4238\n",
      "Epoch 107/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2682 - acc: 0.5095 - val_loss: 8.1967 - val_acc: 0.4258\n",
      "Epoch 108/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2619 - acc: 0.5108 - val_loss: 8.4453 - val_acc: 0.4256\n",
      "Epoch 109/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2612 - acc: 0.5111 - val_loss: 8.1883 - val_acc: 0.4247\n",
      "Epoch 110/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.2598 - acc: 0.5111 - val_loss: 7.9635 - val_acc: 0.4229\n",
      "Epoch 111/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2565 - acc: 0.5153 - val_loss: 8.2086 - val_acc: 0.4245\n",
      "Epoch 112/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2547 - acc: 0.5122 - val_loss: 8.1943 - val_acc: 0.4247\n",
      "Epoch 113/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2506 - acc: 0.5148 - val_loss: 8.1618 - val_acc: 0.4223\n",
      "Epoch 114/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2471 - acc: 0.5163 - val_loss: 8.1849 - val_acc: 0.4208\n",
      "Epoch 115/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.2449 - acc: 0.5181 - val_loss: 8.1453 - val_acc: 0.4221\n",
      "Epoch 116/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.2504 - acc: 0.5156 - val_loss: 8.2243 - val_acc: 0.4229\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2376 - acc: 0.5185 - val_loss: 8.1597 - val_acc: 0.4229\n",
      "Epoch 118/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2386 - acc: 0.5187 - val_loss: 8.2240 - val_acc: 0.4240\n",
      "Epoch 119/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2326 - acc: 0.5222 - val_loss: 8.1198 - val_acc: 0.4216\n",
      "Epoch 120/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2262 - acc: 0.5198 - val_loss: 8.3200 - val_acc: 0.4212\n",
      "Epoch 121/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2283 - acc: 0.5210 - val_loss: 8.3163 - val_acc: 0.4232\n",
      "Epoch 122/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2349 - acc: 0.5199 - val_loss: 8.3835 - val_acc: 0.4242\n",
      "Epoch 123/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2184 - acc: 0.5220 - val_loss: 8.5605 - val_acc: 0.4238\n",
      "Epoch 124/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2253 - acc: 0.5188 - val_loss: 8.6380 - val_acc: 0.4251\n",
      "Epoch 125/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2247 - acc: 0.5220 - val_loss: 8.5014 - val_acc: 0.4238\n",
      "Epoch 126/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.2180 - acc: 0.5248 - val_loss: 8.4579 - val_acc: 0.4225\n",
      "Epoch 127/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.2118 - acc: 0.5248 - val_loss: 8.5815 - val_acc: 0.4245\n",
      "Epoch 128/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2176 - acc: 0.5247 - val_loss: 8.6120 - val_acc: 0.4251\n",
      "Epoch 129/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2163 - acc: 0.5243 - val_loss: 8.5729 - val_acc: 0.4249\n",
      "Epoch 130/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2138 - acc: 0.5225 - val_loss: 8.5138 - val_acc: 0.4236\n",
      "Epoch 131/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2056 - acc: 0.5268 - val_loss: 8.5331 - val_acc: 0.4238\n",
      "Epoch 132/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2014 - acc: 0.5292 - val_loss: 8.6237 - val_acc: 0.4245\n",
      "Epoch 133/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2046 - acc: 0.5278 - val_loss: 8.5735 - val_acc: 0.4242\n",
      "Epoch 134/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.2026 - acc: 0.5284 - val_loss: 8.7163 - val_acc: 0.4245\n",
      "Epoch 135/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1950 - acc: 0.5305 - val_loss: 8.7081 - val_acc: 0.4245\n",
      "Epoch 136/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1983 - acc: 0.5269 - val_loss: 8.7385 - val_acc: 0.4258\n",
      "Epoch 137/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1938 - acc: 0.5345 - val_loss: 8.6422 - val_acc: 0.4234\n",
      "Epoch 138/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1896 - acc: 0.5325 - val_loss: 8.6467 - val_acc: 0.4227\n",
      "Epoch 139/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1891 - acc: 0.5297 - val_loss: 8.5654 - val_acc: 0.4223\n",
      "Epoch 140/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1823 - acc: 0.5368 - val_loss: 8.6547 - val_acc: 0.4232\n",
      "Epoch 141/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1838 - acc: 0.5337 - val_loss: 8.8360 - val_acc: 0.4245\n",
      "Epoch 142/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1841 - acc: 0.5329 - val_loss: 8.5616 - val_acc: 0.4216\n",
      "Epoch 143/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1754 - acc: 0.5366 - val_loss: 8.6137 - val_acc: 0.4234\n",
      "Epoch 144/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1838 - acc: 0.5346 - val_loss: 8.7534 - val_acc: 0.4260\n",
      "Epoch 145/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1726 - acc: 0.5394 - val_loss: 8.6311 - val_acc: 0.4234\n",
      "Epoch 146/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1710 - acc: 0.5393 - val_loss: 8.7468 - val_acc: 0.4236\n",
      "Epoch 147/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1674 - acc: 0.5392 - val_loss: 8.7488 - val_acc: 0.4251\n",
      "Epoch 148/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1732 - acc: 0.5356 - val_loss: 8.7556 - val_acc: 0.4253\n",
      "Epoch 149/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1707 - acc: 0.5406 - val_loss: 8.7696 - val_acc: 0.4242\n",
      "Epoch 150/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1708 - acc: 0.5378 - val_loss: 8.6824 - val_acc: 0.4242\n",
      "Epoch 151/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1665 - acc: 0.5357 - val_loss: 8.7025 - val_acc: 0.4234\n",
      "Epoch 152/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1672 - acc: 0.5401 - val_loss: 8.8397 - val_acc: 0.4256\n",
      "Epoch 153/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1626 - acc: 0.5427 - val_loss: 8.8843 - val_acc: 0.4256\n",
      "Epoch 154/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1544 - acc: 0.5440 - val_loss: 8.8810 - val_acc: 0.4245\n",
      "Epoch 155/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1565 - acc: 0.5418 - val_loss: 8.8033 - val_acc: 0.4229\n",
      "Epoch 156/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1564 - acc: 0.5445 - val_loss: 8.8311 - val_acc: 0.4225\n",
      "Epoch 157/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1484 - acc: 0.5451 - val_loss: 8.8545 - val_acc: 0.4242\n",
      "Epoch 158/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1528 - acc: 0.5433 - val_loss: 8.9066 - val_acc: 0.4245\n",
      "Epoch 159/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1489 - acc: 0.5445 - val_loss: 8.8392 - val_acc: 0.4240\n",
      "Epoch 160/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1427 - acc: 0.5518 - val_loss: 8.8345 - val_acc: 0.4219\n",
      "Epoch 161/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1466 - acc: 0.5465 - val_loss: 8.7978 - val_acc: 0.4227\n",
      "Epoch 162/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1428 - acc: 0.5469 - val_loss: 8.8670 - val_acc: 0.4238\n",
      "Epoch 163/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1428 - acc: 0.5485 - val_loss: 8.7582 - val_acc: 0.4216\n",
      "Epoch 164/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1379 - acc: 0.5526 - val_loss: 8.8844 - val_acc: 0.4238\n",
      "Epoch 165/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1346 - acc: 0.5512 - val_loss: 8.7052 - val_acc: 0.4210\n",
      "Epoch 166/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1327 - acc: 0.5529 - val_loss: 8.8466 - val_acc: 0.4229\n",
      "Epoch 167/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1353 - acc: 0.5497 - val_loss: 8.9320 - val_acc: 0.4245\n",
      "Epoch 168/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1316 - acc: 0.5542 - val_loss: 8.8543 - val_acc: 0.4203\n",
      "Epoch 169/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1308 - acc: 0.5555 - val_loss: 8.9371 - val_acc: 0.4236\n",
      "Epoch 170/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1272 - acc: 0.5535 - val_loss: 8.8766 - val_acc: 0.4205\n",
      "Epoch 171/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1294 - acc: 0.5547 - val_loss: 8.8378 - val_acc: 0.4225\n",
      "Epoch 172/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1200 - acc: 0.5569 - val_loss: 8.7995 - val_acc: 0.4221\n",
      "Epoch 173/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1138 - acc: 0.5606 - val_loss: 8.8081 - val_acc: 0.4184\n",
      "Epoch 174/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.1153 - acc: 0.5573 - val_loss: 8.7889 - val_acc: 0.4199\n",
      "Epoch 175/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1244 - acc: 0.5555 - val_loss: 8.7789 - val_acc: 0.4184\n",
      "Epoch 176/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1242 - acc: 0.5564 - val_loss: 8.7301 - val_acc: 0.4188\n",
      "Epoch 177/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1141 - acc: 0.5568 - val_loss: 8.8754 - val_acc: 0.4242\n",
      "Epoch 178/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1113 - acc: 0.5612 - val_loss: 8.9234 - val_acc: 0.4236\n",
      "Epoch 179/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1044 - acc: 0.5621 - val_loss: 8.8887 - val_acc: 0.4236\n",
      "Epoch 180/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1154 - acc: 0.5573 - val_loss: 8.9045 - val_acc: 0.4210\n",
      "Epoch 181/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1147 - acc: 0.5577 - val_loss: 8.9517 - val_acc: 0.4227\n",
      "Epoch 182/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1100 - acc: 0.5615 - val_loss: 8.9244 - val_acc: 0.4234\n",
      "Epoch 183/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1091 - acc: 0.5583 - val_loss: 8.9363 - val_acc: 0.4240\n",
      "Epoch 184/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1025 - acc: 0.5661 - val_loss: 8.9642 - val_acc: 0.4247\n",
      "Epoch 185/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1108 - acc: 0.5624 - val_loss: 8.9498 - val_acc: 0.4229\n",
      "Epoch 186/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.0990 - acc: 0.5643 - val_loss: 8.8288 - val_acc: 0.4225\n",
      "Epoch 187/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.1002 - acc: 0.5641 - val_loss: 8.7380 - val_acc: 0.4171\n",
      "Epoch 188/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0971 - acc: 0.5642 - val_loss: 8.9641 - val_acc: 0.4240\n",
      "Epoch 189/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.1017 - acc: 0.5610 - val_loss: 8.8984 - val_acc: 0.4205\n",
      "Epoch 190/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0961 - acc: 0.5646 - val_loss: 8.9144 - val_acc: 0.4210\n",
      "Epoch 191/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0953 - acc: 0.5666 - val_loss: 8.9881 - val_acc: 0.4240\n",
      "Epoch 192/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0983 - acc: 0.5684 - val_loss: 8.9592 - val_acc: 0.4240\n",
      "Epoch 193/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 1.0970 - acc: 0.5663 - val_loss: 8.8700 - val_acc: 0.4205\n",
      "Epoch 194/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0986 - acc: 0.5627 - val_loss: 8.9595 - val_acc: 0.4234\n",
      "Epoch 195/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0917 - acc: 0.5665 - val_loss: 8.9702 - val_acc: 0.4225\n",
      "Epoch 196/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0855 - acc: 0.5727 - val_loss: 8.9006 - val_acc: 0.4214\n",
      "Epoch 197/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0875 - acc: 0.5694 - val_loss: 8.8885 - val_acc: 0.4232\n",
      "Epoch 198/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0889 - acc: 0.5688 - val_loss: 8.9100 - val_acc: 0.4208\n",
      "Epoch 199/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0850 - acc: 0.5697 - val_loss: 8.9264 - val_acc: 0.4210\n",
      "Epoch 200/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0807 - acc: 0.5718 - val_loss: 8.8965 - val_acc: 0.4190\n",
      "Epoch 201/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 1.0806 - acc: 0.5753 - val_loss: 8.9671 - val_acc: 0.4214\n",
      "Epoch 202/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 1.0801 - acc: 0.5721 - val_loss: 9.0115 - val_acc: 0.4227\n",
      "Epoch 203/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 1.0800 - acc: 0.5740 - val_loss: 9.0240 - val_acc: 0.4210\n",
      "Epoch 204/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 1.0746 - acc: 0.5758 - val_loss: 8.9540 - val_acc: 0.4192\n",
      "Epoch 205/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 1.0685 - acc: 0.5775 - val_loss: 8.9603 - val_acc: 0.4197\n",
      "Epoch 206/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0727 - acc: 0.5759 - val_loss: 8.9844 - val_acc: 0.4212\n",
      "Epoch 207/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0789 - acc: 0.5747 - val_loss: 8.9909 - val_acc: 0.4229\n",
      "Epoch 208/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0739 - acc: 0.5758 - val_loss: 8.9825 - val_acc: 0.4219\n",
      "Epoch 209/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0592 - acc: 0.5782 - val_loss: 9.0368 - val_acc: 0.4245\n",
      "Epoch 210/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0707 - acc: 0.5748 - val_loss: 8.9415 - val_acc: 0.4203\n",
      "Epoch 211/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0667 - acc: 0.5782 - val_loss: 8.9710 - val_acc: 0.4229\n",
      "Epoch 212/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0617 - acc: 0.5782 - val_loss: 8.9992 - val_acc: 0.4236\n",
      "Epoch 213/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0577 - acc: 0.5797 - val_loss: 8.9152 - val_acc: 0.4223\n",
      "Epoch 214/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0639 - acc: 0.5771 - val_loss: 8.9556 - val_acc: 0.4210\n",
      "Epoch 215/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0481 - acc: 0.5828 - val_loss: 8.9868 - val_acc: 0.4227\n",
      "Epoch 216/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0581 - acc: 0.5798 - val_loss: 8.9816 - val_acc: 0.4219\n",
      "Epoch 217/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0578 - acc: 0.5809 - val_loss: 8.9764 - val_acc: 0.4223\n",
      "Epoch 218/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0658 - acc: 0.5796 - val_loss: 8.9835 - val_acc: 0.4227\n",
      "Epoch 219/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0555 - acc: 0.5817 - val_loss: 9.0108 - val_acc: 0.4203\n",
      "Epoch 220/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0479 - acc: 0.5837 - val_loss: 8.9492 - val_acc: 0.4192\n",
      "Epoch 221/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 1.0457 - acc: 0.5860 - val_loss: 8.9698 - val_acc: 0.4212\n",
      "Epoch 222/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 1.0509 - acc: 0.5837 - val_loss: 9.0081 - val_acc: 0.4201\n",
      "Epoch 223/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0530 - acc: 0.5853 - val_loss: 9.0256 - val_acc: 0.4210\n",
      "Epoch 224/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 1.0469 - acc: 0.5844 - val_loss: 9.0265 - val_acc: 0.4227\n",
      "Epoch 225/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 1.0486 - acc: 0.5864 - val_loss: 9.0503 - val_acc: 0.4212\n",
      "Epoch 226/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0431 - acc: 0.5853 - val_loss: 9.0534 - val_acc: 0.4242\n",
      "Epoch 227/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0474 - acc: 0.5836 - val_loss: 9.0351 - val_acc: 0.4221\n",
      "Epoch 228/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0458 - acc: 0.5863 - val_loss: 9.0508 - val_acc: 0.4197\n",
      "Epoch 229/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0360 - acc: 0.5883 - val_loss: 9.0590 - val_acc: 0.4229\n",
      "Epoch 230/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0503 - acc: 0.5858 - val_loss: 9.0489 - val_acc: 0.4208\n",
      "Epoch 231/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0413 - acc: 0.5864 - val_loss: 8.9917 - val_acc: 0.4212\n",
      "Epoch 232/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0369 - acc: 0.5896 - val_loss: 9.0175 - val_acc: 0.4221\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0327 - acc: 0.5918 - val_loss: 9.0384 - val_acc: 0.4242\n",
      "Epoch 234/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0393 - acc: 0.5884 - val_loss: 9.0118 - val_acc: 0.4201\n",
      "Epoch 235/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0369 - acc: 0.5875 - val_loss: 9.0264 - val_acc: 0.4229\n",
      "Epoch 236/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0371 - acc: 0.5898 - val_loss: 9.0722 - val_acc: 0.4227\n",
      "Epoch 237/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0267 - acc: 0.5931 - val_loss: 9.0557 - val_acc: 0.4221\n",
      "Epoch 238/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0334 - acc: 0.5933 - val_loss: 9.0397 - val_acc: 0.4212\n",
      "Epoch 239/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0235 - acc: 0.5918 - val_loss: 8.9925 - val_acc: 0.4212\n",
      "Epoch 240/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0287 - acc: 0.5921 - val_loss: 9.0456 - val_acc: 0.4203\n",
      "Epoch 241/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0242 - acc: 0.5951 - val_loss: 9.0710 - val_acc: 0.4216\n",
      "Epoch 242/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 1.0215 - acc: 0.5955 - val_loss: 9.0013 - val_acc: 0.4162\n",
      "Epoch 243/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 1.0223 - acc: 0.5957 - val_loss: 8.9502 - val_acc: 0.4175\n",
      "Epoch 244/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 1.0294 - acc: 0.5932 - val_loss: 9.0682 - val_acc: 0.4245\n",
      "Epoch 245/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 1.0299 - acc: 0.5940 - val_loss: 9.0420 - val_acc: 0.4227\n",
      "Epoch 246/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0170 - acc: 0.5965 - val_loss: 9.0603 - val_acc: 0.4240\n",
      "Epoch 247/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0149 - acc: 0.6004 - val_loss: 9.0486 - val_acc: 0.4221\n",
      "Epoch 248/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 1.0133 - acc: 0.5990 - val_loss: 9.0560 - val_acc: 0.4195\n",
      "Epoch 249/1000\n",
      "41259/41259 [==============================] - 80s 2ms/step - loss: 1.0181 - acc: 0.5986 - val_loss: 9.0631 - val_acc: 0.4208\n",
      "Epoch 250/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 1.0120 - acc: 0.6027 - val_loss: 9.0338 - val_acc: 0.4182\n",
      "Epoch 251/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 1.0130 - acc: 0.5976 - val_loss: 9.0958 - val_acc: 0.4229\n",
      "Epoch 252/1000\n",
      "41259/41259 [==============================] - 80s 2ms/step - loss: 1.0084 - acc: 0.6035 - val_loss: 9.0663 - val_acc: 0.4205\n",
      "Epoch 253/1000\n",
      "41259/41259 [==============================] - 80s 2ms/step - loss: 1.0138 - acc: 0.5951 - val_loss: 9.0273 - val_acc: 0.4182\n",
      "Epoch 254/1000\n",
      "41259/41259 [==============================] - 82s 2ms/step - loss: 1.0113 - acc: 0.6001 - val_loss: 9.0466 - val_acc: 0.4227\n",
      "Epoch 255/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 1.0037 - acc: 0.6013 - val_loss: 9.0597 - val_acc: 0.4227\n",
      "Epoch 256/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 1.0093 - acc: 0.6006 - val_loss: 9.0613 - val_acc: 0.4212\n",
      "Epoch 257/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 1.0144 - acc: 0.5972 - val_loss: 9.0111 - val_acc: 0.4201\n",
      "Epoch 258/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 1.0034 - acc: 0.6016 - val_loss: 9.0762 - val_acc: 0.4212\n",
      "Epoch 259/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 1.0050 - acc: 0.6022 - val_loss: 9.0806 - val_acc: 0.4232\n",
      "Epoch 260/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 1.0015 - acc: 0.6038 - val_loss: 9.0664 - val_acc: 0.4225\n",
      "Epoch 261/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.9981 - acc: 0.6067 - val_loss: 9.0638 - val_acc: 0.4227\n",
      "Epoch 262/1000\n",
      "41259/41259 [==============================] - 80s 2ms/step - loss: 1.0062 - acc: 0.6033 - val_loss: 9.0339 - val_acc: 0.4232\n",
      "Epoch 263/1000\n",
      "41259/41259 [==============================] - 83s 2ms/step - loss: 0.9871 - acc: 0.6104 - val_loss: 9.0606 - val_acc: 0.4223\n",
      "Epoch 264/1000\n",
      "41259/41259 [==============================] - 82s 2ms/step - loss: 1.0059 - acc: 0.6035 - val_loss: 9.0395 - val_acc: 0.4214\n",
      "Epoch 265/1000\n",
      "41259/41259 [==============================] - 82s 2ms/step - loss: 1.0039 - acc: 0.6055 - val_loss: 9.0790 - val_acc: 0.4223\n",
      "Epoch 266/1000\n",
      "41259/41259 [==============================] - 84s 2ms/step - loss: 1.0010 - acc: 0.6073 - val_loss: 9.0416 - val_acc: 0.4205\n",
      "Epoch 267/1000\n",
      "41259/41259 [==============================] - 82s 2ms/step - loss: 0.9939 - acc: 0.6091 - val_loss: 9.0865 - val_acc: 0.4223\n",
      "Epoch 268/1000\n",
      "41259/41259 [==============================] - 81s 2ms/step - loss: 0.9951 - acc: 0.6084 - val_loss: 9.0477 - val_acc: 0.4225\n",
      "Epoch 269/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 0.9873 - acc: 0.6111 - val_loss: 9.0918 - val_acc: 0.4229\n",
      "Epoch 270/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 0.9876 - acc: 0.6107 - val_loss: 9.0752 - val_acc: 0.4201\n",
      "Epoch 271/1000\n",
      "41259/41259 [==============================] - 81s 2ms/step - loss: 0.9941 - acc: 0.6104 - val_loss: 9.0840 - val_acc: 0.4219\n",
      "Epoch 272/1000\n",
      "41259/41259 [==============================] - 81s 2ms/step - loss: 0.9959 - acc: 0.6085 - val_loss: 9.0729 - val_acc: 0.4229\n",
      "Epoch 273/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 0.9942 - acc: 0.6084 - val_loss: 9.0479 - val_acc: 0.4229\n",
      "Epoch 274/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.9794 - acc: 0.6117 - val_loss: 9.0412 - val_acc: 0.4221\n",
      "Epoch 275/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.9875 - acc: 0.6094 - val_loss: 9.0557 - val_acc: 0.4238\n",
      "Epoch 276/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.9854 - acc: 0.6113 - val_loss: 9.0588 - val_acc: 0.4216\n",
      "Epoch 277/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.9823 - acc: 0.6135 - val_loss: 9.0528 - val_acc: 0.4234\n",
      "Epoch 278/1000\n",
      "41259/41259 [==============================] - 83s 2ms/step - loss: 0.9820 - acc: 0.6151 - val_loss: 9.0736 - val_acc: 0.4216\n",
      "Epoch 279/1000\n",
      "41259/41259 [==============================] - 81s 2ms/step - loss: 0.9738 - acc: 0.6153 - val_loss: 9.0543 - val_acc: 0.4219\n",
      "Epoch 280/1000\n",
      "41259/41259 [==============================] - 82s 2ms/step - loss: 0.9867 - acc: 0.6123 - val_loss: 9.0412 - val_acc: 0.4208\n",
      "Epoch 281/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 0.9725 - acc: 0.6193 - val_loss: 9.0637 - val_acc: 0.4210\n",
      "Epoch 282/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.9875 - acc: 0.6056 - val_loss: 9.0203 - val_acc: 0.4192\n",
      "Epoch 283/1000\n",
      "41259/41259 [==============================] - 82s 2ms/step - loss: 0.9814 - acc: 0.6138 - val_loss: 9.0469 - val_acc: 0.4208\n",
      "Epoch 284/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 0.9830 - acc: 0.6135 - val_loss: 9.0598 - val_acc: 0.4214\n",
      "Epoch 285/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 0.9661 - acc: 0.6208 - val_loss: 9.0548 - val_acc: 0.4219\n",
      "Epoch 286/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.9827 - acc: 0.6131 - val_loss: 9.0190 - val_acc: 0.4216\n",
      "Epoch 287/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9799 - acc: 0.6160 - val_loss: 9.0500 - val_acc: 0.4221\n",
      "Epoch 288/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9792 - acc: 0.6135 - val_loss: 9.0502 - val_acc: 0.4219\n",
      "Epoch 289/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9687 - acc: 0.6168 - val_loss: 9.0968 - val_acc: 0.4219\n",
      "Epoch 290/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9660 - acc: 0.6173 - val_loss: 9.0765 - val_acc: 0.4171\n",
      "Epoch 291/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9633 - acc: 0.6196 - val_loss: 9.0903 - val_acc: 0.4201\n",
      "Epoch 292/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9757 - acc: 0.6158 - val_loss: 9.0808 - val_acc: 0.4214\n",
      "Epoch 293/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.9627 - acc: 0.6259 - val_loss: 9.0697 - val_acc: 0.4210\n",
      "Epoch 294/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 0.9655 - acc: 0.6196 - val_loss: 9.0739 - val_acc: 0.4216\n",
      "Epoch 295/1000\n",
      "41259/41259 [==============================] - 79s 2ms/step - loss: 0.9589 - acc: 0.6221 - val_loss: 9.1111 - val_acc: 0.4238\n",
      "Epoch 296/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 0.9683 - acc: 0.6205 - val_loss: 9.0925 - val_acc: 0.4227\n",
      "Epoch 297/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9644 - acc: 0.6201 - val_loss: 9.0850 - val_acc: 0.4219\n",
      "Epoch 298/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9638 - acc: 0.6217 - val_loss: 9.1333 - val_acc: 0.4251\n",
      "Epoch 299/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.9540 - acc: 0.6266 - val_loss: 9.1243 - val_acc: 0.4242\n",
      "Epoch 300/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.9557 - acc: 0.6236 - val_loss: 9.0778 - val_acc: 0.4229\n",
      "Epoch 301/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9586 - acc: 0.6215 - val_loss: 9.1022 - val_acc: 0.4223\n",
      "Epoch 302/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9611 - acc: 0.6205 - val_loss: 9.1189 - val_acc: 0.4236\n",
      "Epoch 303/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9571 - acc: 0.6232 - val_loss: 9.1115 - val_acc: 0.4238\n",
      "Epoch 304/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9512 - acc: 0.6270 - val_loss: 9.1312 - val_acc: 0.4212\n",
      "Epoch 305/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9507 - acc: 0.6261 - val_loss: 9.0972 - val_acc: 0.4240\n",
      "Epoch 306/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9504 - acc: 0.6271 - val_loss: 9.1124 - val_acc: 0.4253\n",
      "Epoch 307/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9580 - acc: 0.6226 - val_loss: 9.1155 - val_acc: 0.4197\n",
      "Epoch 308/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9420 - acc: 0.6309 - val_loss: 9.1137 - val_acc: 0.4199\n",
      "Epoch 309/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9564 - acc: 0.6262 - val_loss: 9.0965 - val_acc: 0.4249\n",
      "Epoch 310/1000\n",
      "41259/41259 [==============================] - 70s 2ms/step - loss: 0.9435 - acc: 0.6271 - val_loss: 9.1043 - val_acc: 0.4245\n",
      "Epoch 311/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9478 - acc: 0.6282 - val_loss: 9.1231 - val_acc: 0.4238\n",
      "Epoch 312/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9493 - acc: 0.6307 - val_loss: 9.0965 - val_acc: 0.4229\n",
      "Epoch 313/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9402 - acc: 0.6322 - val_loss: 9.1193 - val_acc: 0.4242\n",
      "Epoch 314/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.9535 - acc: 0.6267 - val_loss: 9.1288 - val_acc: 0.4214\n",
      "Epoch 315/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9484 - acc: 0.6280 - val_loss: 9.1337 - val_acc: 0.4245\n",
      "Epoch 316/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9382 - acc: 0.6297 - val_loss: 9.1300 - val_acc: 0.4236\n",
      "Epoch 317/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9450 - acc: 0.6327 - val_loss: 9.1178 - val_acc: 0.4240\n",
      "Epoch 318/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9347 - acc: 0.6335 - val_loss: 9.1267 - val_acc: 0.4251\n",
      "Epoch 319/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9430 - acc: 0.6288 - val_loss: 9.1189 - val_acc: 0.4245\n",
      "Epoch 320/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.9369 - acc: 0.6329 - val_loss: 9.1533 - val_acc: 0.4212\n",
      "Epoch 321/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9401 - acc: 0.6330 - val_loss: 9.1387 - val_acc: 0.4214\n",
      "Epoch 322/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.9361 - acc: 0.6309 - val_loss: 9.1189 - val_acc: 0.4210\n",
      "Epoch 323/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9333 - acc: 0.6334 - val_loss: 9.1488 - val_acc: 0.4223\n",
      "Epoch 324/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.9416 - acc: 0.6313 - val_loss: 9.1185 - val_acc: 0.4216\n",
      "Epoch 325/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.9370 - acc: 0.6313 - val_loss: 9.1169 - val_acc: 0.4221\n",
      "Epoch 326/1000\n",
      "41259/41259 [==============================] - 78s 2ms/step - loss: 0.9411 - acc: 0.6345 - val_loss: 9.1214 - val_acc: 0.4245\n",
      "Epoch 327/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9281 - acc: 0.6365 - val_loss: 9.1375 - val_acc: 0.4236\n",
      "Epoch 328/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9448 - acc: 0.6317 - val_loss: 9.1410 - val_acc: 0.4234\n",
      "Epoch 329/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9381 - acc: 0.6326 - val_loss: 9.1380 - val_acc: 0.4240\n",
      "Epoch 330/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9295 - acc: 0.6380 - val_loss: 9.1147 - val_acc: 0.4221\n",
      "Epoch 331/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9329 - acc: 0.6351 - val_loss: 9.1365 - val_acc: 0.4232\n",
      "Epoch 332/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9287 - acc: 0.6388 - val_loss: 9.1204 - val_acc: 0.4225\n",
      "Epoch 333/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9230 - acc: 0.6398 - val_loss: 9.1366 - val_acc: 0.4225\n",
      "Epoch 334/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9241 - acc: 0.6386 - val_loss: 9.1314 - val_acc: 0.4201\n",
      "Epoch 335/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9261 - acc: 0.6408 - val_loss: 9.1317 - val_acc: 0.4225\n",
      "Epoch 336/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9332 - acc: 0.6345 - val_loss: 9.1298 - val_acc: 0.4238\n",
      "Epoch 337/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9339 - acc: 0.6328 - val_loss: 9.1388 - val_acc: 0.4251\n",
      "Epoch 338/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9198 - acc: 0.6395 - val_loss: 9.1516 - val_acc: 0.4210\n",
      "Epoch 339/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9342 - acc: 0.6357 - val_loss: 9.1480 - val_acc: 0.4210\n",
      "Epoch 340/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9324 - acc: 0.6360 - val_loss: 9.1385 - val_acc: 0.4188\n",
      "Epoch 341/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9196 - acc: 0.6408 - val_loss: 9.1790 - val_acc: 0.4212\n",
      "Epoch 342/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.9244 - acc: 0.6385 - val_loss: 9.1508 - val_acc: 0.4225\n",
      "Epoch 343/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9253 - acc: 0.6394 - val_loss: 9.1487 - val_acc: 0.4203\n",
      "Epoch 344/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9265 - acc: 0.6394 - val_loss: 9.1530 - val_acc: 0.4227\n",
      "Epoch 345/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9173 - acc: 0.6401 - val_loss: 9.1415 - val_acc: 0.4229\n",
      "Epoch 346/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9134 - acc: 0.6411 - val_loss: 9.1632 - val_acc: 0.4221\n",
      "Epoch 347/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.9240 - acc: 0.6394 - val_loss: 9.1501 - val_acc: 0.4240\n",
      "Epoch 348/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9117 - acc: 0.6434 - val_loss: 9.1412 - val_acc: 0.4199\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9143 - acc: 0.6444 - val_loss: 9.1358 - val_acc: 0.4208\n",
      "Epoch 350/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9186 - acc: 0.6432 - val_loss: 9.1288 - val_acc: 0.4190\n",
      "Epoch 351/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9161 - acc: 0.6432 - val_loss: 9.1550 - val_acc: 0.4225\n",
      "Epoch 352/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9108 - acc: 0.6442 - val_loss: 9.1598 - val_acc: 0.4208\n",
      "Epoch 353/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9129 - acc: 0.6414 - val_loss: 9.1536 - val_acc: 0.4234\n",
      "Epoch 354/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9202 - acc: 0.6404 - val_loss: 9.1565 - val_acc: 0.4240\n",
      "Epoch 355/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9074 - acc: 0.6472 - val_loss: 9.1716 - val_acc: 0.4242\n",
      "Epoch 356/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9121 - acc: 0.6457 - val_loss: 9.1840 - val_acc: 0.4242\n",
      "Epoch 357/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9213 - acc: 0.6397 - val_loss: 9.1633 - val_acc: 0.4234\n",
      "Epoch 358/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9152 - acc: 0.6429 - val_loss: 9.1697 - val_acc: 0.4234\n",
      "Epoch 359/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9218 - acc: 0.6425 - val_loss: 9.1587 - val_acc: 0.4232\n",
      "Epoch 360/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.9033 - acc: 0.6468 - val_loss: 9.1697 - val_acc: 0.4242\n",
      "Epoch 361/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8993 - acc: 0.6517 - val_loss: 9.1555 - val_acc: 0.4223\n",
      "Epoch 362/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9023 - acc: 0.6486 - val_loss: 9.1587 - val_acc: 0.4236\n",
      "Epoch 363/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8965 - acc: 0.6512 - val_loss: 9.1619 - val_acc: 0.4221\n",
      "Epoch 364/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9011 - acc: 0.6509 - val_loss: 9.1756 - val_acc: 0.4253\n",
      "Epoch 365/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9000 - acc: 0.6496 - val_loss: 9.1804 - val_acc: 0.4242\n",
      "Epoch 366/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9017 - acc: 0.6520 - val_loss: 9.1613 - val_acc: 0.4240\n",
      "Epoch 367/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8972 - acc: 0.6521 - val_loss: 9.1686 - val_acc: 0.4242\n",
      "Epoch 368/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8950 - acc: 0.6516 - val_loss: 9.1563 - val_acc: 0.4253\n",
      "Epoch 369/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8985 - acc: 0.6529 - val_loss: 9.1555 - val_acc: 0.4219\n",
      "Epoch 370/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8966 - acc: 0.6503 - val_loss: 9.1678 - val_acc: 0.4245\n",
      "Epoch 371/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9025 - acc: 0.6511 - val_loss: 9.1640 - val_acc: 0.4253\n",
      "Epoch 372/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9009 - acc: 0.6493 - val_loss: 9.1661 - val_acc: 0.4229\n",
      "Epoch 373/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8911 - acc: 0.6523 - val_loss: 9.1591 - val_acc: 0.4247\n",
      "Epoch 374/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8984 - acc: 0.6512 - val_loss: 9.1586 - val_acc: 0.4242\n",
      "Epoch 375/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8942 - acc: 0.6516 - val_loss: 9.1698 - val_acc: 0.4234\n",
      "Epoch 376/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8883 - acc: 0.6536 - val_loss: 9.1741 - val_acc: 0.4238\n",
      "Epoch 377/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9058 - acc: 0.6437 - val_loss: 9.1746 - val_acc: 0.4242\n",
      "Epoch 378/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8877 - acc: 0.6560 - val_loss: 9.1696 - val_acc: 0.4245\n",
      "Epoch 379/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8904 - acc: 0.6517 - val_loss: 9.1783 - val_acc: 0.4242\n",
      "Epoch 380/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.9008 - acc: 0.6488 - val_loss: 9.1698 - val_acc: 0.4247\n",
      "Epoch 381/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8803 - acc: 0.6585 - val_loss: 9.1754 - val_acc: 0.4236\n",
      "Epoch 382/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8829 - acc: 0.6598 - val_loss: 9.1814 - val_acc: 0.4236\n",
      "Epoch 383/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8837 - acc: 0.6558 - val_loss: 9.1797 - val_acc: 0.4247\n",
      "Epoch 384/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8885 - acc: 0.6527 - val_loss: 9.1867 - val_acc: 0.4242\n",
      "Epoch 385/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8873 - acc: 0.6561 - val_loss: 9.1673 - val_acc: 0.4251\n",
      "Epoch 386/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8853 - acc: 0.6572 - val_loss: 9.1891 - val_acc: 0.4238\n",
      "Epoch 387/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8887 - acc: 0.6569 - val_loss: 9.1887 - val_acc: 0.4236\n",
      "Epoch 388/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8835 - acc: 0.6592 - val_loss: 9.1835 - val_acc: 0.4234\n",
      "Epoch 389/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8853 - acc: 0.6560 - val_loss: 9.1850 - val_acc: 0.4247\n",
      "Epoch 390/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8708 - acc: 0.6616 - val_loss: 9.1864 - val_acc: 0.4242\n",
      "Epoch 391/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8917 - acc: 0.6558 - val_loss: 9.1702 - val_acc: 0.4256\n",
      "Epoch 392/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8837 - acc: 0.6581 - val_loss: 9.1768 - val_acc: 0.4214\n",
      "Epoch 393/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8912 - acc: 0.6553 - val_loss: 9.1677 - val_acc: 0.4245\n",
      "Epoch 394/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8788 - acc: 0.6585 - val_loss: 9.1769 - val_acc: 0.4256\n",
      "Epoch 395/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8791 - acc: 0.6595 - val_loss: 9.1777 - val_acc: 0.4238\n",
      "Epoch 396/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8794 - acc: 0.6571 - val_loss: 9.1761 - val_acc: 0.4256\n",
      "Epoch 397/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8805 - acc: 0.6614 - val_loss: 9.1680 - val_acc: 0.4245\n",
      "Epoch 398/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8830 - acc: 0.6578 - val_loss: 9.1817 - val_acc: 0.4232\n",
      "Epoch 399/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8764 - acc: 0.6605 - val_loss: 9.1806 - val_acc: 0.4232\n",
      "Epoch 400/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8761 - acc: 0.6592 - val_loss: 9.1769 - val_acc: 0.4236\n",
      "Epoch 401/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8760 - acc: 0.6622 - val_loss: 9.1714 - val_acc: 0.4247\n",
      "Epoch 402/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8729 - acc: 0.6601 - val_loss: 9.1813 - val_acc: 0.4229\n",
      "Epoch 403/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8648 - acc: 0.6653 - val_loss: 9.1920 - val_acc: 0.4240\n",
      "Epoch 404/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8672 - acc: 0.6656 - val_loss: 9.1844 - val_acc: 0.4240\n",
      "Epoch 405/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8782 - acc: 0.6611 - val_loss: 9.1859 - val_acc: 0.4236\n",
      "Epoch 406/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8586 - acc: 0.6665 - val_loss: 9.1743 - val_acc: 0.4251\n",
      "Epoch 407/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8760 - acc: 0.6636 - val_loss: 9.1991 - val_acc: 0.4236\n",
      "Epoch 408/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8706 - acc: 0.6642 - val_loss: 9.1838 - val_acc: 0.4258\n",
      "Epoch 409/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8707 - acc: 0.6649 - val_loss: 9.1767 - val_acc: 0.4232\n",
      "Epoch 410/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8726 - acc: 0.6603 - val_loss: 9.1682 - val_acc: 0.4251\n",
      "Epoch 411/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8641 - acc: 0.6654 - val_loss: 9.1785 - val_acc: 0.4240\n",
      "Epoch 412/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8670 - acc: 0.6643 - val_loss: 9.1738 - val_acc: 0.4249\n",
      "Epoch 413/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8770 - acc: 0.6624 - val_loss: 9.1767 - val_acc: 0.4242\n",
      "Epoch 414/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8700 - acc: 0.6637 - val_loss: 9.1806 - val_acc: 0.4256\n",
      "Epoch 415/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8623 - acc: 0.6654 - val_loss: 9.1905 - val_acc: 0.4260\n",
      "Epoch 416/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8668 - acc: 0.6659 - val_loss: 9.1678 - val_acc: 0.4251\n",
      "Epoch 417/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8653 - acc: 0.6667 - val_loss: 9.1801 - val_acc: 0.4240\n",
      "Epoch 418/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8610 - acc: 0.6680 - val_loss: 9.1739 - val_acc: 0.4236\n",
      "Epoch 419/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8588 - acc: 0.6687 - val_loss: 9.1877 - val_acc: 0.4236\n",
      "Epoch 420/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8566 - acc: 0.6713 - val_loss: 9.1816 - val_acc: 0.4223\n",
      "Epoch 421/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8769 - acc: 0.6621 - val_loss: 9.1769 - val_acc: 0.4258\n",
      "Epoch 422/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8576 - acc: 0.6666 - val_loss: 9.1712 - val_acc: 0.4240\n",
      "Epoch 423/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8646 - acc: 0.6643 - val_loss: 9.1879 - val_acc: 0.4236\n",
      "Epoch 424/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8613 - acc: 0.6652 - val_loss: 9.1852 - val_acc: 0.4240\n",
      "Epoch 425/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8664 - acc: 0.6666 - val_loss: 9.1815 - val_acc: 0.4253\n",
      "Epoch 426/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8541 - acc: 0.6697 - val_loss: 9.1954 - val_acc: 0.4232\n",
      "Epoch 427/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8560 - acc: 0.6680 - val_loss: 9.1888 - val_acc: 0.4227\n",
      "Epoch 428/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8543 - acc: 0.6713 - val_loss: 9.1776 - val_acc: 0.4245\n",
      "Epoch 429/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8557 - acc: 0.6715 - val_loss: 9.1828 - val_acc: 0.4240\n",
      "Epoch 430/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8574 - acc: 0.6687 - val_loss: 9.1994 - val_acc: 0.4238\n",
      "Epoch 431/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8482 - acc: 0.6729 - val_loss: 9.1918 - val_acc: 0.4232\n",
      "Epoch 432/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8486 - acc: 0.6747 - val_loss: 9.1869 - val_acc: 0.4247\n",
      "Epoch 433/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8599 - acc: 0.6705 - val_loss: 9.1733 - val_acc: 0.4249\n",
      "Epoch 434/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8512 - acc: 0.6693 - val_loss: 9.1867 - val_acc: 0.4247\n",
      "Epoch 435/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8416 - acc: 0.6768 - val_loss: 9.1885 - val_acc: 0.4225\n",
      "Epoch 436/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8477 - acc: 0.6712 - val_loss: 9.1912 - val_acc: 0.4247\n",
      "Epoch 437/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8437 - acc: 0.6776 - val_loss: 9.1862 - val_acc: 0.4245\n",
      "Epoch 438/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8431 - acc: 0.6764 - val_loss: 9.1873 - val_acc: 0.4238\n",
      "Epoch 439/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8508 - acc: 0.6739 - val_loss: 9.1754 - val_acc: 0.4251\n",
      "Epoch 440/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8590 - acc: 0.6711 - val_loss: 9.1835 - val_acc: 0.4247\n",
      "Epoch 441/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8425 - acc: 0.6740 - val_loss: 9.1976 - val_acc: 0.4234\n",
      "Epoch 442/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8504 - acc: 0.6718 - val_loss: 9.1946 - val_acc: 0.4249\n",
      "Epoch 443/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8542 - acc: 0.6718 - val_loss: 9.1811 - val_acc: 0.4256\n",
      "Epoch 444/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8476 - acc: 0.6724 - val_loss: 9.1920 - val_acc: 0.4247\n",
      "Epoch 445/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8566 - acc: 0.6708 - val_loss: 9.1963 - val_acc: 0.4232\n",
      "Epoch 446/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8498 - acc: 0.6730 - val_loss: 9.1918 - val_acc: 0.4245\n",
      "Epoch 447/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8355 - acc: 0.6798 - val_loss: 9.1879 - val_acc: 0.4256\n",
      "Epoch 448/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8381 - acc: 0.6748 - val_loss: 9.1860 - val_acc: 0.4240\n",
      "Epoch 449/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8510 - acc: 0.6720 - val_loss: 9.1825 - val_acc: 0.4240\n",
      "Epoch 450/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8437 - acc: 0.6746 - val_loss: 9.1887 - val_acc: 0.4249\n",
      "Epoch 451/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8377 - acc: 0.6784 - val_loss: 9.1882 - val_acc: 0.4251\n",
      "Epoch 452/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8410 - acc: 0.6754 - val_loss: 9.1870 - val_acc: 0.4242\n",
      "Epoch 453/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8406 - acc: 0.6767 - val_loss: 9.1903 - val_acc: 0.4236\n",
      "Epoch 454/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8323 - acc: 0.6799 - val_loss: 9.1871 - val_acc: 0.4249\n",
      "Epoch 455/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8404 - acc: 0.6759 - val_loss: 9.1861 - val_acc: 0.4245\n",
      "Epoch 456/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8357 - acc: 0.6811 - val_loss: 9.1867 - val_acc: 0.4249\n",
      "Epoch 457/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8298 - acc: 0.6808 - val_loss: 9.1930 - val_acc: 0.4251\n",
      "Epoch 458/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8433 - acc: 0.6754 - val_loss: 9.1917 - val_acc: 0.4236\n",
      "Epoch 459/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8366 - acc: 0.6779 - val_loss: 9.2122 - val_acc: 0.4240\n",
      "Epoch 460/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8378 - acc: 0.6771 - val_loss: 9.1934 - val_acc: 0.4249\n",
      "Epoch 461/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8329 - acc: 0.6789 - val_loss: 9.2084 - val_acc: 0.4234\n",
      "Epoch 462/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8399 - acc: 0.6784 - val_loss: 9.2041 - val_acc: 0.4240\n",
      "Epoch 463/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8306 - acc: 0.6809 - val_loss: 9.1970 - val_acc: 0.4245\n",
      "Epoch 464/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8300 - acc: 0.6825 - val_loss: 9.2038 - val_acc: 0.4247\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8293 - acc: 0.6832 - val_loss: 9.1882 - val_acc: 0.4251\n",
      "Epoch 466/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8312 - acc: 0.6812 - val_loss: 9.1971 - val_acc: 0.4264\n",
      "Epoch 467/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8433 - acc: 0.6723 - val_loss: 9.1958 - val_acc: 0.4236\n",
      "Epoch 468/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8345 - acc: 0.6790 - val_loss: 9.1912 - val_acc: 0.4238\n",
      "Epoch 469/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8290 - acc: 0.6843 - val_loss: 9.2052 - val_acc: 0.4238\n",
      "Epoch 470/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8318 - acc: 0.6773 - val_loss: 9.2015 - val_acc: 0.4232\n",
      "Epoch 471/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8293 - acc: 0.6837 - val_loss: 9.1918 - val_acc: 0.4260\n",
      "Epoch 472/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8303 - acc: 0.6814 - val_loss: 9.1885 - val_acc: 0.4253\n",
      "Epoch 473/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8251 - acc: 0.6843 - val_loss: 9.2036 - val_acc: 0.4240\n",
      "Epoch 474/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8351 - acc: 0.6815 - val_loss: 9.1964 - val_acc: 0.4238\n",
      "Epoch 475/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8350 - acc: 0.6804 - val_loss: 9.2019 - val_acc: 0.4238\n",
      "Epoch 476/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8182 - acc: 0.6879 - val_loss: 9.1957 - val_acc: 0.4236\n",
      "Epoch 477/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8299 - acc: 0.6800 - val_loss: 9.2033 - val_acc: 0.4227\n",
      "Epoch 478/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8324 - acc: 0.6823 - val_loss: 9.1964 - val_acc: 0.4238\n",
      "Epoch 479/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8259 - acc: 0.6841 - val_loss: 9.1966 - val_acc: 0.4249\n",
      "Epoch 480/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8373 - acc: 0.6762 - val_loss: 9.1885 - val_acc: 0.4240\n",
      "Epoch 481/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8402 - acc: 0.6769 - val_loss: 9.1904 - val_acc: 0.4234\n",
      "Epoch 482/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8285 - acc: 0.6816 - val_loss: 9.1891 - val_acc: 0.4236\n",
      "Epoch 483/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8195 - acc: 0.6899 - val_loss: 9.1891 - val_acc: 0.4236\n",
      "Epoch 484/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8256 - acc: 0.6827 - val_loss: 9.1990 - val_acc: 0.4212\n",
      "Epoch 485/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8228 - acc: 0.6842 - val_loss: 9.1868 - val_acc: 0.4253\n",
      "Epoch 486/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8321 - acc: 0.6808 - val_loss: 9.1868 - val_acc: 0.4232\n",
      "Epoch 487/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8320 - acc: 0.6802 - val_loss: 9.2202 - val_acc: 0.4236\n",
      "Epoch 488/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8359 - acc: 0.6819 - val_loss: 9.2001 - val_acc: 0.4234\n",
      "Epoch 489/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8298 - acc: 0.6841 - val_loss: 9.1960 - val_acc: 0.4236\n",
      "Epoch 490/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8216 - acc: 0.6863 - val_loss: 9.1822 - val_acc: 0.4236\n",
      "Epoch 491/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8312 - acc: 0.6834 - val_loss: 9.1951 - val_acc: 0.4238\n",
      "Epoch 492/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8286 - acc: 0.6800 - val_loss: 9.2039 - val_acc: 0.4227\n",
      "Epoch 493/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8283 - acc: 0.6833 - val_loss: 9.2020 - val_acc: 0.4238\n",
      "Epoch 494/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8227 - acc: 0.6865 - val_loss: 9.2158 - val_acc: 0.4227\n",
      "Epoch 495/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8190 - acc: 0.6865 - val_loss: 9.2145 - val_acc: 0.4214\n",
      "Epoch 496/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8214 - acc: 0.6871 - val_loss: 9.2004 - val_acc: 0.4249\n",
      "Epoch 497/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8217 - acc: 0.6877 - val_loss: 9.1990 - val_acc: 0.4249\n",
      "Epoch 498/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8262 - acc: 0.6865 - val_loss: 9.1967 - val_acc: 0.4253\n",
      "Epoch 499/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8259 - acc: 0.6852 - val_loss: 9.2047 - val_acc: 0.4245\n",
      "Epoch 500/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8191 - acc: 0.6894 - val_loss: 9.1947 - val_acc: 0.4242\n",
      "Epoch 501/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8207 - acc: 0.6869 - val_loss: 9.2044 - val_acc: 0.4236\n",
      "Epoch 502/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8200 - acc: 0.6864 - val_loss: 9.2245 - val_acc: 0.4229\n",
      "Epoch 503/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8301 - acc: 0.6831 - val_loss: 9.2015 - val_acc: 0.4242\n",
      "Epoch 504/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8235 - acc: 0.6824 - val_loss: 9.1884 - val_acc: 0.4249\n",
      "Epoch 505/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8094 - acc: 0.6895 - val_loss: 9.1934 - val_acc: 0.4262\n",
      "Epoch 506/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8162 - acc: 0.6872 - val_loss: 9.1958 - val_acc: 0.4251\n",
      "Epoch 507/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8222 - acc: 0.6876 - val_loss: 9.1880 - val_acc: 0.4249\n",
      "Epoch 508/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8170 - acc: 0.6907 - val_loss: 9.1961 - val_acc: 0.4251\n",
      "Epoch 509/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8193 - acc: 0.6887 - val_loss: 9.1858 - val_acc: 0.4249\n",
      "Epoch 510/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8298 - acc: 0.6825 - val_loss: 9.2022 - val_acc: 0.4234\n",
      "Epoch 511/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8194 - acc: 0.6871 - val_loss: 9.1987 - val_acc: 0.4260\n",
      "Epoch 512/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8154 - acc: 0.6874 - val_loss: 9.1878 - val_acc: 0.4240\n",
      "Epoch 513/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8003 - acc: 0.6918 - val_loss: 9.2069 - val_acc: 0.4225\n",
      "Epoch 514/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8014 - acc: 0.6938 - val_loss: 9.1889 - val_acc: 0.4253\n",
      "Epoch 515/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8083 - acc: 0.6897 - val_loss: 9.1925 - val_acc: 0.4253\n",
      "Epoch 516/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8053 - acc: 0.6910 - val_loss: 9.1973 - val_acc: 0.4249\n",
      "Epoch 517/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8156 - acc: 0.6883 - val_loss: 9.1971 - val_acc: 0.4249\n",
      "Epoch 518/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8047 - acc: 0.6921 - val_loss: 9.1941 - val_acc: 0.4245\n",
      "Epoch 519/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8057 - acc: 0.6924 - val_loss: 9.1926 - val_acc: 0.4247\n",
      "Epoch 520/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8016 - acc: 0.6956 - val_loss: 9.1974 - val_acc: 0.4240\n",
      "Epoch 521/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7953 - acc: 0.6963 - val_loss: 9.1899 - val_acc: 0.4262\n",
      "Epoch 522/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8005 - acc: 0.6961 - val_loss: 9.2031 - val_acc: 0.4249\n",
      "Epoch 523/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8022 - acc: 0.6934 - val_loss: 9.1994 - val_acc: 0.4264\n",
      "Epoch 524/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7971 - acc: 0.6952 - val_loss: 9.1976 - val_acc: 0.4256\n",
      "Epoch 525/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7961 - acc: 0.6974 - val_loss: 9.2067 - val_acc: 0.4256\n",
      "Epoch 526/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8068 - acc: 0.6936 - val_loss: 9.2041 - val_acc: 0.4258\n",
      "Epoch 527/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7986 - acc: 0.6959 - val_loss: 9.1977 - val_acc: 0.4256\n",
      "Epoch 528/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8093 - acc: 0.6878 - val_loss: 9.1964 - val_acc: 0.4260\n",
      "Epoch 529/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8043 - acc: 0.6916 - val_loss: 9.1982 - val_acc: 0.4253\n",
      "Epoch 530/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8075 - acc: 0.6924 - val_loss: 9.1966 - val_acc: 0.4236\n",
      "Epoch 531/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8078 - acc: 0.6918 - val_loss: 9.2048 - val_acc: 0.4256\n",
      "Epoch 532/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8033 - acc: 0.6942 - val_loss: 9.1987 - val_acc: 0.4247\n",
      "Epoch 533/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7928 - acc: 0.6963 - val_loss: 9.1981 - val_acc: 0.4256\n",
      "Epoch 534/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7947 - acc: 0.6972 - val_loss: 9.2014 - val_acc: 0.4258\n",
      "Epoch 535/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8020 - acc: 0.6930 - val_loss: 9.1997 - val_acc: 0.4258\n",
      "Epoch 536/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8028 - acc: 0.6943 - val_loss: 9.2024 - val_acc: 0.4242\n",
      "Epoch 537/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7935 - acc: 0.6968 - val_loss: 9.2013 - val_acc: 0.4238\n",
      "Epoch 538/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7966 - acc: 0.6966 - val_loss: 9.1985 - val_acc: 0.4247\n",
      "Epoch 539/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7924 - acc: 0.6966 - val_loss: 9.2008 - val_acc: 0.4240\n",
      "Epoch 540/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7888 - acc: 0.6997 - val_loss: 9.2069 - val_acc: 0.4249\n",
      "Epoch 541/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7897 - acc: 0.6996 - val_loss: 9.2074 - val_acc: 0.4249\n",
      "Epoch 542/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8091 - acc: 0.6901 - val_loss: 9.2060 - val_acc: 0.4251\n",
      "Epoch 543/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8090 - acc: 0.6901 - val_loss: 9.2020 - val_acc: 0.4251\n",
      "Epoch 544/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8041 - acc: 0.6943 - val_loss: 9.1950 - val_acc: 0.4251\n",
      "Epoch 545/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7911 - acc: 0.6988 - val_loss: 9.1990 - val_acc: 0.4247\n",
      "Epoch 546/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7961 - acc: 0.6978 - val_loss: 9.1945 - val_acc: 0.4247\n",
      "Epoch 547/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7889 - acc: 0.6992 - val_loss: 9.1996 - val_acc: 0.4245\n",
      "Epoch 548/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7974 - acc: 0.6975 - val_loss: 9.1925 - val_acc: 0.4256\n",
      "Epoch 549/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7958 - acc: 0.6974 - val_loss: 9.1976 - val_acc: 0.4242\n",
      "Epoch 550/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7945 - acc: 0.6987 - val_loss: 9.2013 - val_acc: 0.4253\n",
      "Epoch 551/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8174 - acc: 0.6888 - val_loss: 9.1995 - val_acc: 0.4247\n",
      "Epoch 552/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8096 - acc: 0.6942 - val_loss: 9.1915 - val_acc: 0.4258\n",
      "Epoch 553/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8012 - acc: 0.6950 - val_loss: 9.2112 - val_acc: 0.4245\n",
      "Epoch 554/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8011 - acc: 0.6958 - val_loss: 9.2055 - val_acc: 0.4238\n",
      "Epoch 555/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7987 - acc: 0.6962 - val_loss: 9.2039 - val_acc: 0.4232\n",
      "Epoch 556/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7973 - acc: 0.6963 - val_loss: 9.2020 - val_acc: 0.4258\n",
      "Epoch 557/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8035 - acc: 0.6948 - val_loss: 9.2020 - val_acc: 0.4251\n",
      "Epoch 558/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7844 - acc: 0.7043 - val_loss: 9.1952 - val_acc: 0.4258\n",
      "Epoch 559/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7983 - acc: 0.6965 - val_loss: 9.2054 - val_acc: 0.4251\n",
      "Epoch 560/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8109 - acc: 0.6912 - val_loss: 9.1991 - val_acc: 0.4253\n",
      "Epoch 561/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7943 - acc: 0.6972 - val_loss: 9.2052 - val_acc: 0.4249\n",
      "Epoch 562/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8020 - acc: 0.6958 - val_loss: 9.2031 - val_acc: 0.4258\n",
      "Epoch 563/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7990 - acc: 0.6974 - val_loss: 9.1985 - val_acc: 0.4256\n",
      "Epoch 564/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7913 - acc: 0.7043 - val_loss: 9.1983 - val_acc: 0.4240\n",
      "Epoch 565/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7945 - acc: 0.6995 - val_loss: 9.1948 - val_acc: 0.4238\n",
      "Epoch 566/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8102 - acc: 0.6892 - val_loss: 9.2041 - val_acc: 0.4251\n",
      "Epoch 567/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7933 - acc: 0.7003 - val_loss: 9.2027 - val_acc: 0.4238\n",
      "Epoch 568/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7940 - acc: 0.6987 - val_loss: 9.2112 - val_acc: 0.4238\n",
      "Epoch 569/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7792 - acc: 0.7068 - val_loss: 9.2037 - val_acc: 0.4253\n",
      "Epoch 570/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7947 - acc: 0.6990 - val_loss: 9.2014 - val_acc: 0.4253\n",
      "Epoch 571/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8055 - acc: 0.6941 - val_loss: 9.2083 - val_acc: 0.4234\n",
      "Epoch 572/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8090 - acc: 0.6937 - val_loss: 9.2034 - val_acc: 0.4238\n",
      "Epoch 573/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7844 - acc: 0.7029 - val_loss: 9.2147 - val_acc: 0.4234\n",
      "Epoch 574/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7881 - acc: 0.7005 - val_loss: 9.2039 - val_acc: 0.4240\n",
      "Epoch 575/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7873 - acc: 0.7028 - val_loss: 9.2042 - val_acc: 0.4253\n",
      "Epoch 576/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7920 - acc: 0.7010 - val_loss: 9.2082 - val_acc: 0.4247\n",
      "Epoch 577/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7910 - acc: 0.7025 - val_loss: 9.2026 - val_acc: 0.4251\n",
      "Epoch 578/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7930 - acc: 0.7004 - val_loss: 9.1977 - val_acc: 0.4256\n",
      "Epoch 579/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7978 - acc: 0.6974 - val_loss: 9.1941 - val_acc: 0.4256\n",
      "Epoch 580/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7975 - acc: 0.6983 - val_loss: 9.1981 - val_acc: 0.4264\n",
      "Epoch 581/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8008 - acc: 0.6958 - val_loss: 9.1973 - val_acc: 0.4260\n",
      "Epoch 582/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7995 - acc: 0.6966 - val_loss: 9.1988 - val_acc: 0.4249\n",
      "Epoch 583/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7986 - acc: 0.6981 - val_loss: 9.1817 - val_acc: 0.4256\n",
      "Epoch 584/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7968 - acc: 0.7003 - val_loss: 9.1986 - val_acc: 0.4249\n",
      "Epoch 585/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7866 - acc: 0.7019 - val_loss: 9.1986 - val_acc: 0.4258\n",
      "Epoch 586/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8045 - acc: 0.6944 - val_loss: 9.1867 - val_acc: 0.4264\n",
      "Epoch 587/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8057 - acc: 0.6949 - val_loss: 9.1954 - val_acc: 0.4258\n",
      "Epoch 588/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7826 - acc: 0.7029 - val_loss: 9.2025 - val_acc: 0.4262\n",
      "Epoch 589/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8034 - acc: 0.6949 - val_loss: 9.2023 - val_acc: 0.4266\n",
      "Epoch 590/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7963 - acc: 0.7004 - val_loss: 9.2050 - val_acc: 0.4262\n",
      "Epoch 591/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7893 - acc: 0.7012 - val_loss: 9.1970 - val_acc: 0.4269\n",
      "Epoch 592/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7992 - acc: 0.6983 - val_loss: 9.1953 - val_acc: 0.4260\n",
      "Epoch 593/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7839 - acc: 0.7042 - val_loss: 9.2042 - val_acc: 0.4258\n",
      "Epoch 594/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7888 - acc: 0.7021 - val_loss: 9.1951 - val_acc: 0.4256\n",
      "Epoch 595/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7978 - acc: 0.6990 - val_loss: 9.2065 - val_acc: 0.4242\n",
      "Epoch 596/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7912 - acc: 0.7017 - val_loss: 9.2052 - val_acc: 0.4238\n",
      "Epoch 597/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7955 - acc: 0.6970 - val_loss: 9.1983 - val_acc: 0.4258\n",
      "Epoch 598/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7878 - acc: 0.7018 - val_loss: 9.1989 - val_acc: 0.4269\n",
      "Epoch 599/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7960 - acc: 0.6974 - val_loss: 9.2063 - val_acc: 0.4256\n",
      "Epoch 600/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7981 - acc: 0.6979 - val_loss: 9.2035 - val_acc: 0.4264\n",
      "Epoch 601/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7912 - acc: 0.7027 - val_loss: 9.1999 - val_acc: 0.4249\n",
      "Epoch 602/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7834 - acc: 0.7037 - val_loss: 9.2109 - val_acc: 0.4258\n",
      "Epoch 603/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7849 - acc: 0.7041 - val_loss: 9.2245 - val_acc: 0.4219\n",
      "Epoch 604/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7780 - acc: 0.7076 - val_loss: 9.2169 - val_acc: 0.4240\n",
      "Epoch 605/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7860 - acc: 0.7036 - val_loss: 9.2117 - val_acc: 0.4264\n",
      "Epoch 606/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7918 - acc: 0.6992 - val_loss: 9.2151 - val_acc: 0.4249\n",
      "Epoch 607/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.8014 - acc: 0.6948 - val_loss: 9.1978 - val_acc: 0.4229\n",
      "Epoch 608/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8138 - acc: 0.6911 - val_loss: 9.1954 - val_acc: 0.4251\n",
      "Epoch 609/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7877 - acc: 0.7012 - val_loss: 9.2024 - val_acc: 0.4249\n",
      "Epoch 610/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7902 - acc: 0.7020 - val_loss: 9.2106 - val_acc: 0.4238\n",
      "Epoch 611/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7931 - acc: 0.6977 - val_loss: 9.2030 - val_acc: 0.4260\n",
      "Epoch 612/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7926 - acc: 0.6992 - val_loss: 9.2063 - val_acc: 0.4260\n",
      "Epoch 613/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7950 - acc: 0.6986 - val_loss: 9.2084 - val_acc: 0.4264\n",
      "Epoch 614/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7893 - acc: 0.7014 - val_loss: 9.2012 - val_acc: 0.4271\n",
      "Epoch 615/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7924 - acc: 0.7011 - val_loss: 9.2065 - val_acc: 0.4258\n",
      "Epoch 616/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7788 - acc: 0.7043 - val_loss: 9.2056 - val_acc: 0.4245\n",
      "Epoch 617/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7798 - acc: 0.7063 - val_loss: 9.2094 - val_acc: 0.4249\n",
      "Epoch 618/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7796 - acc: 0.7071 - val_loss: 9.2037 - val_acc: 0.4256\n",
      "Epoch 619/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7726 - acc: 0.7089 - val_loss: 9.2024 - val_acc: 0.4262\n",
      "Epoch 620/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7897 - acc: 0.7025 - val_loss: 9.2012 - val_acc: 0.4264\n",
      "Epoch 621/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7792 - acc: 0.7032 - val_loss: 9.2033 - val_acc: 0.4271\n",
      "Epoch 622/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7796 - acc: 0.7054 - val_loss: 9.2084 - val_acc: 0.4260\n",
      "Epoch 623/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7789 - acc: 0.7038 - val_loss: 9.2037 - val_acc: 0.4262\n",
      "Epoch 624/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7798 - acc: 0.7068 - val_loss: 9.2009 - val_acc: 0.4264\n",
      "Epoch 625/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7787 - acc: 0.7069 - val_loss: 9.2046 - val_acc: 0.4266\n",
      "Epoch 626/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7845 - acc: 0.7030 - val_loss: 9.2071 - val_acc: 0.4269\n",
      "Epoch 627/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7922 - acc: 0.7014 - val_loss: 9.2079 - val_acc: 0.4269\n",
      "Epoch 628/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7988 - acc: 0.7003 - val_loss: 9.2094 - val_acc: 0.4264\n",
      "Epoch 629/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7974 - acc: 0.6989 - val_loss: 9.2169 - val_acc: 0.4264\n",
      "Epoch 630/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7817 - acc: 0.7073 - val_loss: 9.2111 - val_acc: 0.4262\n",
      "Epoch 631/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7894 - acc: 0.7022 - val_loss: 9.2121 - val_acc: 0.4269\n",
      "Epoch 632/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7695 - acc: 0.7115 - val_loss: 9.2168 - val_acc: 0.4258\n",
      "Epoch 633/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7895 - acc: 0.7043 - val_loss: 9.2107 - val_acc: 0.4266\n",
      "Epoch 634/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7787 - acc: 0.7066 - val_loss: 9.2113 - val_acc: 0.4264\n",
      "Epoch 635/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.8009 - acc: 0.6968 - val_loss: 9.2070 - val_acc: 0.4238\n",
      "Epoch 636/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7891 - acc: 0.7028 - val_loss: 9.2034 - val_acc: 0.4249\n",
      "Epoch 637/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7804 - acc: 0.7061 - val_loss: 9.2084 - val_acc: 0.4262\n",
      "Epoch 638/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7713 - acc: 0.7096 - val_loss: 9.2094 - val_acc: 0.4266\n",
      "Epoch 639/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7778 - acc: 0.7069 - val_loss: 9.2055 - val_acc: 0.4266\n",
      "Epoch 640/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7751 - acc: 0.7085 - val_loss: 9.2140 - val_acc: 0.4269\n",
      "Epoch 641/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7722 - acc: 0.7086 - val_loss: 9.2061 - val_acc: 0.4271\n",
      "Epoch 642/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7776 - acc: 0.7070 - val_loss: 9.2096 - val_acc: 0.4266\n",
      "Epoch 643/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7870 - acc: 0.7028 - val_loss: 9.2075 - val_acc: 0.4271\n",
      "Epoch 644/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7821 - acc: 0.7040 - val_loss: 9.2108 - val_acc: 0.4264\n",
      "Epoch 645/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7871 - acc: 0.7014 - val_loss: 9.2116 - val_acc: 0.4262\n",
      "Epoch 646/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7839 - acc: 0.7045 - val_loss: 9.2142 - val_acc: 0.4266\n",
      "Epoch 647/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7814 - acc: 0.7055 - val_loss: 9.1938 - val_acc: 0.4271\n",
      "Epoch 648/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7782 - acc: 0.7053 - val_loss: 9.2104 - val_acc: 0.4266\n",
      "Epoch 649/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7779 - acc: 0.7091 - val_loss: 9.2052 - val_acc: 0.4258\n",
      "Epoch 650/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7882 - acc: 0.7044 - val_loss: 9.2111 - val_acc: 0.4269\n",
      "Epoch 651/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7768 - acc: 0.7074 - val_loss: 9.2096 - val_acc: 0.4269\n",
      "Epoch 652/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7846 - acc: 0.7060 - val_loss: 9.2099 - val_acc: 0.4269\n",
      "Epoch 653/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7856 - acc: 0.7042 - val_loss: 9.2049 - val_acc: 0.4271\n",
      "Epoch 654/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7654 - acc: 0.7139 - val_loss: 9.2070 - val_acc: 0.4264\n",
      "Epoch 655/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7721 - acc: 0.7101 - val_loss: 9.2029 - val_acc: 0.4266\n",
      "Epoch 656/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7787 - acc: 0.7059 - val_loss: 9.2083 - val_acc: 0.4260\n",
      "Epoch 657/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7775 - acc: 0.7054 - val_loss: 9.2019 - val_acc: 0.4258\n",
      "Epoch 658/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7934 - acc: 0.7001 - val_loss: 9.2029 - val_acc: 0.4262\n",
      "Epoch 659/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7886 - acc: 0.7021 - val_loss: 9.2097 - val_acc: 0.4262\n",
      "Epoch 660/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7872 - acc: 0.7012 - val_loss: 9.2139 - val_acc: 0.4249\n",
      "Epoch 661/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7872 - acc: 0.7032 - val_loss: 9.2074 - val_acc: 0.4256\n",
      "Epoch 662/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7831 - acc: 0.7023 - val_loss: 9.1981 - val_acc: 0.4266\n",
      "Epoch 663/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7773 - acc: 0.7057 - val_loss: 9.2080 - val_acc: 0.4253\n",
      "Epoch 664/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7777 - acc: 0.7071 - val_loss: 9.2052 - val_acc: 0.4271\n",
      "Epoch 665/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7801 - acc: 0.7042 - val_loss: 9.1964 - val_acc: 0.4264\n",
      "Epoch 666/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7891 - acc: 0.7021 - val_loss: 9.2099 - val_acc: 0.4264\n",
      "Epoch 667/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7697 - acc: 0.7093 - val_loss: 9.2060 - val_acc: 0.4262\n",
      "Epoch 668/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7778 - acc: 0.7060 - val_loss: 9.2081 - val_acc: 0.4262\n",
      "Epoch 669/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7820 - acc: 0.7027 - val_loss: 9.2131 - val_acc: 0.4262\n",
      "Epoch 670/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7839 - acc: 0.7030 - val_loss: 9.2124 - val_acc: 0.4258\n",
      "Epoch 671/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7641 - acc: 0.7113 - val_loss: 9.2097 - val_acc: 0.4253\n",
      "Epoch 672/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7863 - acc: 0.7043 - val_loss: 9.2075 - val_acc: 0.4245\n",
      "Epoch 673/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7758 - acc: 0.7077 - val_loss: 9.2126 - val_acc: 0.4253\n",
      "Epoch 674/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7781 - acc: 0.7069 - val_loss: 9.2135 - val_acc: 0.4251\n",
      "Epoch 675/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7793 - acc: 0.7038 - val_loss: 9.1970 - val_acc: 0.4256\n",
      "Epoch 676/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7891 - acc: 0.7014 - val_loss: 9.2066 - val_acc: 0.4262\n",
      "Epoch 677/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7828 - acc: 0.7057 - val_loss: 9.2111 - val_acc: 0.4253\n",
      "Epoch 678/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.7772 - acc: 0.7055 - val_loss: 9.2046 - val_acc: 0.4249\n",
      "Epoch 679/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.7874 - acc: 0.7016 - val_loss: 9.2060 - val_acc: 0.4249\n",
      "Epoch 680/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7835 - acc: 0.7016 - val_loss: 9.2124 - val_acc: 0.4256\n",
      "Epoch 681/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7788 - acc: 0.7059 - val_loss: 9.2024 - val_acc: 0.4260\n",
      "Epoch 682/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7772 - acc: 0.7028 - val_loss: 9.2055 - val_acc: 0.4264\n",
      "Epoch 683/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7832 - acc: 0.7023 - val_loss: 9.1979 - val_acc: 0.4260\n",
      "Epoch 684/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7856 - acc: 0.7050 - val_loss: 9.2025 - val_acc: 0.4251\n",
      "Epoch 685/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7907 - acc: 0.7037 - val_loss: 9.1998 - val_acc: 0.4258\n",
      "Epoch 686/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7874 - acc: 0.7007 - val_loss: 9.2071 - val_acc: 0.4262\n",
      "Epoch 687/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7812 - acc: 0.7045 - val_loss: 9.2138 - val_acc: 0.4260\n",
      "Epoch 688/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7807 - acc: 0.7029 - val_loss: 9.2046 - val_acc: 0.4271\n",
      "Epoch 689/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7880 - acc: 0.7035 - val_loss: 9.2115 - val_acc: 0.4238\n",
      "Epoch 690/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7789 - acc: 0.7044 - val_loss: 9.2163 - val_acc: 0.4247\n",
      "Epoch 691/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7818 - acc: 0.7066 - val_loss: 9.2183 - val_acc: 0.4249\n",
      "Epoch 692/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7861 - acc: 0.7037 - val_loss: 9.2118 - val_acc: 0.4266\n",
      "Epoch 693/1000\n",
      "41259/41259 [==============================] - 76s 2ms/step - loss: 0.7921 - acc: 0.6990 - val_loss: 9.2165 - val_acc: 0.4269\n",
      "Epoch 694/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7930 - acc: 0.7004 - val_loss: 9.2140 - val_acc: 0.4242\n",
      "Epoch 695/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7935 - acc: 0.7016 - val_loss: 9.2065 - val_acc: 0.4266\n",
      "Epoch 696/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.8028 - acc: 0.6966 - val_loss: 9.2118 - val_acc: 0.4249\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7758 - acc: 0.7069 - val_loss: 9.1959 - val_acc: 0.4260\n",
      "Epoch 698/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7747 - acc: 0.7062 - val_loss: 9.2094 - val_acc: 0.4249\n",
      "Epoch 699/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7780 - acc: 0.7075 - val_loss: 9.2075 - val_acc: 0.4245\n",
      "Epoch 700/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7808 - acc: 0.7058 - val_loss: 9.2080 - val_acc: 0.4236\n",
      "Epoch 701/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7895 - acc: 0.7020 - val_loss: 9.2067 - val_acc: 0.4251\n",
      "Epoch 702/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7816 - acc: 0.7051 - val_loss: 9.2165 - val_acc: 0.4249\n",
      "Epoch 703/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7785 - acc: 0.7072 - val_loss: 9.2116 - val_acc: 0.4242\n",
      "Epoch 704/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7788 - acc: 0.7078 - val_loss: 9.2158 - val_acc: 0.4260\n",
      "Epoch 705/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7795 - acc: 0.7070 - val_loss: 9.2117 - val_acc: 0.4260\n",
      "Epoch 706/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7931 - acc: 0.7000 - val_loss: 9.2172 - val_acc: 0.4247\n",
      "Epoch 707/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7885 - acc: 0.7019 - val_loss: 9.2116 - val_acc: 0.4266\n",
      "Epoch 708/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7983 - acc: 0.6982 - val_loss: 9.2159 - val_acc: 0.4253\n",
      "Epoch 709/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7813 - acc: 0.7044 - val_loss: 9.2092 - val_acc: 0.4260\n",
      "Epoch 710/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7917 - acc: 0.6996 - val_loss: 9.2109 - val_acc: 0.4260\n",
      "Epoch 711/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7814 - acc: 0.7046 - val_loss: 9.2168 - val_acc: 0.4249\n",
      "Epoch 712/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7938 - acc: 0.6974 - val_loss: 9.2128 - val_acc: 0.4247\n",
      "Epoch 713/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.8057 - acc: 0.6939 - val_loss: 9.2045 - val_acc: 0.4260\n",
      "Epoch 714/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7848 - acc: 0.7038 - val_loss: 9.2054 - val_acc: 0.4271\n",
      "Epoch 715/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7908 - acc: 0.7007 - val_loss: 9.2185 - val_acc: 0.4249\n",
      "Epoch 716/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7690 - acc: 0.7091 - val_loss: 9.2122 - val_acc: 0.4247\n",
      "Epoch 717/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7889 - acc: 0.7008 - val_loss: 9.2248 - val_acc: 0.4225\n",
      "Epoch 718/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7745 - acc: 0.7070 - val_loss: 9.2242 - val_acc: 0.4232\n",
      "Epoch 719/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7761 - acc: 0.7052 - val_loss: 9.2098 - val_acc: 0.4251\n",
      "Epoch 720/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7847 - acc: 0.7033 - val_loss: 9.2197 - val_acc: 0.4245\n",
      "Epoch 721/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7844 - acc: 0.7049 - val_loss: 9.2225 - val_acc: 0.4232\n",
      "Epoch 722/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7789 - acc: 0.7050 - val_loss: 9.2201 - val_acc: 0.4247\n",
      "Epoch 723/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7686 - acc: 0.7130 - val_loss: 9.2273 - val_acc: 0.4232\n",
      "Epoch 724/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7765 - acc: 0.7065 - val_loss: 9.2268 - val_acc: 0.4223\n",
      "Epoch 725/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7778 - acc: 0.7055 - val_loss: 9.2219 - val_acc: 0.4236\n",
      "Epoch 726/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7714 - acc: 0.7068 - val_loss: 9.2099 - val_acc: 0.4234\n",
      "Epoch 727/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7809 - acc: 0.7013 - val_loss: 9.2130 - val_acc: 0.4225\n",
      "Epoch 728/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7798 - acc: 0.7046 - val_loss: 9.2253 - val_acc: 0.4232\n",
      "Epoch 729/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7802 - acc: 0.7026 - val_loss: 9.2451 - val_acc: 0.4205\n",
      "Epoch 730/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7703 - acc: 0.7083 - val_loss: 9.2205 - val_acc: 0.4227\n",
      "Epoch 731/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7749 - acc: 0.7094 - val_loss: 9.2098 - val_acc: 0.4249\n",
      "Epoch 732/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7689 - acc: 0.7094 - val_loss: 9.2168 - val_acc: 0.4234\n",
      "Epoch 733/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7651 - acc: 0.7110 - val_loss: 9.2080 - val_acc: 0.4245\n",
      "Epoch 734/1000\n",
      "41259/41259 [==============================] - 75s 2ms/step - loss: 0.7699 - acc: 0.7090 - val_loss: 9.2160 - val_acc: 0.4242\n",
      "Epoch 735/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.7765 - acc: 0.7073 - val_loss: 9.2061 - val_acc: 0.4236\n",
      "Epoch 736/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.7776 - acc: 0.7039 - val_loss: 9.2130 - val_acc: 0.4229\n",
      "Epoch 737/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.7637 - acc: 0.7105 - val_loss: 9.2200 - val_acc: 0.4236\n",
      "Epoch 738/1000\n",
      "41259/41259 [==============================] - 77s 2ms/step - loss: 0.7643 - acc: 0.7102 - val_loss: 9.2262 - val_acc: 0.4247\n",
      "Epoch 739/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7733 - acc: 0.7093 - val_loss: 9.2175 - val_acc: 0.4234\n",
      "Epoch 740/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7661 - acc: 0.7127 - val_loss: 9.2143 - val_acc: 0.4249\n",
      "Epoch 741/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7771 - acc: 0.7076 - val_loss: 9.2304 - val_acc: 0.4249\n",
      "Epoch 742/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7706 - acc: 0.7071 - val_loss: 9.2248 - val_acc: 0.4238\n",
      "Epoch 743/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7774 - acc: 0.7051 - val_loss: 9.2162 - val_acc: 0.4236\n",
      "Epoch 744/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7731 - acc: 0.7077 - val_loss: 9.2170 - val_acc: 0.4234\n",
      "Epoch 745/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7729 - acc: 0.7081 - val_loss: 9.2168 - val_acc: 0.4227\n",
      "Epoch 746/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7768 - acc: 0.7074 - val_loss: 9.2125 - val_acc: 0.4256\n",
      "Epoch 747/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7731 - acc: 0.7085 - val_loss: 9.2068 - val_acc: 0.4249\n",
      "Epoch 748/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7582 - acc: 0.7134 - val_loss: 9.2161 - val_acc: 0.4245\n",
      "Epoch 749/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7680 - acc: 0.7089 - val_loss: 9.2045 - val_acc: 0.4236\n",
      "Epoch 750/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7702 - acc: 0.7100 - val_loss: 9.2137 - val_acc: 0.4249\n",
      "Epoch 751/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7707 - acc: 0.7106 - val_loss: 9.2150 - val_acc: 0.4242\n",
      "Epoch 752/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7691 - acc: 0.7086 - val_loss: 9.2070 - val_acc: 0.4247\n",
      "Epoch 753/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7585 - acc: 0.7131 - val_loss: 9.2064 - val_acc: 0.4247\n",
      "Epoch 754/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7671 - acc: 0.7086 - val_loss: 9.2072 - val_acc: 0.4258\n",
      "Epoch 755/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7555 - acc: 0.7154 - val_loss: 9.2064 - val_acc: 0.4236\n",
      "Epoch 756/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7734 - acc: 0.7076 - val_loss: 9.2140 - val_acc: 0.4247\n",
      "Epoch 757/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7674 - acc: 0.7103 - val_loss: 9.2100 - val_acc: 0.4262\n",
      "Epoch 758/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7675 - acc: 0.7101 - val_loss: 9.2091 - val_acc: 0.4247\n",
      "Epoch 759/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7669 - acc: 0.7135 - val_loss: 9.2135 - val_acc: 0.4245\n",
      "Epoch 760/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7655 - acc: 0.7092 - val_loss: 9.2143 - val_acc: 0.4249\n",
      "Epoch 761/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7671 - acc: 0.7101 - val_loss: 9.2079 - val_acc: 0.4242\n",
      "Epoch 762/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7572 - acc: 0.7157 - val_loss: 9.2188 - val_acc: 0.4242\n",
      "Epoch 763/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7598 - acc: 0.7155 - val_loss: 9.2158 - val_acc: 0.4251\n",
      "Epoch 764/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7522 - acc: 0.7168 - val_loss: 9.2166 - val_acc: 0.4234\n",
      "Epoch 765/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7669 - acc: 0.7115 - val_loss: 9.2224 - val_acc: 0.4234\n",
      "Epoch 766/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7823 - acc: 0.7059 - val_loss: 9.2067 - val_acc: 0.4234\n",
      "Epoch 767/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7951 - acc: 0.6970 - val_loss: 9.2152 - val_acc: 0.4253\n",
      "Epoch 768/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7997 - acc: 0.6982 - val_loss: 9.2249 - val_acc: 0.4240\n",
      "Epoch 769/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7775 - acc: 0.7073 - val_loss: 9.2092 - val_acc: 0.4256\n",
      "Epoch 770/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7581 - acc: 0.7138 - val_loss: 9.2269 - val_acc: 0.4247\n",
      "Epoch 771/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7740 - acc: 0.7066 - val_loss: 9.2106 - val_acc: 0.4251\n",
      "Epoch 772/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7708 - acc: 0.7119 - val_loss: 9.2142 - val_acc: 0.4236\n",
      "Epoch 773/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7699 - acc: 0.7088 - val_loss: 9.2251 - val_acc: 0.4234\n",
      "Epoch 774/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7638 - acc: 0.7125 - val_loss: 9.2201 - val_acc: 0.4238\n",
      "Epoch 775/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7672 - acc: 0.7093 - val_loss: 9.2128 - val_acc: 0.4249\n",
      "Epoch 776/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7757 - acc: 0.7077 - val_loss: 9.2168 - val_acc: 0.4247\n",
      "Epoch 777/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7625 - acc: 0.7118 - val_loss: 9.2224 - val_acc: 0.4242\n",
      "Epoch 778/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7699 - acc: 0.7077 - val_loss: 9.2262 - val_acc: 0.4236\n",
      "Epoch 779/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7764 - acc: 0.7062 - val_loss: 9.2184 - val_acc: 0.4245\n",
      "Epoch 780/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7592 - acc: 0.7118 - val_loss: 9.2247 - val_acc: 0.4232\n",
      "Epoch 781/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7589 - acc: 0.7131 - val_loss: 9.2215 - val_acc: 0.4232\n",
      "Epoch 782/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7688 - acc: 0.7125 - val_loss: 9.2188 - val_acc: 0.4247\n",
      "Epoch 783/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7702 - acc: 0.7096 - val_loss: 9.2263 - val_acc: 0.4216\n",
      "Epoch 784/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7767 - acc: 0.7047 - val_loss: 9.2120 - val_acc: 0.4251\n",
      "Epoch 785/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7577 - acc: 0.7141 - val_loss: 9.2159 - val_acc: 0.4253\n",
      "Epoch 786/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7725 - acc: 0.7090 - val_loss: 9.2204 - val_acc: 0.4253\n",
      "Epoch 787/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7617 - acc: 0.7147 - val_loss: 9.2422 - val_acc: 0.4223\n",
      "Epoch 788/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7676 - acc: 0.7107 - val_loss: 9.2195 - val_acc: 0.4247\n",
      "Epoch 789/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7785 - acc: 0.7049 - val_loss: 9.2169 - val_acc: 0.4245\n",
      "Epoch 790/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7586 - acc: 0.7146 - val_loss: 9.2125 - val_acc: 0.4242\n",
      "Epoch 791/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7585 - acc: 0.7141 - val_loss: 9.2153 - val_acc: 0.4253\n",
      "Epoch 792/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7666 - acc: 0.7121 - val_loss: 9.2096 - val_acc: 0.4245\n",
      "Epoch 793/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7635 - acc: 0.7099 - val_loss: 9.2206 - val_acc: 0.4238\n",
      "Epoch 794/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7766 - acc: 0.7061 - val_loss: 9.2139 - val_acc: 0.4245\n",
      "Epoch 795/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7510 - acc: 0.7172 - val_loss: 9.2093 - val_acc: 0.4242\n",
      "Epoch 796/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7487 - acc: 0.7200 - val_loss: 9.2074 - val_acc: 0.4249\n",
      "Epoch 797/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7655 - acc: 0.7098 - val_loss: 9.2238 - val_acc: 0.4236\n",
      "Epoch 798/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7513 - acc: 0.7173 - val_loss: 9.2217 - val_acc: 0.4238\n",
      "Epoch 799/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7496 - acc: 0.7188 - val_loss: 9.2090 - val_acc: 0.4245\n",
      "Epoch 800/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7744 - acc: 0.7075 - val_loss: 9.2192 - val_acc: 0.4245\n",
      "Epoch 801/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7705 - acc: 0.7075 - val_loss: 9.2236 - val_acc: 0.4256\n",
      "Epoch 802/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7609 - acc: 0.7139 - val_loss: 9.2274 - val_acc: 0.4232\n",
      "Epoch 803/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7471 - acc: 0.7199 - val_loss: 9.2359 - val_acc: 0.4221\n",
      "Epoch 804/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7641 - acc: 0.7109 - val_loss: 9.2014 - val_acc: 0.4251\n",
      "Epoch 805/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7589 - acc: 0.7161 - val_loss: 9.2218 - val_acc: 0.4236\n",
      "Epoch 806/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7644 - acc: 0.7113 - val_loss: 9.1969 - val_acc: 0.4251\n",
      "Epoch 807/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7611 - acc: 0.7141 - val_loss: 9.2155 - val_acc: 0.4256\n",
      "Epoch 808/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7620 - acc: 0.7147 - val_loss: 9.2131 - val_acc: 0.4249\n",
      "Epoch 809/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7587 - acc: 0.7136 - val_loss: 9.2203 - val_acc: 0.4253\n",
      "Epoch 810/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7725 - acc: 0.7102 - val_loss: 9.2213 - val_acc: 0.4249\n",
      "Epoch 811/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7534 - acc: 0.7163 - val_loss: 9.2123 - val_acc: 0.4245\n",
      "Epoch 812/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7555 - acc: 0.7164 - val_loss: 9.2176 - val_acc: 0.4251\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7707 - acc: 0.7119 - val_loss: 9.2116 - val_acc: 0.4247\n",
      "Epoch 814/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7727 - acc: 0.7096 - val_loss: 9.2151 - val_acc: 0.4260\n",
      "Epoch 815/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7700 - acc: 0.7087 - val_loss: 9.2202 - val_acc: 0.4260\n",
      "Epoch 816/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7597 - acc: 0.7138 - val_loss: 9.2121 - val_acc: 0.4251\n",
      "Epoch 817/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7704 - acc: 0.7107 - val_loss: 9.2193 - val_acc: 0.4249\n",
      "Epoch 818/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7560 - acc: 0.7182 - val_loss: 9.2150 - val_acc: 0.4251\n",
      "Epoch 819/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7432 - acc: 0.7215 - val_loss: 9.2094 - val_acc: 0.4251\n",
      "Epoch 820/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7621 - acc: 0.7138 - val_loss: 9.2214 - val_acc: 0.4247\n",
      "Epoch 821/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7501 - acc: 0.7180 - val_loss: 9.2025 - val_acc: 0.4269\n",
      "Epoch 822/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7609 - acc: 0.7139 - val_loss: 9.2100 - val_acc: 0.4262\n",
      "Epoch 823/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7513 - acc: 0.7192 - val_loss: 9.2147 - val_acc: 0.4262\n",
      "Epoch 824/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7485 - acc: 0.7204 - val_loss: 9.2006 - val_acc: 0.4240\n",
      "Epoch 825/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7419 - acc: 0.7199 - val_loss: 9.2208 - val_acc: 0.4245\n",
      "Epoch 826/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7553 - acc: 0.7147 - val_loss: 9.2187 - val_acc: 0.4266\n",
      "Epoch 827/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7570 - acc: 0.7178 - val_loss: 9.2252 - val_acc: 0.4251\n",
      "Epoch 828/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7619 - acc: 0.7125 - val_loss: 9.2239 - val_acc: 0.4258\n",
      "Epoch 829/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7454 - acc: 0.7230 - val_loss: 9.2177 - val_acc: 0.4258\n",
      "Epoch 830/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7382 - acc: 0.7254 - val_loss: 9.2153 - val_acc: 0.4266\n",
      "Epoch 831/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7492 - acc: 0.7194 - val_loss: 9.2095 - val_acc: 0.4266\n",
      "Epoch 832/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7467 - acc: 0.7195 - val_loss: 9.2087 - val_acc: 0.4258\n",
      "Epoch 833/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7401 - acc: 0.7214 - val_loss: 9.2057 - val_acc: 0.4253\n",
      "Epoch 834/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7481 - acc: 0.7205 - val_loss: 9.2170 - val_acc: 0.4238\n",
      "Epoch 835/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7377 - acc: 0.7240 - val_loss: 9.2227 - val_acc: 0.4262\n",
      "Epoch 836/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7556 - acc: 0.7152 - val_loss: 9.2190 - val_acc: 0.4253\n",
      "Epoch 837/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7493 - acc: 0.7184 - val_loss: 9.2178 - val_acc: 0.4253\n",
      "Epoch 838/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7488 - acc: 0.7203 - val_loss: 9.2236 - val_acc: 0.4256\n",
      "Epoch 839/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7580 - acc: 0.7158 - val_loss: 9.2182 - val_acc: 0.4256\n",
      "Epoch 840/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7591 - acc: 0.7167 - val_loss: 9.2185 - val_acc: 0.4258\n",
      "Epoch 841/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7576 - acc: 0.7159 - val_loss: 9.2177 - val_acc: 0.4266\n",
      "Epoch 842/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7404 - acc: 0.7216 - val_loss: 9.2214 - val_acc: 0.4262\n",
      "Epoch 843/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7552 - acc: 0.7153 - val_loss: 9.2165 - val_acc: 0.4262\n",
      "Epoch 844/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7692 - acc: 0.7109 - val_loss: 9.2142 - val_acc: 0.4269\n",
      "Epoch 845/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7420 - acc: 0.7228 - val_loss: 9.2086 - val_acc: 0.4251\n",
      "Epoch 846/1000\n",
      "41259/41259 [==============================] - 73s 2ms/step - loss: 0.7564 - acc: 0.7165 - val_loss: 9.2064 - val_acc: 0.4258\n",
      "Epoch 847/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7557 - acc: 0.7202 - val_loss: 9.2197 - val_acc: 0.4249\n",
      "Epoch 848/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7566 - acc: 0.7158 - val_loss: 9.2212 - val_acc: 0.4258\n",
      "Epoch 849/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7375 - acc: 0.7231 - val_loss: 9.2260 - val_acc: 0.4258\n",
      "Epoch 850/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7504 - acc: 0.7186 - val_loss: 9.2292 - val_acc: 0.4260\n",
      "Epoch 851/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7503 - acc: 0.7194 - val_loss: 9.2231 - val_acc: 0.4260\n",
      "Epoch 852/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7634 - acc: 0.7143 - val_loss: 9.2317 - val_acc: 0.4245\n",
      "Epoch 853/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7536 - acc: 0.7162 - val_loss: 9.2281 - val_acc: 0.4256\n",
      "Epoch 854/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7503 - acc: 0.7182 - val_loss: 9.2226 - val_acc: 0.4247\n",
      "Epoch 855/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7404 - acc: 0.7238 - val_loss: 9.2192 - val_acc: 0.4258\n",
      "Epoch 856/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7325 - acc: 0.7257 - val_loss: 9.2258 - val_acc: 0.4251\n",
      "Epoch 857/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7403 - acc: 0.7223 - val_loss: 9.2338 - val_acc: 0.4245\n",
      "Epoch 858/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7515 - acc: 0.7180 - val_loss: 9.2230 - val_acc: 0.4256\n",
      "Epoch 859/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7469 - acc: 0.7206 - val_loss: 9.2192 - val_acc: 0.4264\n",
      "Epoch 860/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7577 - acc: 0.7159 - val_loss: 9.2282 - val_acc: 0.4251\n",
      "Epoch 861/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7449 - acc: 0.7205 - val_loss: 9.2214 - val_acc: 0.4262\n",
      "Epoch 862/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7431 - acc: 0.7219 - val_loss: 9.2125 - val_acc: 0.4245\n",
      "Epoch 863/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7425 - acc: 0.7235 - val_loss: 9.2236 - val_acc: 0.4262\n",
      "Epoch 864/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7466 - acc: 0.7225 - val_loss: 9.2198 - val_acc: 0.4256\n",
      "Epoch 865/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7440 - acc: 0.7214 - val_loss: 9.2260 - val_acc: 0.4249\n",
      "Epoch 866/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7732 - acc: 0.7077 - val_loss: 9.2215 - val_acc: 0.4258\n",
      "Epoch 867/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7587 - acc: 0.7152 - val_loss: 9.2189 - val_acc: 0.4253\n",
      "Epoch 868/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7677 - acc: 0.7098 - val_loss: 9.2141 - val_acc: 0.4258\n",
      "Epoch 869/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7602 - acc: 0.7139 - val_loss: 9.2149 - val_acc: 0.4264\n",
      "Epoch 870/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7607 - acc: 0.7139 - val_loss: 9.2272 - val_acc: 0.4251\n",
      "Epoch 871/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7669 - acc: 0.7114 - val_loss: 9.2160 - val_acc: 0.4245\n",
      "Epoch 872/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7608 - acc: 0.7129 - val_loss: 9.2168 - val_acc: 0.4247\n",
      "Epoch 873/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7573 - acc: 0.7135 - val_loss: 9.2231 - val_acc: 0.4247\n",
      "Epoch 874/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7531 - acc: 0.7167 - val_loss: 9.2185 - val_acc: 0.4242\n",
      "Epoch 875/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7625 - acc: 0.7135 - val_loss: 9.2230 - val_acc: 0.4253\n",
      "Epoch 876/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7666 - acc: 0.7108 - val_loss: 9.2203 - val_acc: 0.4251\n",
      "Epoch 877/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7407 - acc: 0.7230 - val_loss: 9.2288 - val_acc: 0.4251\n",
      "Epoch 878/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7550 - acc: 0.7151 - val_loss: 9.2208 - val_acc: 0.4260\n",
      "Epoch 879/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7492 - acc: 0.7198 - val_loss: 9.2209 - val_acc: 0.4258\n",
      "Epoch 880/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7435 - acc: 0.7209 - val_loss: 9.2192 - val_acc: 0.4253\n",
      "Epoch 881/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7520 - acc: 0.7167 - val_loss: 9.2221 - val_acc: 0.4253\n",
      "Epoch 882/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7613 - acc: 0.7158 - val_loss: 9.2162 - val_acc: 0.4260\n",
      "Epoch 883/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7521 - acc: 0.7181 - val_loss: 9.2218 - val_acc: 0.4258\n",
      "Epoch 884/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7554 - acc: 0.7190 - val_loss: 9.2122 - val_acc: 0.4249\n",
      "Epoch 885/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7580 - acc: 0.7141 - val_loss: 9.2088 - val_acc: 0.4256\n",
      "Epoch 886/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7731 - acc: 0.7081 - val_loss: 9.2028 - val_acc: 0.4260\n",
      "Epoch 887/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7683 - acc: 0.7092 - val_loss: 9.2060 - val_acc: 0.4262\n",
      "Epoch 888/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7584 - acc: 0.7157 - val_loss: 9.2112 - val_acc: 0.4260\n",
      "Epoch 889/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7611 - acc: 0.7157 - val_loss: 9.2186 - val_acc: 0.4253\n",
      "Epoch 890/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7630 - acc: 0.7134 - val_loss: 9.2164 - val_acc: 0.4260\n",
      "Epoch 891/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7566 - acc: 0.7148 - val_loss: 9.2202 - val_acc: 0.4256\n",
      "Epoch 892/1000\n",
      "41259/41259 [==============================] - 74s 2ms/step - loss: 0.7580 - acc: 0.7133 - val_loss: 9.2140 - val_acc: 0.4269\n",
      "Epoch 893/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7542 - acc: 0.7195 - val_loss: 9.2222 - val_acc: 0.4260\n",
      "Epoch 894/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7533 - acc: 0.7165 - val_loss: 9.2196 - val_acc: 0.4262\n",
      "Epoch 895/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7419 - acc: 0.7210 - val_loss: 9.2153 - val_acc: 0.4262\n",
      "Epoch 896/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7568 - acc: 0.7167 - val_loss: 9.2161 - val_acc: 0.4266\n",
      "Epoch 897/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7610 - acc: 0.7139 - val_loss: 9.2186 - val_acc: 0.4260\n",
      "Epoch 898/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7653 - acc: 0.7133 - val_loss: 9.2102 - val_acc: 0.4256\n",
      "Epoch 899/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7520 - acc: 0.7181 - val_loss: 9.2160 - val_acc: 0.4269\n",
      "Epoch 900/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7515 - acc: 0.7159 - val_loss: 9.2130 - val_acc: 0.4256\n",
      "Epoch 901/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7675 - acc: 0.7093 - val_loss: 9.2103 - val_acc: 0.4264\n",
      "Epoch 902/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7700 - acc: 0.7080 - val_loss: 9.2074 - val_acc: 0.4256\n",
      "Epoch 903/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7682 - acc: 0.7116 - val_loss: 9.2061 - val_acc: 0.4264\n",
      "Epoch 904/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7564 - acc: 0.7172 - val_loss: 9.2105 - val_acc: 0.4258\n",
      "Epoch 905/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7571 - acc: 0.7145 - val_loss: 9.2137 - val_acc: 0.4264\n",
      "Epoch 906/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7438 - acc: 0.7205 - val_loss: 9.2139 - val_acc: 0.4260\n",
      "Epoch 907/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7503 - acc: 0.7193 - val_loss: 9.2134 - val_acc: 0.4266\n",
      "Epoch 908/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7429 - acc: 0.7204 - val_loss: 9.2049 - val_acc: 0.4266\n",
      "Epoch 909/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7503 - acc: 0.7184 - val_loss: 9.2099 - val_acc: 0.4260\n",
      "Epoch 910/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7566 - acc: 0.7159 - val_loss: 9.2161 - val_acc: 0.4256\n",
      "Epoch 911/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7443 - acc: 0.7199 - val_loss: 9.2212 - val_acc: 0.4253\n",
      "Epoch 912/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7713 - acc: 0.7109 - val_loss: 9.2220 - val_acc: 0.4245\n",
      "Epoch 913/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7699 - acc: 0.7111 - val_loss: 9.2126 - val_acc: 0.4253\n",
      "Epoch 914/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7544 - acc: 0.7171 - val_loss: 9.2192 - val_acc: 0.4247\n",
      "Epoch 915/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7652 - acc: 0.7140 - val_loss: 9.2142 - val_acc: 0.4271\n",
      "Epoch 916/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7497 - acc: 0.7191 - val_loss: 9.2142 - val_acc: 0.4260\n",
      "Epoch 917/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7643 - acc: 0.7153 - val_loss: 9.2085 - val_acc: 0.4258\n",
      "Epoch 918/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7647 - acc: 0.7145 - val_loss: 9.2107 - val_acc: 0.4266\n",
      "Epoch 919/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7583 - acc: 0.7154 - val_loss: 9.2109 - val_acc: 0.4256\n",
      "Epoch 920/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7697 - acc: 0.7123 - val_loss: 9.2108 - val_acc: 0.4258\n",
      "Epoch 921/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7560 - acc: 0.7156 - val_loss: 9.2093 - val_acc: 0.4249\n",
      "Epoch 922/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7618 - acc: 0.7135 - val_loss: 9.2116 - val_acc: 0.4264\n",
      "Epoch 923/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7695 - acc: 0.7130 - val_loss: 9.2151 - val_acc: 0.4260\n",
      "Epoch 924/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7700 - acc: 0.7115 - val_loss: 9.2151 - val_acc: 0.4256\n",
      "Epoch 925/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7532 - acc: 0.7190 - val_loss: 9.2099 - val_acc: 0.4266\n",
      "Epoch 926/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7413 - acc: 0.7223 - val_loss: 9.2119 - val_acc: 0.4269\n",
      "Epoch 927/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7561 - acc: 0.7172 - val_loss: 9.2050 - val_acc: 0.4258\n",
      "Epoch 928/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7607 - acc: 0.7147 - val_loss: 9.2166 - val_acc: 0.4256\n",
      "Epoch 929/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7555 - acc: 0.7181 - val_loss: 9.2178 - val_acc: 0.4258\n",
      "Epoch 930/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7558 - acc: 0.7175 - val_loss: 9.2109 - val_acc: 0.4264\n",
      "Epoch 931/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7563 - acc: 0.7140 - val_loss: 9.2103 - val_acc: 0.4258\n",
      "Epoch 932/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7473 - acc: 0.7196 - val_loss: 9.2036 - val_acc: 0.4251\n",
      "Epoch 933/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7548 - acc: 0.7176 - val_loss: 9.2117 - val_acc: 0.4266\n",
      "Epoch 934/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7603 - acc: 0.7160 - val_loss: 9.2102 - val_acc: 0.4258\n",
      "Epoch 935/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7731 - acc: 0.7125 - val_loss: 9.2113 - val_acc: 0.4258\n",
      "Epoch 936/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7677 - acc: 0.7119 - val_loss: 9.2075 - val_acc: 0.4258\n",
      "Epoch 937/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7585 - acc: 0.7157 - val_loss: 9.2072 - val_acc: 0.4253\n",
      "Epoch 938/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7437 - acc: 0.7220 - val_loss: 9.2213 - val_acc: 0.4249\n",
      "Epoch 939/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7579 - acc: 0.7156 - val_loss: 9.2211 - val_acc: 0.4251\n",
      "Epoch 940/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7636 - acc: 0.7132 - val_loss: 9.2163 - val_acc: 0.4253\n",
      "Epoch 941/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7496 - acc: 0.7171 - val_loss: 9.2113 - val_acc: 0.4249\n",
      "Epoch 942/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7464 - acc: 0.7197 - val_loss: 9.2141 - val_acc: 0.4247\n",
      "Epoch 943/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7685 - acc: 0.7101 - val_loss: 9.2182 - val_acc: 0.4247\n",
      "Epoch 944/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7757 - acc: 0.7086 - val_loss: 9.2197 - val_acc: 0.4249\n",
      "Epoch 945/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7586 - acc: 0.7148 - val_loss: 9.2220 - val_acc: 0.4253\n",
      "Epoch 946/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7509 - acc: 0.7217 - val_loss: 9.2088 - val_acc: 0.4253\n",
      "Epoch 947/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7468 - acc: 0.7210 - val_loss: 9.2210 - val_acc: 0.4253\n",
      "Epoch 948/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7571 - acc: 0.7162 - val_loss: 9.2226 - val_acc: 0.4251\n",
      "Epoch 949/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7541 - acc: 0.7172 - val_loss: 9.2217 - val_acc: 0.4258\n",
      "Epoch 950/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7481 - acc: 0.7194 - val_loss: 9.2218 - val_acc: 0.4258\n",
      "Epoch 951/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7635 - acc: 0.7135 - val_loss: 9.2159 - val_acc: 0.4271\n",
      "Epoch 952/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7613 - acc: 0.7145 - val_loss: 9.2120 - val_acc: 0.4256\n",
      "Epoch 953/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7709 - acc: 0.7104 - val_loss: 9.2111 - val_acc: 0.4249\n",
      "Epoch 954/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7505 - acc: 0.7187 - val_loss: 9.2193 - val_acc: 0.4253\n",
      "Epoch 955/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7564 - acc: 0.7156 - val_loss: 9.2202 - val_acc: 0.4258\n",
      "Epoch 956/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7737 - acc: 0.7069 - val_loss: 9.2165 - val_acc: 0.4260\n",
      "Epoch 957/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7776 - acc: 0.7092 - val_loss: 9.2120 - val_acc: 0.4260\n",
      "Epoch 958/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7727 - acc: 0.7091 - val_loss: 9.2173 - val_acc: 0.4258\n",
      "Epoch 959/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7716 - acc: 0.7104 - val_loss: 9.2123 - val_acc: 0.4262\n",
      "Epoch 960/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7640 - acc: 0.7145 - val_loss: 9.2046 - val_acc: 0.4264\n",
      "Epoch 961/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7589 - acc: 0.7167 - val_loss: 9.2201 - val_acc: 0.4253\n",
      "Epoch 962/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7635 - acc: 0.7150 - val_loss: 9.2119 - val_acc: 0.4256\n",
      "Epoch 963/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7547 - acc: 0.7195 - val_loss: 9.2152 - val_acc: 0.4256\n",
      "Epoch 964/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7656 - acc: 0.7132 - val_loss: 9.2149 - val_acc: 0.4258\n",
      "Epoch 965/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7497 - acc: 0.7221 - val_loss: 9.2132 - val_acc: 0.4262\n",
      "Epoch 966/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7657 - acc: 0.7125 - val_loss: 9.2227 - val_acc: 0.4253\n",
      "Epoch 967/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7661 - acc: 0.7122 - val_loss: 9.2165 - val_acc: 0.4253\n",
      "Epoch 968/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7424 - acc: 0.7229 - val_loss: 9.2149 - val_acc: 0.4260\n",
      "Epoch 969/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7493 - acc: 0.7192 - val_loss: 9.2154 - val_acc: 0.4258\n",
      "Epoch 970/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7530 - acc: 0.7180 - val_loss: 9.2187 - val_acc: 0.4258\n",
      "Epoch 971/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7651 - acc: 0.7114 - val_loss: 9.2097 - val_acc: 0.4256\n",
      "Epoch 972/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7665 - acc: 0.7093 - val_loss: 9.2177 - val_acc: 0.4256\n",
      "Epoch 973/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7659 - acc: 0.7122 - val_loss: 9.2175 - val_acc: 0.4251\n",
      "Epoch 974/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7487 - acc: 0.7197 - val_loss: 9.2109 - val_acc: 0.4260\n",
      "Epoch 975/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7581 - acc: 0.7143 - val_loss: 9.2153 - val_acc: 0.4256\n",
      "Epoch 976/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7670 - acc: 0.7118 - val_loss: 9.2191 - val_acc: 0.4253\n",
      "Epoch 977/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7519 - acc: 0.7171 - val_loss: 9.2170 - val_acc: 0.4253\n",
      "Epoch 978/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7527 - acc: 0.7190 - val_loss: 9.2150 - val_acc: 0.4249\n",
      "Epoch 979/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7561 - acc: 0.7191 - val_loss: 9.2202 - val_acc: 0.4249\n",
      "Epoch 980/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7544 - acc: 0.7188 - val_loss: 9.2165 - val_acc: 0.4253\n",
      "Epoch 981/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7656 - acc: 0.7133 - val_loss: 9.2079 - val_acc: 0.4253\n",
      "Epoch 982/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7702 - acc: 0.7079 - val_loss: 9.2091 - val_acc: 0.4256\n",
      "Epoch 983/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7573 - acc: 0.7173 - val_loss: 9.2049 - val_acc: 0.4253\n",
      "Epoch 984/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7548 - acc: 0.7193 - val_loss: 9.2074 - val_acc: 0.4251\n",
      "Epoch 985/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7583 - acc: 0.7173 - val_loss: 9.2063 - val_acc: 0.4256\n",
      "Epoch 986/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7538 - acc: 0.7193 - val_loss: 9.2113 - val_acc: 0.4242\n",
      "Epoch 987/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7445 - acc: 0.7204 - val_loss: 9.2106 - val_acc: 0.4253\n",
      "Epoch 988/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7565 - acc: 0.7205 - val_loss: 9.2061 - val_acc: 0.4260\n",
      "Epoch 989/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7541 - acc: 0.7184 - val_loss: 9.2069 - val_acc: 0.4260\n",
      "Epoch 990/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7486 - acc: 0.7210 - val_loss: 9.2171 - val_acc: 0.4258\n",
      "Epoch 991/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7403 - acc: 0.7226 - val_loss: 9.2168 - val_acc: 0.4247\n",
      "Epoch 992/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7503 - acc: 0.7210 - val_loss: 9.2087 - val_acc: 0.4253\n",
      "Epoch 993/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7332 - acc: 0.7266 - val_loss: 9.2069 - val_acc: 0.4256\n",
      "Epoch 994/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7478 - acc: 0.7211 - val_loss: 9.2088 - val_acc: 0.4249\n",
      "Epoch 995/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7266 - acc: 0.7277 - val_loss: 9.2007 - val_acc: 0.4260\n",
      "Epoch 996/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7419 - acc: 0.7230 - val_loss: 9.2051 - val_acc: 0.4256\n",
      "Epoch 997/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7342 - acc: 0.7255 - val_loss: 9.2201 - val_acc: 0.4256\n",
      "Epoch 998/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7669 - acc: 0.7120 - val_loss: 9.1992 - val_acc: 0.4262\n",
      "Epoch 999/1000\n",
      "41259/41259 [==============================] - 72s 2ms/step - loss: 0.7546 - acc: 0.7178 - val_loss: 9.2101 - val_acc: 0.4253\n",
      "Epoch 1000/1000\n",
      "41259/41259 [==============================] - 71s 2ms/step - loss: 0.7434 - acc: 0.7216 - val_loss: 9.2142 - val_acc: 0.4253\n"
     ]
    }
   ],
   "source": [
    "history = model_.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x211c9776390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model1.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x211c2ef2e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model1.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x211c0172940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.savefig('model1.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "first = Input(shape=(2048,))\n",
    "\n",
    "model = Dense(1024, activation='relu')(first)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(512, activation='relu')(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(256, activation='relu')(model)\n",
    "model = Dropout(0.5)(model)\n",
    "model = Dense(10, activation='softmax')(model)\n",
    "\n",
    "model_ = Model(inputs=[first], outputs=model)\n",
    "\n",
    "sgd = SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model_.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41259 samples, validate on 4594 samples\n",
      "Epoch 1/1000\n",
      "41259/41259 [==============================] - 48s 1ms/step - loss: 1.8227 - acc: 0.4131 - val_loss: 1.8387 - val_acc: 0.4279\n",
      "Epoch 2/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 1.7310 - acc: 0.4341 - val_loss: 1.7828 - val_acc: 0.4279\n",
      "Epoch 3/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 1.7154 - acc: 0.4343 - val_loss: 1.7645 - val_acc: 0.4279\n",
      "Epoch 4/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 1.7048 - acc: 0.4343 - val_loss: 1.7647 - val_acc: 0.4279\n",
      "Epoch 5/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.6976 - acc: 0.4343 - val_loss: 1.7584 - val_acc: 0.4279\n",
      "Epoch 6/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 1.6890 - acc: 0.4343 - val_loss: 1.7532 - val_acc: 0.4279\n",
      "Epoch 7/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 1.6842 - acc: 0.4343 - val_loss: 1.7508 - val_acc: 0.4279\n",
      "Epoch 8/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 1.6784 - acc: 0.4343 - val_loss: 1.7534 - val_acc: 0.4279\n",
      "Epoch 9/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.6733 - acc: 0.4343 - val_loss: 1.7575 - val_acc: 0.4279\n",
      "Epoch 10/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.6715 - acc: 0.4343 - val_loss: 1.7580 - val_acc: 0.4279\n",
      "Epoch 11/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 1.6651 - acc: 0.4343 - val_loss: 1.7580 - val_acc: 0.4279\n",
      "Epoch 12/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 1.6641 - acc: 0.4344 - val_loss: 1.7593 - val_acc: 0.4279\n",
      "Epoch 13/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.6601 - acc: 0.4344 - val_loss: 1.7704 - val_acc: 0.4279\n",
      "Epoch 14/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 1.6544 - acc: 0.4343 - val_loss: 1.7702 - val_acc: 0.4279\n",
      "Epoch 15/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 1.6528 - acc: 0.4343 - val_loss: 1.7726 - val_acc: 0.4279\n",
      "Epoch 16/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 1.6487 - acc: 0.4344 - val_loss: 1.7786 - val_acc: 0.4279\n",
      "Epoch 17/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 1.6451 - acc: 0.4347 - val_loss: 1.7843 - val_acc: 0.4279\n",
      "Epoch 18/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 1.6430 - acc: 0.4349 - val_loss: 1.7859 - val_acc: 0.4279\n",
      "Epoch 19/1000\n",
      "41259/41259 [==============================] - 41s 996us/step - loss: 1.6388 - acc: 0.4352 - val_loss: 1.8016 - val_acc: 0.4279\n",
      "Epoch 20/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 1.6356 - acc: 0.4355 - val_loss: 1.8168 - val_acc: 0.4279\n",
      "Epoch 21/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.6338 - acc: 0.4356 - val_loss: 1.7951 - val_acc: 0.4279\n",
      "Epoch 22/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 1.6291 - acc: 0.4367 - val_loss: 1.7917 - val_acc: 0.4279\n",
      "Epoch 23/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 1.6258 - acc: 0.4371 - val_loss: 1.8064 - val_acc: 0.4279\n",
      "Epoch 24/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 1.6226 - acc: 0.4381 - val_loss: 1.8002 - val_acc: 0.4279\n",
      "Epoch 25/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 1.6166 - acc: 0.4393 - val_loss: 1.8131 - val_acc: 0.4279\n",
      "Epoch 26/1000\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 1.6160 - acc: 0.4386 - val_loss: 1.8020 - val_acc: 0.4279\n",
      "Epoch 27/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 1.6086 - acc: 0.4400 - val_loss: 1.8045 - val_acc: 0.4279\n",
      "Epoch 28/1000\n",
      "41259/41259 [==============================] - 41s 995us/step - loss: 1.6095 - acc: 0.4402 - val_loss: 1.8190 - val_acc: 0.4279\n",
      "Epoch 29/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 1.6035 - acc: 0.4417 - val_loss: 1.8027 - val_acc: 0.4269\n",
      "Epoch 30/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 1.5989 - acc: 0.4413 - val_loss: 1.8230 - val_acc: 0.4279\n",
      "Epoch 31/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.5962 - acc: 0.4436 - val_loss: 1.8592 - val_acc: 0.4279\n",
      "Epoch 32/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.5939 - acc: 0.4411 - val_loss: 1.8587 - val_acc: 0.4279\n",
      "Epoch 33/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.5896 - acc: 0.4428 - val_loss: 1.8608 - val_acc: 0.4277\n",
      "Epoch 34/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 1.5834 - acc: 0.4451 - val_loss: 1.8733 - val_acc: 0.4264\n",
      "Epoch 35/1000\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 1.5789 - acc: 0.4454 - val_loss: 1.8888 - val_acc: 0.4279\n",
      "Epoch 36/1000\n",
      "41259/41259 [==============================] - 41s 994us/step - loss: 1.5761 - acc: 0.4470 - val_loss: 1.8822 - val_acc: 0.4271\n",
      "Epoch 37/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 1.5757 - acc: 0.4451 - val_loss: 1.8703 - val_acc: 0.4245\n",
      "Epoch 38/1000\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 1.5677 - acc: 0.4475 - val_loss: 1.8851 - val_acc: 0.4234\n",
      "Epoch 39/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 1.5639 - acc: 0.4491 - val_loss: 1.8936 - val_acc: 0.4234\n",
      "Epoch 40/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.5598 - acc: 0.4493 - val_loss: 1.9109 - val_acc: 0.4238\n",
      "Epoch 41/1000\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 1.5567 - acc: 0.4504 - val_loss: 1.9166 - val_acc: 0.4229\n",
      "Epoch 42/1000\n",
      "41259/41259 [==============================] - 41s 995us/step - loss: 1.5546 - acc: 0.4511 - val_loss: 1.9401 - val_acc: 0.4223\n",
      "Epoch 43/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 1.5469 - acc: 0.4522 - val_loss: 1.9347 - val_acc: 0.4131\n",
      "Epoch 44/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 1.5446 - acc: 0.4509 - val_loss: 1.9601 - val_acc: 0.4221\n",
      "Epoch 45/1000\n",
      "41259/41259 [==============================] - 41s 994us/step - loss: 1.5369 - acc: 0.4538 - val_loss: 1.9811 - val_acc: 0.4142\n",
      "Epoch 46/1000\n",
      "41259/41259 [==============================] - 41s 995us/step - loss: 1.5342 - acc: 0.4550 - val_loss: 2.0199 - val_acc: 0.4040\n",
      "Epoch 47/1000\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 1.5305 - acc: 0.4567 - val_loss: 1.9813 - val_acc: 0.3135\n",
      "Epoch 48/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 1.5254 - acc: 0.4573 - val_loss: 1.9503 - val_acc: 0.3872\n",
      "Epoch 49/1000\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 1.5196 - acc: 0.4583 - val_loss: 1.9630 - val_acc: 0.3420\n",
      "Epoch 50/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 1.5180 - acc: 0.4583 - val_loss: 2.0181 - val_acc: 0.3731\n",
      "Epoch 51/1000\n",
      "41259/41259 [==============================] - 41s 994us/step - loss: 1.5106 - acc: 0.4605 - val_loss: 2.0885 - val_acc: 0.4088\n",
      "Epoch 52/1000\n",
      "41259/41259 [==============================] - 41s 994us/step - loss: 1.5070 - acc: 0.4615 - val_loss: 2.1343 - val_acc: 0.4103\n",
      "Epoch 53/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 1.5036 - acc: 0.4633 - val_loss: 2.1286 - val_acc: 0.3844\n",
      "Epoch 54/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 1.5013 - acc: 0.4642 - val_loss: 2.0902 - val_acc: 0.3418\n",
      "Epoch 55/1000\n",
      "41259/41259 [==============================] - 41s 995us/step - loss: 1.4917 - acc: 0.4643 - val_loss: 2.1751 - val_acc: 0.3213\n",
      "Epoch 56/1000\n",
      "41259/41259 [==============================] - 41s 996us/step - loss: 1.4876 - acc: 0.4687 - val_loss: 2.2034 - val_acc: 0.3502\n",
      "Epoch 57/1000\n",
      "41259/41259 [==============================] - 41s 998us/step - loss: 1.4840 - acc: 0.4691 - val_loss: 2.1619 - val_acc: 0.3618\n",
      "Epoch 58/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 1.4785 - acc: 0.4685 - val_loss: 2.1239 - val_acc: 0.3204\n",
      "Epoch 59/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 1.4719 - acc: 0.4719 - val_loss: 2.1163 - val_acc: 0.2701\n",
      "Epoch 60/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 1.4679 - acc: 0.4718 - val_loss: 2.2125 - val_acc: 0.3407\n",
      "Epoch 61/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.4651 - acc: 0.4710 - val_loss: 2.2502 - val_acc: 0.3267\n",
      "Epoch 62/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.4600 - acc: 0.4721 - val_loss: 2.3506 - val_acc: 0.3391\n",
      "Epoch 63/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 1.4590 - acc: 0.4750 - val_loss: 2.3597 - val_acc: 0.3748\n",
      "Epoch 64/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 1.4467 - acc: 0.4784 - val_loss: 2.2759 - val_acc: 0.3206\n",
      "Epoch 65/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.4457 - acc: 0.4762 - val_loss: 2.4802 - val_acc: 0.3705\n",
      "Epoch 66/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.4415 - acc: 0.4791 - val_loss: 2.3986 - val_acc: 0.3431\n",
      "Epoch 67/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 1.4289 - acc: 0.4817 - val_loss: 2.5626 - val_acc: 0.3633\n",
      "Epoch 68/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 1.4289 - acc: 0.4828 - val_loss: 2.5948 - val_acc: 0.3659\n",
      "Epoch 69/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.4236 - acc: 0.4840 - val_loss: 2.4746 - val_acc: 0.3278\n",
      "Epoch 70/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.4221 - acc: 0.4831 - val_loss: 2.7244 - val_acc: 0.3653\n",
      "Epoch 71/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.4151 - acc: 0.4858 - val_loss: 2.5472 - val_acc: 0.3237\n",
      "Epoch 72/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 1.4128 - acc: 0.4870 - val_loss: 2.6922 - val_acc: 0.3383\n",
      "Epoch 73/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 1.4035 - acc: 0.4876 - val_loss: 2.6026 - val_acc: 0.3008\n",
      "Epoch 74/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.4026 - acc: 0.4894 - val_loss: 2.6854 - val_acc: 0.3263\n",
      "Epoch 75/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.3964 - acc: 0.4924 - val_loss: 2.8784 - val_acc: 0.3039\n",
      "Epoch 76/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 1.3879 - acc: 0.4945 - val_loss: 3.1755 - val_acc: 0.3640\n",
      "Epoch 77/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 1.3889 - acc: 0.4958 - val_loss: 3.1030 - val_acc: 0.3811\n",
      "Epoch 78/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.3793 - acc: 0.4951 - val_loss: 2.8580 - val_acc: 0.3235\n",
      "Epoch 79/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.3761 - acc: 0.4969 - val_loss: 2.9440 - val_acc: 0.3330\n",
      "Epoch 80/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.3708 - acc: 0.4997 - val_loss: 2.8552 - val_acc: 0.3252\n",
      "Epoch 81/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 1.3650 - acc: 0.4982 - val_loss: 2.9042 - val_acc: 0.3084\n",
      "Epoch 82/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.3635 - acc: 0.4982 - val_loss: 3.0460 - val_acc: 0.3191\n",
      "Epoch 83/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.3538 - acc: 0.5038 - val_loss: 2.9104 - val_acc: 0.3069\n",
      "Epoch 84/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.3507 - acc: 0.5044 - val_loss: 3.1049 - val_acc: 0.2828\n",
      "Epoch 85/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 1.3465 - acc: 0.5036 - val_loss: 3.3305 - val_acc: 0.3150\n",
      "Epoch 86/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 1.3416 - acc: 0.5042 - val_loss: 3.3344 - val_acc: 0.3187\n",
      "Epoch 87/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.3347 - acc: 0.5076 - val_loss: 3.4373 - val_acc: 0.3267\n",
      "Epoch 88/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 1.3314 - acc: 0.5096 - val_loss: 3.2778 - val_acc: 0.2939\n",
      "Epoch 89/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.3211 - acc: 0.5139 - val_loss: 3.2705 - val_acc: 0.3026\n",
      "Epoch 90/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 1.3164 - acc: 0.5147 - val_loss: 3.3343 - val_acc: 0.2571\n",
      "Epoch 91/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.3157 - acc: 0.5143 - val_loss: 3.5959 - val_acc: 0.3357\n",
      "Epoch 92/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.3129 - acc: 0.5170 - val_loss: 3.5914 - val_acc: 0.3052\n",
      "Epoch 93/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.3079 - acc: 0.5155 - val_loss: 3.6620 - val_acc: 0.3000\n",
      "Epoch 94/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.3003 - acc: 0.5189 - val_loss: 3.5838 - val_acc: 0.3074\n",
      "Epoch 95/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 1.2915 - acc: 0.5211 - val_loss: 4.0054 - val_acc: 0.3568\n",
      "Epoch 96/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.2883 - acc: 0.5225 - val_loss: 3.6632 - val_acc: 0.2841\n",
      "Epoch 97/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.2842 - acc: 0.5228 - val_loss: 3.6931 - val_acc: 0.2701\n",
      "Epoch 98/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.2780 - acc: 0.5264 - val_loss: 4.1990 - val_acc: 0.3402\n",
      "Epoch 99/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.2753 - acc: 0.5264 - val_loss: 4.1220 - val_acc: 0.3265\n",
      "Epoch 100/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 1.2710 - acc: 0.5304 - val_loss: 4.3811 - val_acc: 0.3409\n",
      "Epoch 101/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.2688 - acc: 0.5286 - val_loss: 4.1069 - val_acc: 0.3341\n",
      "Epoch 102/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 1.2644 - acc: 0.5287 - val_loss: 4.2748 - val_acc: 0.3302\n",
      "Epoch 103/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.2665 - acc: 0.5293 - val_loss: 4.1347 - val_acc: 0.2730\n",
      "Epoch 104/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.2542 - acc: 0.5328 - val_loss: 4.5652 - val_acc: 0.3169\n",
      "Epoch 105/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 1.2546 - acc: 0.5345 - val_loss: 4.6623 - val_acc: 0.3646\n",
      "Epoch 106/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 1.2415 - acc: 0.5363 - val_loss: 4.3344 - val_acc: 0.3459\n",
      "Epoch 107/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.2365 - acc: 0.5400 - val_loss: 4.4465 - val_acc: 0.2869\n",
      "Epoch 108/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 1.2365 - acc: 0.5395 - val_loss: 4.6969 - val_acc: 0.2952\n",
      "Epoch 109/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.2363 - acc: 0.5404 - val_loss: 4.5639 - val_acc: 0.3198\n",
      "Epoch 110/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 1.2269 - acc: 0.5413 - val_loss: 4.8049 - val_acc: 0.3272\n",
      "Epoch 111/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 1.2219 - acc: 0.5430 - val_loss: 4.7234 - val_acc: 0.3385\n",
      "Epoch 112/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 1.2185 - acc: 0.5432 - val_loss: 4.8329 - val_acc: 0.3350\n",
      "Epoch 113/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.2107 - acc: 0.5480 - val_loss: 4.6224 - val_acc: 0.3158\n",
      "Epoch 114/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 1.2062 - acc: 0.5489 - val_loss: 4.9515 - val_acc: 0.3259\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 40s 974us/step - loss: 1.2036 - acc: 0.5502 - val_loss: 4.6067 - val_acc: 0.3172\n",
      "Epoch 116/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.1972 - acc: 0.5542 - val_loss: 4.8727 - val_acc: 0.3398\n",
      "Epoch 117/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1941 - acc: 0.5536 - val_loss: 4.9721 - val_acc: 0.3080\n",
      "Epoch 118/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.1886 - acc: 0.5555 - val_loss: 5.0274 - val_acc: 0.3004\n",
      "Epoch 119/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 1.1830 - acc: 0.5580 - val_loss: 5.0502 - val_acc: 0.2936\n",
      "Epoch 120/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 1.1802 - acc: 0.5556 - val_loss: 5.0057 - val_acc: 0.3182\n",
      "Epoch 121/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.1707 - acc: 0.5597 - val_loss: 5.4911 - val_acc: 0.3193\n",
      "Epoch 122/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1785 - acc: 0.5579 - val_loss: 5.2243 - val_acc: 0.3611\n",
      "Epoch 123/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1665 - acc: 0.5614 - val_loss: 5.3581 - val_acc: 0.2847\n",
      "Epoch 124/1000\n",
      "41259/41259 [==============================] - 40s 968us/step - loss: 1.1625 - acc: 0.5632 - val_loss: 5.2812 - val_acc: 0.3135\n",
      "Epoch 125/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 1.1585 - acc: 0.5643 - val_loss: 5.4178 - val_acc: 0.3100\n",
      "Epoch 126/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 1.1551 - acc: 0.5657 - val_loss: 5.4503 - val_acc: 0.3165\n",
      "Epoch 127/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.1488 - acc: 0.5688 - val_loss: 5.4428 - val_acc: 0.2532\n",
      "Epoch 128/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1534 - acc: 0.5644 - val_loss: 5.4793 - val_acc: 0.3006\n",
      "Epoch 129/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 1.1386 - acc: 0.5707 - val_loss: 5.5143 - val_acc: 0.2658\n",
      "Epoch 130/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 1.1335 - acc: 0.5765 - val_loss: 5.4405 - val_acc: 0.2923\n",
      "Epoch 131/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1321 - acc: 0.5746 - val_loss: 5.6992 - val_acc: 0.3583\n",
      "Epoch 132/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 1.1322 - acc: 0.5750 - val_loss: 5.2873 - val_acc: 0.2995\n",
      "Epoch 133/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1237 - acc: 0.5771 - val_loss: 5.7371 - val_acc: 0.3313\n",
      "Epoch 134/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 1.1213 - acc: 0.5765 - val_loss: 5.7968 - val_acc: 0.3337\n",
      "Epoch 135/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1164 - acc: 0.5778 - val_loss: 5.5141 - val_acc: 0.2989\n",
      "Epoch 136/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.1173 - acc: 0.5798 - val_loss: 5.8596 - val_acc: 0.2987\n",
      "Epoch 137/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 1.1051 - acc: 0.5855 - val_loss: 5.9704 - val_acc: 0.3267\n",
      "Epoch 138/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 1.1070 - acc: 0.5801 - val_loss: 6.0048 - val_acc: 0.3032\n",
      "Epoch 139/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 1.0991 - acc: 0.5871 - val_loss: 5.8176 - val_acc: 0.3328\n",
      "Epoch 140/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 1.0922 - acc: 0.5867 - val_loss: 5.9429 - val_acc: 0.3511\n",
      "Epoch 141/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.0936 - acc: 0.5868 - val_loss: 5.9739 - val_acc: 0.3047\n",
      "Epoch 142/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.0930 - acc: 0.5876 - val_loss: 5.8640 - val_acc: 0.3370\n",
      "Epoch 143/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 1.0873 - acc: 0.5862 - val_loss: 5.8594 - val_acc: 0.3261\n",
      "Epoch 144/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 1.0775 - acc: 0.5942 - val_loss: 6.1606 - val_acc: 0.3618\n",
      "Epoch 145/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.0711 - acc: 0.5942 - val_loss: 6.2560 - val_acc: 0.3274\n",
      "Epoch 146/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 1.0792 - acc: 0.5938 - val_loss: 6.0153 - val_acc: 0.3576\n",
      "Epoch 147/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 1.0651 - acc: 0.5989 - val_loss: 6.1888 - val_acc: 0.3468\n",
      "Epoch 148/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.0664 - acc: 0.5988 - val_loss: 5.9840 - val_acc: 0.3265\n",
      "Epoch 149/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 1.0621 - acc: 0.6016 - val_loss: 6.3368 - val_acc: 0.3163\n",
      "Epoch 150/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 1.0502 - acc: 0.6051 - val_loss: 6.3409 - val_acc: 0.3119\n",
      "Epoch 151/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.0535 - acc: 0.5990 - val_loss: 6.2932 - val_acc: 0.3435\n",
      "Epoch 152/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 1.0502 - acc: 0.6036 - val_loss: 6.4774 - val_acc: 0.3365\n",
      "Epoch 153/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.0470 - acc: 0.6031 - val_loss: 6.6029 - val_acc: 0.3276\n",
      "Epoch 154/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 1.0445 - acc: 0.6057 - val_loss: 6.5943 - val_acc: 0.3228\n",
      "Epoch 155/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 1.0380 - acc: 0.6080 - val_loss: 6.6547 - val_acc: 0.3663\n",
      "Epoch 156/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 1.0430 - acc: 0.6050 - val_loss: 6.5775 - val_acc: 0.3563\n",
      "Epoch 157/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.0348 - acc: 0.6076 - val_loss: 6.5622 - val_acc: 0.3605\n",
      "Epoch 158/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 1.0245 - acc: 0.6117 - val_loss: 6.5613 - val_acc: 0.3633\n",
      "Epoch 159/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 1.0254 - acc: 0.6131 - val_loss: 6.6152 - val_acc: 0.3592\n",
      "Epoch 160/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.0227 - acc: 0.6134 - val_loss: 6.5532 - val_acc: 0.3439\n",
      "Epoch 161/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 1.0232 - acc: 0.6134 - val_loss: 6.4865 - val_acc: 0.3574\n",
      "Epoch 162/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 1.0141 - acc: 0.6146 - val_loss: 6.4689 - val_acc: 0.3539\n",
      "Epoch 163/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 1.0105 - acc: 0.6189 - val_loss: 6.6173 - val_acc: 0.3524\n",
      "Epoch 164/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 1.0109 - acc: 0.6181 - val_loss: 6.6331 - val_acc: 0.3219\n",
      "Epoch 165/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.0034 - acc: 0.6212 - val_loss: 6.8423 - val_acc: 0.2808\n",
      "Epoch 166/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 1.0007 - acc: 0.6212 - val_loss: 6.6062 - val_acc: 0.3215\n",
      "Epoch 167/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.9984 - acc: 0.6207 - val_loss: 6.6904 - val_acc: 0.3174\n",
      "Epoch 168/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.9879 - acc: 0.6247 - val_loss: 6.9186 - val_acc: 0.3724\n",
      "Epoch 169/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.9947 - acc: 0.6245 - val_loss: 6.8577 - val_acc: 0.3751\n",
      "Epoch 170/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.9872 - acc: 0.6278 - val_loss: 6.8931 - val_acc: 0.3711\n",
      "Epoch 171/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.9790 - acc: 0.6295 - val_loss: 6.8612 - val_acc: 0.3507\n",
      "Epoch 172/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.9826 - acc: 0.6282 - val_loss: 6.7990 - val_acc: 0.3683\n",
      "Epoch 173/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.9814 - acc: 0.6263 - val_loss: 6.6848 - val_acc: 0.3300\n",
      "Epoch 174/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.9729 - acc: 0.6331 - val_loss: 6.9240 - val_acc: 0.3692\n",
      "Epoch 175/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.9718 - acc: 0.6332 - val_loss: 7.0538 - val_acc: 0.3883\n",
      "Epoch 176/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.9739 - acc: 0.6309 - val_loss: 7.0321 - val_acc: 0.3555\n",
      "Epoch 177/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.9701 - acc: 0.6327 - val_loss: 7.0619 - val_acc: 0.3518\n",
      "Epoch 178/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.9664 - acc: 0.6357 - val_loss: 7.0111 - val_acc: 0.3681\n",
      "Epoch 179/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.9624 - acc: 0.6335 - val_loss: 6.9431 - val_acc: 0.3594\n",
      "Epoch 180/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.9566 - acc: 0.6329 - val_loss: 6.9662 - val_acc: 0.3772\n",
      "Epoch 181/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.9611 - acc: 0.6379 - val_loss: 7.0973 - val_acc: 0.3646\n",
      "Epoch 182/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.9482 - acc: 0.6398 - val_loss: 7.0235 - val_acc: 0.3398\n",
      "Epoch 183/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.9458 - acc: 0.6419 - val_loss: 6.9390 - val_acc: 0.3594\n",
      "Epoch 184/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.9423 - acc: 0.6431 - val_loss: 7.0287 - val_acc: 0.3396\n",
      "Epoch 185/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.9385 - acc: 0.6451 - val_loss: 7.1347 - val_acc: 0.3672\n",
      "Epoch 186/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.9356 - acc: 0.6464 - val_loss: 6.9388 - val_acc: 0.3452\n",
      "Epoch 187/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.9322 - acc: 0.6458 - val_loss: 7.0176 - val_acc: 0.3535\n",
      "Epoch 188/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.9425 - acc: 0.6438 - val_loss: 6.9998 - val_acc: 0.3548\n",
      "Epoch 189/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.9283 - acc: 0.6475 - val_loss: 7.2255 - val_acc: 0.3385\n",
      "Epoch 190/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.9237 - acc: 0.6505 - val_loss: 7.0546 - val_acc: 0.3631\n",
      "Epoch 191/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.9203 - acc: 0.6500 - val_loss: 7.2528 - val_acc: 0.3816\n",
      "Epoch 192/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.9262 - acc: 0.6494 - val_loss: 7.1645 - val_acc: 0.3383\n",
      "Epoch 193/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.9110 - acc: 0.6542 - val_loss: 7.3817 - val_acc: 0.3859\n",
      "Epoch 194/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.9078 - acc: 0.6569 - val_loss: 7.2380 - val_acc: 0.3513\n",
      "Epoch 195/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.9120 - acc: 0.6552 - val_loss: 7.3160 - val_acc: 0.3661\n",
      "Epoch 196/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.9098 - acc: 0.6542 - val_loss: 7.2958 - val_acc: 0.3642\n",
      "Epoch 197/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.9089 - acc: 0.6554 - val_loss: 7.5551 - val_acc: 0.3988\n",
      "Epoch 198/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.8968 - acc: 0.6602 - val_loss: 7.3183 - val_acc: 0.3509\n",
      "Epoch 199/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.8912 - acc: 0.6630 - val_loss: 7.2758 - val_acc: 0.3707\n",
      "Epoch 200/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.8905 - acc: 0.6646 - val_loss: 7.3161 - val_acc: 0.3724\n",
      "Epoch 201/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.8960 - acc: 0.6639 - val_loss: 7.3234 - val_acc: 0.3544\n",
      "Epoch 202/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.8835 - acc: 0.6671 - val_loss: 7.5268 - val_acc: 0.3896\n",
      "Epoch 203/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.8852 - acc: 0.6617 - val_loss: 7.4037 - val_acc: 0.3955\n",
      "Epoch 204/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 0.8823 - acc: 0.6663 - val_loss: 7.2925 - val_acc: 0.3668\n",
      "Epoch 205/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 0.8695 - acc: 0.6683 - val_loss: 7.6043 - val_acc: 0.3997\n",
      "Epoch 206/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.8746 - acc: 0.6678 - val_loss: 7.4704 - val_acc: 0.3829\n",
      "Epoch 207/1000\n",
      "41259/41259 [==============================] - 41s 999us/step - loss: 0.8615 - acc: 0.6747 - val_loss: 7.8554 - val_acc: 0.4094\n",
      "Epoch 208/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8715 - acc: 0.6722 - val_loss: 8.1539 - val_acc: 0.4184\n",
      "Epoch 209/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8650 - acc: 0.6717 - val_loss: 7.6973 - val_acc: 0.4092\n",
      "Epoch 210/1000\n",
      "41259/41259 [==============================] - 41s 999us/step - loss: 0.8631 - acc: 0.6712 - val_loss: 7.7949 - val_acc: 0.4051\n",
      "Epoch 211/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.8557 - acc: 0.6763 - val_loss: 7.9885 - val_acc: 0.4155\n",
      "Epoch 212/1000\n",
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.8485 - acc: 0.6785 - val_loss: 7.5553 - val_acc: 0.3936\n",
      "Epoch 213/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8471 - acc: 0.6811 - val_loss: 7.9056 - val_acc: 0.4131\n",
      "Epoch 214/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8547 - acc: 0.6776 - val_loss: 7.8510 - val_acc: 0.4118\n",
      "Epoch 215/1000\n",
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.8423 - acc: 0.6818 - val_loss: 7.7452 - val_acc: 0.4010\n",
      "Epoch 216/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8410 - acc: 0.6817 - val_loss: 7.5819 - val_acc: 0.3842\n",
      "Epoch 217/1000\n",
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.8421 - acc: 0.6804 - val_loss: 7.6387 - val_acc: 0.3907\n",
      "Epoch 218/1000\n",
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.8433 - acc: 0.6813 - val_loss: 7.9662 - val_acc: 0.4084\n",
      "Epoch 219/1000\n",
      "41259/41259 [==============================] - 41s 997us/step - loss: 0.8336 - acc: 0.6848 - val_loss: 7.6340 - val_acc: 0.3840\n",
      "Epoch 220/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 0.8338 - acc: 0.6851 - val_loss: 7.7858 - val_acc: 0.3981\n",
      "Epoch 221/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 0.8332 - acc: 0.6844 - val_loss: 7.6842 - val_acc: 0.3966\n",
      "Epoch 222/1000\n",
      "41259/41259 [==============================] - 41s 999us/step - loss: 0.8290 - acc: 0.6865 - val_loss: 7.6380 - val_acc: 0.3875\n",
      "Epoch 223/1000\n",
      "41259/41259 [==============================] - 41s 998us/step - loss: 0.8147 - acc: 0.6910 - val_loss: 7.8278 - val_acc: 0.3986\n",
      "Epoch 224/1000\n",
      "41259/41259 [==============================] - 44s 1ms/step - loss: 0.8278 - acc: 0.6871 - val_loss: 7.7040 - val_acc: 0.3925\n",
      "Epoch 225/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8165 - acc: 0.6905 - val_loss: 7.6487 - val_acc: 0.3983\n",
      "Epoch 226/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8159 - acc: 0.6912 - val_loss: 7.8301 - val_acc: 0.3992\n",
      "Epoch 227/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8163 - acc: 0.6909 - val_loss: 7.7667 - val_acc: 0.4016\n",
      "Epoch 228/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.8102 - acc: 0.6947 - val_loss: 7.7469 - val_acc: 0.4016\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.8087 - acc: 0.6950 - val_loss: 7.5539 - val_acc: 0.3901\n",
      "Epoch 230/1000\n",
      "41259/41259 [==============================] - 41s 997us/step - loss: 0.8088 - acc: 0.6970 - val_loss: 7.6893 - val_acc: 0.3955\n",
      "Epoch 231/1000\n",
      "41259/41259 [==============================] - 41s 994us/step - loss: 0.8034 - acc: 0.6984 - val_loss: 7.8986 - val_acc: 0.4062\n",
      "Epoch 232/1000\n",
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.8008 - acc: 0.6983 - val_loss: 7.5154 - val_acc: 0.3672\n",
      "Epoch 233/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.7888 - acc: 0.7014 - val_loss: 7.5577 - val_acc: 0.3822\n",
      "Epoch 234/1000\n",
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.7882 - acc: 0.7039 - val_loss: 7.6870 - val_acc: 0.3916\n",
      "Epoch 235/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.7854 - acc: 0.7023 - val_loss: 7.5801 - val_acc: 0.3583\n",
      "Epoch 236/1000\n",
      "41259/41259 [==============================] - 43s 1ms/step - loss: 0.7772 - acc: 0.7062 - val_loss: 7.5725 - val_acc: 0.3529\n",
      "Epoch 237/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.7862 - acc: 0.7036 - val_loss: 7.5742 - val_acc: 0.3696\n",
      "Epoch 238/1000\n",
      "41259/41259 [==============================] - 43s 1ms/step - loss: 0.7801 - acc: 0.7074 - val_loss: 7.7706 - val_acc: 0.3949\n",
      "Epoch 239/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.7830 - acc: 0.7031 - val_loss: 7.5880 - val_acc: 0.3766\n",
      "Epoch 240/1000\n",
      "41259/41259 [==============================] - 43s 1ms/step - loss: 0.7772 - acc: 0.7067 - val_loss: 8.0491 - val_acc: 0.4112\n",
      "Epoch 241/1000\n",
      "41259/41259 [==============================] - 43s 1ms/step - loss: 0.7803 - acc: 0.7049 - val_loss: 7.9989 - val_acc: 0.4112\n",
      "Epoch 242/1000\n",
      "41259/41259 [==============================] - 44s 1ms/step - loss: 0.7621 - acc: 0.7119 - val_loss: 7.9091 - val_acc: 0.4018\n",
      "Epoch 243/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.7656 - acc: 0.7117 - val_loss: 7.7033 - val_acc: 0.3790\n",
      "Epoch 244/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.7754 - acc: 0.7063 - val_loss: 7.7406 - val_acc: 0.3914\n",
      "Epoch 245/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.7657 - acc: 0.7131 - val_loss: 7.9384 - val_acc: 0.3962\n",
      "Epoch 246/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.7598 - acc: 0.7152 - val_loss: 8.0047 - val_acc: 0.3816\n",
      "Epoch 247/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.7587 - acc: 0.7158 - val_loss: 7.7509 - val_acc: 0.3805\n",
      "Epoch 248/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7629 - acc: 0.7108 - val_loss: 7.9826 - val_acc: 0.3881\n",
      "Epoch 249/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7557 - acc: 0.7131 - val_loss: 7.7316 - val_acc: 0.3761\n",
      "Epoch 250/1000\n",
      "41259/41259 [==============================] - 41s 994us/step - loss: 0.7525 - acc: 0.7153 - val_loss: 7.8763 - val_acc: 0.3838\n",
      "Epoch 251/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 0.7477 - acc: 0.7214 - val_loss: 7.8675 - val_acc: 0.3973\n",
      "Epoch 252/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.7525 - acc: 0.7166 - val_loss: 7.9366 - val_acc: 0.4068\n",
      "Epoch 253/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.7548 - acc: 0.7179 - val_loss: 7.9451 - val_acc: 0.3912\n",
      "Epoch 254/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 0.7352 - acc: 0.7229 - val_loss: 7.9159 - val_acc: 0.3923\n",
      "Epoch 255/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 0.7395 - acc: 0.7219 - val_loss: 7.8298 - val_acc: 0.3944\n",
      "Epoch 256/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7395 - acc: 0.7211 - val_loss: 7.9492 - val_acc: 0.3966\n",
      "Epoch 257/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.7289 - acc: 0.7268 - val_loss: 8.0522 - val_acc: 0.3988\n",
      "Epoch 258/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.7399 - acc: 0.7212 - val_loss: 7.8458 - val_acc: 0.3825\n",
      "Epoch 259/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 0.7317 - acc: 0.7253 - val_loss: 7.7874 - val_acc: 0.3663\n",
      "Epoch 260/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 0.7300 - acc: 0.7274 - val_loss: 7.9166 - val_acc: 0.3788\n",
      "Epoch 261/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7386 - acc: 0.7224 - val_loss: 7.8534 - val_acc: 0.3966\n",
      "Epoch 262/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.7243 - acc: 0.7279 - val_loss: 7.7215 - val_acc: 0.3668\n",
      "Epoch 263/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 0.7193 - acc: 0.7288 - val_loss: 7.7011 - val_acc: 0.3770\n",
      "Epoch 264/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7245 - acc: 0.7283 - val_loss: 7.9461 - val_acc: 0.3960\n",
      "Epoch 265/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 0.7222 - acc: 0.7281 - val_loss: 7.9462 - val_acc: 0.3973\n",
      "Epoch 266/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 0.7128 - acc: 0.7322 - val_loss: 7.8258 - val_acc: 0.3694\n",
      "Epoch 267/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.7151 - acc: 0.7330 - val_loss: 7.7698 - val_acc: 0.3727\n",
      "Epoch 268/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.7041 - acc: 0.7346 - val_loss: 7.8561 - val_acc: 0.3859\n",
      "Epoch 269/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 0.7104 - acc: 0.7353 - val_loss: 7.8338 - val_acc: 0.3811\n",
      "Epoch 270/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7023 - acc: 0.7363 - val_loss: 7.9886 - val_acc: 0.4001\n",
      "Epoch 271/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7055 - acc: 0.7364 - val_loss: 7.9965 - val_acc: 0.3973\n",
      "Epoch 272/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 0.6962 - acc: 0.7389 - val_loss: 8.0408 - val_acc: 0.3955\n",
      "Epoch 273/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.7052 - acc: 0.7361 - val_loss: 7.9398 - val_acc: 0.3890\n",
      "Epoch 274/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.7029 - acc: 0.7373 - val_loss: 8.0864 - val_acc: 0.3955\n",
      "Epoch 275/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.6935 - acc: 0.7403 - val_loss: 8.2228 - val_acc: 0.4049\n",
      "Epoch 276/1000\n",
      "41259/41259 [==============================] - 41s 991us/step - loss: 0.6957 - acc: 0.7409 - val_loss: 8.3337 - val_acc: 0.4081\n",
      "Epoch 277/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.6901 - acc: 0.7406 - val_loss: 8.3335 - val_acc: 0.4116\n",
      "Epoch 278/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.6928 - acc: 0.7404 - val_loss: 8.0959 - val_acc: 0.3997\n",
      "Epoch 279/1000\n",
      "41259/41259 [==============================] - 41s 990us/step - loss: 0.6927 - acc: 0.7395 - val_loss: 8.2369 - val_acc: 0.4068\n",
      "Epoch 280/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 0.6856 - acc: 0.7431 - val_loss: 8.1706 - val_acc: 0.4071\n",
      "Epoch 281/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 0.6812 - acc: 0.7457 - val_loss: 8.1912 - val_acc: 0.4040\n",
      "Epoch 282/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 0.6840 - acc: 0.7439 - val_loss: 8.1897 - val_acc: 0.4071\n",
      "Epoch 283/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.6838 - acc: 0.7442 - val_loss: 8.1485 - val_acc: 0.4029\n",
      "Epoch 284/1000\n",
      "41259/41259 [==============================] - 41s 994us/step - loss: 0.6877 - acc: 0.7434 - val_loss: 8.1659 - val_acc: 0.4068\n",
      "Epoch 285/1000\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 0.6810 - acc: 0.7448 - val_loss: 8.1851 - val_acc: 0.4094\n",
      "Epoch 286/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6759 - acc: 0.7491 - val_loss: 8.1128 - val_acc: 0.4081\n",
      "Epoch 287/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.6754 - acc: 0.7490 - val_loss: 8.1668 - val_acc: 0.4075\n",
      "Epoch 288/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.6733 - acc: 0.7503 - val_loss: 7.9497 - val_acc: 0.4016\n",
      "Epoch 289/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6690 - acc: 0.7519 - val_loss: 8.1553 - val_acc: 0.4086\n",
      "Epoch 290/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6677 - acc: 0.7487 - val_loss: 8.1401 - val_acc: 0.4081\n",
      "Epoch 291/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.6625 - acc: 0.7535 - val_loss: 8.3484 - val_acc: 0.4138\n",
      "Epoch 292/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.6602 - acc: 0.7508 - val_loss: 8.0759 - val_acc: 0.4077\n",
      "Epoch 293/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.6631 - acc: 0.7529 - val_loss: 8.1269 - val_acc: 0.4042\n",
      "Epoch 294/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6624 - acc: 0.7512 - val_loss: 8.1759 - val_acc: 0.4081\n",
      "Epoch 295/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.6607 - acc: 0.7533 - val_loss: 8.3506 - val_acc: 0.4155\n",
      "Epoch 296/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.6559 - acc: 0.7565 - val_loss: 8.5288 - val_acc: 0.4164\n",
      "Epoch 297/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.6518 - acc: 0.7583 - val_loss: 8.0198 - val_acc: 0.4005\n",
      "Epoch 298/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.6534 - acc: 0.7555 - val_loss: 8.0832 - val_acc: 0.4038\n",
      "Epoch 299/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.6490 - acc: 0.7576 - val_loss: 8.0416 - val_acc: 0.4044\n",
      "Epoch 300/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6427 - acc: 0.7617 - val_loss: 8.3686 - val_acc: 0.4114\n",
      "Epoch 301/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6469 - acc: 0.7618 - val_loss: 8.5664 - val_acc: 0.4173\n",
      "Epoch 302/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.6491 - acc: 0.7597 - val_loss: 7.9272 - val_acc: 0.3914\n",
      "Epoch 303/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.6397 - acc: 0.7635 - val_loss: 7.8812 - val_acc: 0.3888\n",
      "Epoch 304/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6411 - acc: 0.7609 - val_loss: 8.3409 - val_acc: 0.4145\n",
      "Epoch 305/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6392 - acc: 0.7633 - val_loss: 8.0550 - val_acc: 0.4023\n",
      "Epoch 306/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6444 - acc: 0.7573 - val_loss: 7.9914 - val_acc: 0.4027\n",
      "Epoch 307/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.6387 - acc: 0.7659 - val_loss: 7.7303 - val_acc: 0.3524\n",
      "Epoch 308/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6317 - acc: 0.7645 - val_loss: 7.8478 - val_acc: 0.3990\n",
      "Epoch 309/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6260 - acc: 0.7660 - val_loss: 7.9206 - val_acc: 0.3997\n",
      "Epoch 310/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.6316 - acc: 0.7647 - val_loss: 8.2568 - val_acc: 0.4127\n",
      "Epoch 311/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.6303 - acc: 0.7634 - val_loss: 8.5288 - val_acc: 0.4199\n",
      "Epoch 312/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.6241 - acc: 0.7672 - val_loss: 8.0000 - val_acc: 0.4018\n",
      "Epoch 313/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6344 - acc: 0.7654 - val_loss: 8.3779 - val_acc: 0.4171\n",
      "Epoch 314/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.6342 - acc: 0.7656 - val_loss: 7.8976 - val_acc: 0.3955\n",
      "Epoch 315/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.6198 - acc: 0.7722 - val_loss: 7.9524 - val_acc: 0.4003\n",
      "Epoch 316/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.6314 - acc: 0.7670 - val_loss: 8.0914 - val_acc: 0.4077\n",
      "Epoch 317/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6240 - acc: 0.7680 - val_loss: 8.1594 - val_acc: 0.4066\n",
      "Epoch 318/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.6237 - acc: 0.7700 - val_loss: 8.2797 - val_acc: 0.4116\n",
      "Epoch 319/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.6213 - acc: 0.7713 - val_loss: 8.2708 - val_acc: 0.4086\n",
      "Epoch 320/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.6182 - acc: 0.7710 - val_loss: 8.2682 - val_acc: 0.4101\n",
      "Epoch 321/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.6149 - acc: 0.7724 - val_loss: 8.2410 - val_acc: 0.4077\n",
      "Epoch 322/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.6256 - acc: 0.7721 - val_loss: 8.2139 - val_acc: 0.4051\n",
      "Epoch 323/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.6129 - acc: 0.7717 - val_loss: 8.1553 - val_acc: 0.4040\n",
      "Epoch 324/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.6231 - acc: 0.7694 - val_loss: 8.3920 - val_acc: 0.4125\n",
      "Epoch 325/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.6176 - acc: 0.7689 - val_loss: 8.1268 - val_acc: 0.3992\n",
      "Epoch 326/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.6124 - acc: 0.7721 - val_loss: 8.7734 - val_acc: 0.4177\n",
      "Epoch 327/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.6010 - acc: 0.7765 - val_loss: 8.4470 - val_acc: 0.4101\n",
      "Epoch 328/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6137 - acc: 0.7717 - val_loss: 8.4465 - val_acc: 0.4136\n",
      "Epoch 329/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.5989 - acc: 0.7779 - val_loss: 8.3493 - val_acc: 0.4110\n",
      "Epoch 330/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6018 - acc: 0.7780 - val_loss: 8.2909 - val_acc: 0.4055\n",
      "Epoch 331/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.6008 - acc: 0.7777 - val_loss: 8.1116 - val_acc: 0.3983\n",
      "Epoch 332/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.6023 - acc: 0.7784 - val_loss: 8.8717 - val_acc: 0.4192\n",
      "Epoch 333/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.6054 - acc: 0.7775 - val_loss: 8.4446 - val_acc: 0.4101\n",
      "Epoch 334/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.5866 - acc: 0.7823 - val_loss: 8.2820 - val_acc: 0.4101\n",
      "Epoch 335/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.6061 - acc: 0.7753 - val_loss: 8.5234 - val_acc: 0.4131\n",
      "Epoch 336/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.5939 - acc: 0.7787 - val_loss: 8.4754 - val_acc: 0.4112\n",
      "Epoch 337/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.5945 - acc: 0.7812 - val_loss: 8.4338 - val_acc: 0.4127\n",
      "Epoch 338/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.5901 - acc: 0.7823 - val_loss: 8.4101 - val_acc: 0.4094\n",
      "Epoch 339/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.5989 - acc: 0.7788 - val_loss: 8.5085 - val_acc: 0.4116\n",
      "Epoch 340/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.5914 - acc: 0.7821 - val_loss: 8.4691 - val_acc: 0.4136\n",
      "Epoch 341/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.5888 - acc: 0.7845 - val_loss: 8.6122 - val_acc: 0.4116\n",
      "Epoch 342/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.5720 - acc: 0.7912 - val_loss: 8.6384 - val_acc: 0.4158\n",
      "Epoch 343/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5776 - acc: 0.7882 - val_loss: 8.6926 - val_acc: 0.4153\n",
      "Epoch 344/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.5734 - acc: 0.7898 - val_loss: 8.5204 - val_acc: 0.4123\n",
      "Epoch 345/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.5772 - acc: 0.7877 - val_loss: 8.7573 - val_acc: 0.4164\n",
      "Epoch 346/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5686 - acc: 0.7897 - val_loss: 8.5880 - val_acc: 0.4149\n",
      "Epoch 347/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5780 - acc: 0.7863 - val_loss: 8.4265 - val_acc: 0.4138\n",
      "Epoch 348/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.5767 - acc: 0.7843 - val_loss: 8.7133 - val_acc: 0.4179\n",
      "Epoch 349/1000\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.5743 - acc: 0.7896 - val_loss: 8.6427 - val_acc: 0.4192\n",
      "Epoch 350/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.5604 - acc: 0.7933 - val_loss: 8.6420 - val_acc: 0.4153\n",
      "Epoch 351/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.5685 - acc: 0.7925 - val_loss: 8.4132 - val_acc: 0.4151\n",
      "Epoch 352/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5732 - acc: 0.7901 - val_loss: 8.2957 - val_acc: 0.4125\n",
      "Epoch 353/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5580 - acc: 0.7931 - val_loss: 8.4151 - val_acc: 0.4151\n",
      "Epoch 354/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5586 - acc: 0.7941 - val_loss: 8.3753 - val_acc: 0.4108\n",
      "Epoch 355/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.5654 - acc: 0.7945 - val_loss: 8.6320 - val_acc: 0.4175\n",
      "Epoch 356/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5650 - acc: 0.7952 - val_loss: 8.4395 - val_acc: 0.4127\n",
      "Epoch 357/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5570 - acc: 0.7972 - val_loss: 8.2951 - val_acc: 0.4118\n",
      "Epoch 358/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5610 - acc: 0.7954 - val_loss: 8.2921 - val_acc: 0.4125\n",
      "Epoch 359/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.5531 - acc: 0.7969 - val_loss: 8.3640 - val_acc: 0.4136\n",
      "Epoch 360/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.5567 - acc: 0.7965 - val_loss: 8.4041 - val_acc: 0.4127\n",
      "Epoch 361/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5529 - acc: 0.7992 - val_loss: 8.5078 - val_acc: 0.4142\n",
      "Epoch 362/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.5478 - acc: 0.7978 - val_loss: 8.7998 - val_acc: 0.4173\n",
      "Epoch 363/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.5518 - acc: 0.7976 - val_loss: 8.5983 - val_acc: 0.4149\n",
      "Epoch 364/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.5442 - acc: 0.7995 - val_loss: 8.2519 - val_acc: 0.4101\n",
      "Epoch 365/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.5493 - acc: 0.7994 - val_loss: 8.4371 - val_acc: 0.4149\n",
      "Epoch 366/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.5504 - acc: 0.7976 - val_loss: 8.7129 - val_acc: 0.4158\n",
      "Epoch 367/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.5416 - acc: 0.8016 - val_loss: 8.6010 - val_acc: 0.4129\n",
      "Epoch 368/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.5427 - acc: 0.8031 - val_loss: 8.6889 - val_acc: 0.4164\n",
      "Epoch 369/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.5461 - acc: 0.7975 - val_loss: 8.3597 - val_acc: 0.4097\n",
      "Epoch 370/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.5436 - acc: 0.8020 - val_loss: 8.6542 - val_acc: 0.4153\n",
      "Epoch 371/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5373 - acc: 0.8055 - val_loss: 8.7209 - val_acc: 0.4177\n",
      "Epoch 372/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.5408 - acc: 0.8008 - val_loss: 8.8532 - val_acc: 0.4175\n",
      "Epoch 373/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.5326 - acc: 0.8049 - val_loss: 8.7955 - val_acc: 0.4168\n",
      "Epoch 374/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.5280 - acc: 0.8066 - val_loss: 8.9497 - val_acc: 0.4184\n",
      "Epoch 375/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.5363 - acc: 0.8036 - val_loss: 8.8416 - val_acc: 0.4136\n",
      "Epoch 376/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.5314 - acc: 0.8040 - val_loss: 8.7391 - val_acc: 0.4162\n",
      "Epoch 377/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.5295 - acc: 0.8053 - val_loss: 8.6855 - val_acc: 0.4138\n",
      "Epoch 378/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.5278 - acc: 0.8071 - val_loss: 8.9775 - val_acc: 0.4208\n",
      "Epoch 379/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5362 - acc: 0.8039 - val_loss: 8.8536 - val_acc: 0.4164\n",
      "Epoch 380/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.5311 - acc: 0.8078 - val_loss: 8.8588 - val_acc: 0.4173\n",
      "Epoch 381/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.5166 - acc: 0.8114 - val_loss: 8.7183 - val_acc: 0.4149\n",
      "Epoch 382/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.5271 - acc: 0.8086 - val_loss: 9.0250 - val_acc: 0.4203\n",
      "Epoch 383/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5200 - acc: 0.8103 - val_loss: 9.0444 - val_acc: 0.4201\n",
      "Epoch 384/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.5233 - acc: 0.8097 - val_loss: 8.9421 - val_acc: 0.4173\n",
      "Epoch 385/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.5222 - acc: 0.8091 - val_loss: 8.9877 - val_acc: 0.4192\n",
      "Epoch 386/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.5226 - acc: 0.8092 - val_loss: 8.9424 - val_acc: 0.4175\n",
      "Epoch 387/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.5228 - acc: 0.8079 - val_loss: 9.0430 - val_acc: 0.4201\n",
      "Epoch 388/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.5107 - acc: 0.8119 - val_loss: 9.0828 - val_acc: 0.4208\n",
      "Epoch 389/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.5197 - acc: 0.8090 - val_loss: 9.0009 - val_acc: 0.4186\n",
      "Epoch 390/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.5170 - acc: 0.8124 - val_loss: 8.9473 - val_acc: 0.4188\n",
      "Epoch 391/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5208 - acc: 0.8099 - val_loss: 8.9176 - val_acc: 0.4166\n",
      "Epoch 392/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.5101 - acc: 0.8133 - val_loss: 8.8124 - val_acc: 0.4158\n",
      "Epoch 393/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.5112 - acc: 0.8120 - val_loss: 8.7977 - val_acc: 0.4142\n",
      "Epoch 394/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5108 - acc: 0.8122 - val_loss: 8.8788 - val_acc: 0.4164\n",
      "Epoch 395/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.5172 - acc: 0.8107 - val_loss: 8.8872 - val_acc: 0.4175\n",
      "Epoch 396/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.5086 - acc: 0.8137 - val_loss: 8.7109 - val_acc: 0.4138\n",
      "Epoch 397/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.5099 - acc: 0.8144 - val_loss: 8.6971 - val_acc: 0.4136\n",
      "Epoch 398/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.5041 - acc: 0.8175 - val_loss: 8.9584 - val_acc: 0.4192\n",
      "Epoch 399/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.5089 - acc: 0.8133 - val_loss: 8.7180 - val_acc: 0.4145\n",
      "Epoch 400/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.5046 - acc: 0.8164 - val_loss: 8.6332 - val_acc: 0.4142\n",
      "Epoch 401/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.5100 - acc: 0.8138 - val_loss: 8.7930 - val_acc: 0.4166\n",
      "Epoch 402/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4974 - acc: 0.8181 - val_loss: 8.6087 - val_acc: 0.4131\n",
      "Epoch 403/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.5071 - acc: 0.8165 - val_loss: 8.6439 - val_acc: 0.4136\n",
      "Epoch 404/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.5033 - acc: 0.8176 - val_loss: 8.7170 - val_acc: 0.4166\n",
      "Epoch 405/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.4960 - acc: 0.8182 - val_loss: 8.8180 - val_acc: 0.4168\n",
      "Epoch 406/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.4957 - acc: 0.8207 - val_loss: 8.6632 - val_acc: 0.4136\n",
      "Epoch 407/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.4900 - acc: 0.8232 - val_loss: 8.9103 - val_acc: 0.4173\n",
      "Epoch 408/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.4968 - acc: 0.8189 - val_loss: 8.8615 - val_acc: 0.4168\n",
      "Epoch 409/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4909 - acc: 0.8190 - val_loss: 8.6957 - val_acc: 0.4138\n",
      "Epoch 410/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.4936 - acc: 0.8200 - val_loss: 9.0315 - val_acc: 0.4234\n",
      "Epoch 411/1000\n",
      "41259/41259 [==============================] - 42s 1ms/step - loss: 0.4840 - acc: 0.8231 - val_loss: 8.9884 - val_acc: 0.4201\n",
      "Epoch 412/1000\n",
      "41259/41259 [==============================] - 41s 1ms/step - loss: 0.4846 - acc: 0.8220 - val_loss: 8.9037 - val_acc: 0.4182\n",
      "Epoch 413/1000\n",
      "41259/41259 [==============================] - 41s 998us/step - loss: 0.4858 - acc: 0.8236 - val_loss: 8.7585 - val_acc: 0.4149\n",
      "Epoch 414/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4886 - acc: 0.8222 - val_loss: 8.6882 - val_acc: 0.4110\n",
      "Epoch 415/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4851 - acc: 0.8223 - val_loss: 8.6419 - val_acc: 0.4160\n",
      "Epoch 416/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4789 - acc: 0.8269 - val_loss: 8.7865 - val_acc: 0.4164\n",
      "Epoch 417/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.4814 - acc: 0.8233 - val_loss: 9.0175 - val_acc: 0.4203\n",
      "Epoch 418/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4845 - acc: 0.8218 - val_loss: 8.8706 - val_acc: 0.4186\n",
      "Epoch 419/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4832 - acc: 0.8237 - val_loss: 8.8225 - val_acc: 0.4147\n",
      "Epoch 420/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4831 - acc: 0.8259 - val_loss: 8.8758 - val_acc: 0.4166\n",
      "Epoch 421/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4793 - acc: 0.8242 - val_loss: 9.0379 - val_acc: 0.4221\n",
      "Epoch 422/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.4724 - acc: 0.8301 - val_loss: 8.9491 - val_acc: 0.4184\n",
      "Epoch 423/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4784 - acc: 0.8263 - val_loss: 9.0673 - val_acc: 0.4227\n",
      "Epoch 424/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4745 - acc: 0.8271 - val_loss: 9.0052 - val_acc: 0.4210\n",
      "Epoch 425/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4735 - acc: 0.8281 - val_loss: 8.9701 - val_acc: 0.4186\n",
      "Epoch 426/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4732 - acc: 0.8289 - val_loss: 8.9250 - val_acc: 0.4179\n",
      "Epoch 427/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.4708 - acc: 0.8262 - val_loss: 8.9940 - val_acc: 0.4212\n",
      "Epoch 428/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4733 - acc: 0.8277 - val_loss: 8.9113 - val_acc: 0.4199\n",
      "Epoch 429/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4604 - acc: 0.8329 - val_loss: 9.0289 - val_acc: 0.4229\n",
      "Epoch 430/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.4719 - acc: 0.8284 - val_loss: 8.9671 - val_acc: 0.4216\n",
      "Epoch 431/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4671 - acc: 0.8310 - val_loss: 8.9013 - val_acc: 0.4171\n",
      "Epoch 432/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.4642 - acc: 0.8329 - val_loss: 8.9119 - val_acc: 0.4182\n",
      "Epoch 433/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.4720 - acc: 0.8275 - val_loss: 8.9288 - val_acc: 0.4177\n",
      "Epoch 434/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4580 - acc: 0.8348 - val_loss: 8.9563 - val_acc: 0.4184\n",
      "Epoch 435/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4614 - acc: 0.8320 - val_loss: 8.9900 - val_acc: 0.4205\n",
      "Epoch 436/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4576 - acc: 0.8327 - val_loss: 9.0656 - val_acc: 0.4221\n",
      "Epoch 437/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.4540 - acc: 0.8352 - val_loss: 9.0515 - val_acc: 0.4216\n",
      "Epoch 438/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.4559 - acc: 0.8347 - val_loss: 8.9575 - val_acc: 0.4164\n",
      "Epoch 439/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4581 - acc: 0.8334 - val_loss: 8.9645 - val_acc: 0.4182\n",
      "Epoch 440/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.4625 - acc: 0.8302 - val_loss: 8.8067 - val_acc: 0.4162\n",
      "Epoch 441/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.4608 - acc: 0.8319 - val_loss: 9.0723 - val_acc: 0.4232\n",
      "Epoch 442/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.4501 - acc: 0.8350 - val_loss: 9.0235 - val_acc: 0.4201\n",
      "Epoch 443/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4546 - acc: 0.8350 - val_loss: 9.0311 - val_acc: 0.4197\n",
      "Epoch 444/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4568 - acc: 0.8360 - val_loss: 8.9925 - val_acc: 0.4208\n",
      "Epoch 445/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4495 - acc: 0.8375 - val_loss: 8.8246 - val_acc: 0.4145\n",
      "Epoch 446/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.4529 - acc: 0.8358 - val_loss: 9.0648 - val_acc: 0.4216\n",
      "Epoch 447/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.4444 - acc: 0.8358 - val_loss: 9.0226 - val_acc: 0.4199\n",
      "Epoch 448/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4441 - acc: 0.8361 - val_loss: 8.9422 - val_acc: 0.4186\n",
      "Epoch 449/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4556 - acc: 0.8343 - val_loss: 8.9915 - val_acc: 0.4210\n",
      "Epoch 450/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4491 - acc: 0.8371 - val_loss: 8.9902 - val_acc: 0.4197\n",
      "Epoch 451/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4471 - acc: 0.8363 - val_loss: 9.0722 - val_acc: 0.4201\n",
      "Epoch 452/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.4473 - acc: 0.8372 - val_loss: 9.1108 - val_acc: 0.4223\n",
      "Epoch 453/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.4405 - acc: 0.8378 - val_loss: 8.9072 - val_acc: 0.4168\n",
      "Epoch 454/1000\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 0.4370 - acc: 0.8424 - val_loss: 9.0789 - val_acc: 0.4201\n",
      "Epoch 455/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4351 - acc: 0.8427 - val_loss: 8.9859 - val_acc: 0.4173\n",
      "Epoch 456/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.4453 - acc: 0.8399 - val_loss: 8.9350 - val_acc: 0.4164\n",
      "Epoch 457/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.4410 - acc: 0.8401 - val_loss: 8.9857 - val_acc: 0.4173\n",
      "Epoch 458/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4379 - acc: 0.8403 - val_loss: 9.0628 - val_acc: 0.4214\n",
      "Epoch 459/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.4348 - acc: 0.8420 - val_loss: 8.9539 - val_acc: 0.4166\n",
      "Epoch 460/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.4375 - acc: 0.8406 - val_loss: 8.9644 - val_acc: 0.4162\n",
      "Epoch 461/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.4341 - acc: 0.8433 - val_loss: 8.8895 - val_acc: 0.4158\n",
      "Epoch 462/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4387 - acc: 0.8427 - val_loss: 9.0832 - val_acc: 0.4223\n",
      "Epoch 463/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4349 - acc: 0.8411 - val_loss: 8.7975 - val_acc: 0.4136\n",
      "Epoch 464/1000\n",
      "41259/41259 [==============================] - 41s 999us/step - loss: 0.4410 - acc: 0.8413 - val_loss: 8.8133 - val_acc: 0.4164\n",
      "Epoch 465/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.4295 - acc: 0.8439 - val_loss: 8.7336 - val_acc: 0.4136\n",
      "Epoch 466/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.4366 - acc: 0.8421 - val_loss: 8.9177 - val_acc: 0.4173\n",
      "Epoch 467/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 0.4290 - acc: 0.8462 - val_loss: 8.9865 - val_acc: 0.4205\n",
      "Epoch 468/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4351 - acc: 0.8430 - val_loss: 9.0541 - val_acc: 0.4221\n",
      "Epoch 469/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4354 - acc: 0.8442 - val_loss: 8.9052 - val_acc: 0.4147\n",
      "Epoch 470/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.4338 - acc: 0.8422 - val_loss: 8.9702 - val_acc: 0.4214\n",
      "Epoch 471/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4417 - acc: 0.8389 - val_loss: 8.9219 - val_acc: 0.4177\n",
      "Epoch 472/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.4346 - acc: 0.8427 - val_loss: 8.9986 - val_acc: 0.4208\n",
      "Epoch 473/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.4316 - acc: 0.8444 - val_loss: 8.6506 - val_acc: 0.4127\n",
      "Epoch 474/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.4240 - acc: 0.8459 - val_loss: 8.9927 - val_acc: 0.4184\n",
      "Epoch 475/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.4301 - acc: 0.8442 - val_loss: 8.9711 - val_acc: 0.4177\n",
      "Epoch 476/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.4276 - acc: 0.8469 - val_loss: 8.9714 - val_acc: 0.4188\n",
      "Epoch 477/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.4209 - acc: 0.8494 - val_loss: 8.9634 - val_acc: 0.4197\n",
      "Epoch 478/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.4225 - acc: 0.8466 - val_loss: 8.9803 - val_acc: 0.4162\n",
      "Epoch 479/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.4157 - acc: 0.8492 - val_loss: 9.0155 - val_acc: 0.4197\n",
      "Epoch 480/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.4263 - acc: 0.8452 - val_loss: 8.8880 - val_acc: 0.4160\n",
      "Epoch 481/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4206 - acc: 0.8459 - val_loss: 8.7952 - val_acc: 0.4140\n",
      "Epoch 482/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.4245 - acc: 0.8472 - val_loss: 9.0476 - val_acc: 0.4223\n",
      "Epoch 483/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.4249 - acc: 0.8471 - val_loss: 9.0071 - val_acc: 0.4188\n",
      "Epoch 484/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.4215 - acc: 0.8466 - val_loss: 8.7118 - val_acc: 0.4136\n",
      "Epoch 485/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4187 - acc: 0.8485 - val_loss: 8.9556 - val_acc: 0.4158\n",
      "Epoch 486/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.4126 - acc: 0.8493 - val_loss: 8.9850 - val_acc: 0.4171\n",
      "Epoch 487/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.4234 - acc: 0.8469 - val_loss: 9.0006 - val_acc: 0.4155\n",
      "Epoch 488/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.4228 - acc: 0.8473 - val_loss: 9.0486 - val_acc: 0.4208\n",
      "Epoch 489/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.4225 - acc: 0.8488 - val_loss: 9.0572 - val_acc: 0.4205\n",
      "Epoch 490/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.4184 - acc: 0.8487 - val_loss: 9.0479 - val_acc: 0.4212\n",
      "Epoch 491/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4187 - acc: 0.8491 - val_loss: 9.0624 - val_acc: 0.4212\n",
      "Epoch 492/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.4245 - acc: 0.8463 - val_loss: 9.1122 - val_acc: 0.4214\n",
      "Epoch 493/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.4165 - acc: 0.8478 - val_loss: 9.0534 - val_acc: 0.4177\n",
      "Epoch 494/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.4084 - acc: 0.8524 - val_loss: 8.9698 - val_acc: 0.4158\n",
      "Epoch 495/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.4166 - acc: 0.8486 - val_loss: 9.0409 - val_acc: 0.4225\n",
      "Epoch 496/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.4238 - acc: 0.8472 - val_loss: 9.0780 - val_acc: 0.4227\n",
      "Epoch 497/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.4135 - acc: 0.8507 - val_loss: 9.0876 - val_acc: 0.4216\n",
      "Epoch 498/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.4076 - acc: 0.8531 - val_loss: 9.0886 - val_acc: 0.4188\n",
      "Epoch 499/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4058 - acc: 0.8530 - val_loss: 9.0729 - val_acc: 0.4208\n",
      "Epoch 500/1000\n",
      "41259/41259 [==============================] - 41s 992us/step - loss: 0.4142 - acc: 0.8513 - val_loss: 9.0073 - val_acc: 0.4151\n",
      "Epoch 501/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.4092 - acc: 0.8521 - val_loss: 9.0158 - val_acc: 0.4166\n",
      "Epoch 502/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.4043 - acc: 0.8516 - val_loss: 9.0799 - val_acc: 0.4236\n",
      "Epoch 503/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.4044 - acc: 0.8550 - val_loss: 8.9947 - val_acc: 0.4160\n",
      "Epoch 504/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.4180 - acc: 0.8472 - val_loss: 9.0302 - val_acc: 0.4179\n",
      "Epoch 505/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.4106 - acc: 0.8530 - val_loss: 9.1071 - val_acc: 0.4219\n",
      "Epoch 506/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.4087 - acc: 0.8522 - val_loss: 9.0613 - val_acc: 0.4208\n",
      "Epoch 507/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.4012 - acc: 0.8550 - val_loss: 9.0466 - val_acc: 0.4175\n",
      "Epoch 508/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 0.4174 - acc: 0.8519 - val_loss: 9.0876 - val_acc: 0.4221\n",
      "Epoch 509/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.4118 - acc: 0.8516 - val_loss: 8.9862 - val_acc: 0.4177\n",
      "Epoch 510/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.3963 - acc: 0.8537 - val_loss: 9.0267 - val_acc: 0.4188\n",
      "Epoch 511/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3959 - acc: 0.8565 - val_loss: 9.0200 - val_acc: 0.4166\n",
      "Epoch 512/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.3917 - acc: 0.8586 - val_loss: 9.1058 - val_acc: 0.4219\n",
      "Epoch 513/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.4002 - acc: 0.8577 - val_loss: 9.0315 - val_acc: 0.4175\n",
      "Epoch 514/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.3997 - acc: 0.8556 - val_loss: 9.0472 - val_acc: 0.4192\n",
      "Epoch 515/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3983 - acc: 0.8557 - val_loss: 9.1085 - val_acc: 0.4221\n",
      "Epoch 516/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3900 - acc: 0.8572 - val_loss: 9.0452 - val_acc: 0.4208\n",
      "Epoch 517/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3921 - acc: 0.8581 - val_loss: 9.0830 - val_acc: 0.4221\n",
      "Epoch 518/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3968 - acc: 0.8565 - val_loss: 9.0953 - val_acc: 0.4223\n",
      "Epoch 519/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3922 - acc: 0.8601 - val_loss: 9.1076 - val_acc: 0.4229\n",
      "Epoch 520/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.4007 - acc: 0.8554 - val_loss: 9.0670 - val_acc: 0.4208\n",
      "Epoch 521/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3945 - acc: 0.8579 - val_loss: 9.1113 - val_acc: 0.4221\n",
      "Epoch 522/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3848 - acc: 0.8618 - val_loss: 9.1232 - val_acc: 0.4225\n",
      "Epoch 523/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3941 - acc: 0.8590 - val_loss: 9.0774 - val_acc: 0.4203\n",
      "Epoch 524/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3935 - acc: 0.8579 - val_loss: 9.0803 - val_acc: 0.4225\n",
      "Epoch 525/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3899 - acc: 0.8591 - val_loss: 9.0292 - val_acc: 0.4162\n",
      "Epoch 526/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3890 - acc: 0.8588 - val_loss: 9.0829 - val_acc: 0.4214\n",
      "Epoch 527/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3839 - acc: 0.8610 - val_loss: 9.1111 - val_acc: 0.4195\n",
      "Epoch 528/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.3901 - acc: 0.8606 - val_loss: 9.1582 - val_acc: 0.4238\n",
      "Epoch 529/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3936 - acc: 0.8588 - val_loss: 9.0824 - val_acc: 0.4184\n",
      "Epoch 530/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3820 - acc: 0.8633 - val_loss: 9.1147 - val_acc: 0.4216\n",
      "Epoch 531/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3772 - acc: 0.8635 - val_loss: 9.1214 - val_acc: 0.4216\n",
      "Epoch 532/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3841 - acc: 0.8626 - val_loss: 9.0873 - val_acc: 0.4192\n",
      "Epoch 533/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.3782 - acc: 0.8647 - val_loss: 9.0072 - val_acc: 0.4153\n",
      "Epoch 534/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.3781 - acc: 0.8633 - val_loss: 9.0442 - val_acc: 0.4162\n",
      "Epoch 535/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.3716 - acc: 0.8661 - val_loss: 9.1071 - val_acc: 0.4229\n",
      "Epoch 536/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3773 - acc: 0.8639 - val_loss: 8.9335 - val_acc: 0.4138\n",
      "Epoch 537/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3777 - acc: 0.8637 - val_loss: 8.9484 - val_acc: 0.4138\n",
      "Epoch 538/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3779 - acc: 0.8653 - val_loss: 8.9735 - val_acc: 0.4162\n",
      "Epoch 539/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3724 - acc: 0.8670 - val_loss: 9.0619 - val_acc: 0.4203\n",
      "Epoch 540/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3758 - acc: 0.8641 - val_loss: 9.0524 - val_acc: 0.4197\n",
      "Epoch 541/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.3715 - acc: 0.8657 - val_loss: 8.9985 - val_acc: 0.4149\n",
      "Epoch 542/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.3713 - acc: 0.8655 - val_loss: 9.0488 - val_acc: 0.4182\n",
      "Epoch 543/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3748 - acc: 0.8652 - val_loss: 9.0900 - val_acc: 0.4216\n",
      "Epoch 544/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.3725 - acc: 0.8651 - val_loss: 8.9796 - val_acc: 0.4151\n",
      "Epoch 545/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3694 - acc: 0.8658 - val_loss: 9.0293 - val_acc: 0.4166\n",
      "Epoch 546/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3682 - acc: 0.8673 - val_loss: 9.0357 - val_acc: 0.4153\n",
      "Epoch 547/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.3775 - acc: 0.8658 - val_loss: 9.0362 - val_acc: 0.4153\n",
      "Epoch 548/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3751 - acc: 0.8656 - val_loss: 8.9266 - val_acc: 0.4140\n",
      "Epoch 549/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.3709 - acc: 0.8680 - val_loss: 9.0338 - val_acc: 0.4151\n",
      "Epoch 550/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 0.3636 - acc: 0.8693 - val_loss: 9.0528 - val_acc: 0.4149\n",
      "Epoch 551/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3741 - acc: 0.8673 - val_loss: 9.0799 - val_acc: 0.4179\n",
      "Epoch 552/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3723 - acc: 0.8668 - val_loss: 9.1083 - val_acc: 0.4205\n",
      "Epoch 553/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3713 - acc: 0.8667 - val_loss: 9.0078 - val_acc: 0.4151\n",
      "Epoch 554/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.3620 - acc: 0.8706 - val_loss: 9.0957 - val_acc: 0.4216\n",
      "Epoch 555/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.3583 - acc: 0.8717 - val_loss: 9.0560 - val_acc: 0.4166\n",
      "Epoch 556/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.3605 - acc: 0.8705 - val_loss: 9.0696 - val_acc: 0.4182\n",
      "Epoch 557/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3579 - acc: 0.8720 - val_loss: 9.1045 - val_acc: 0.4212\n",
      "Epoch 558/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3668 - acc: 0.8678 - val_loss: 9.0557 - val_acc: 0.4164\n",
      "Epoch 559/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3704 - acc: 0.8689 - val_loss: 8.9107 - val_acc: 0.4123\n",
      "Epoch 560/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.3557 - acc: 0.8713 - val_loss: 9.1118 - val_acc: 0.4179\n",
      "Epoch 561/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3575 - acc: 0.8700 - val_loss: 9.1220 - val_acc: 0.4184\n",
      "Epoch 562/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.3583 - acc: 0.8716 - val_loss: 9.1289 - val_acc: 0.4208\n",
      "Epoch 563/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3478 - acc: 0.8760 - val_loss: 9.0672 - val_acc: 0.4175\n",
      "Epoch 564/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3564 - acc: 0.8730 - val_loss: 9.0515 - val_acc: 0.4153\n",
      "Epoch 565/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3572 - acc: 0.8727 - val_loss: 9.0169 - val_acc: 0.4151\n",
      "Epoch 566/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3581 - acc: 0.8733 - val_loss: 9.0841 - val_acc: 0.4153\n",
      "Epoch 567/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3465 - acc: 0.8751 - val_loss: 9.0915 - val_acc: 0.4190\n",
      "Epoch 568/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3499 - acc: 0.8743 - val_loss: 9.0882 - val_acc: 0.4182\n",
      "Epoch 569/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.3519 - acc: 0.8736 - val_loss: 9.1041 - val_acc: 0.4216\n",
      "Epoch 570/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3543 - acc: 0.8729 - val_loss: 9.1185 - val_acc: 0.4216\n",
      "Epoch 571/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3497 - acc: 0.8738 - val_loss: 9.1038 - val_acc: 0.4212\n",
      "Epoch 572/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3459 - acc: 0.8754 - val_loss: 9.0748 - val_acc: 0.4208\n",
      "Epoch 573/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3556 - acc: 0.8742 - val_loss: 9.0739 - val_acc: 0.4205\n",
      "Epoch 574/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.3530 - acc: 0.8757 - val_loss: 9.0550 - val_acc: 0.4192\n",
      "Epoch 575/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3513 - acc: 0.8737 - val_loss: 9.0385 - val_acc: 0.4175\n",
      "Epoch 576/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3564 - acc: 0.8750 - val_loss: 9.0804 - val_acc: 0.4186\n",
      "Epoch 577/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3559 - acc: 0.8724 - val_loss: 9.1156 - val_acc: 0.4214\n",
      "Epoch 578/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3467 - acc: 0.8769 - val_loss: 9.1071 - val_acc: 0.4212\n",
      "Epoch 579/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3489 - acc: 0.8745 - val_loss: 9.0984 - val_acc: 0.4214\n",
      "Epoch 580/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3436 - acc: 0.8752 - val_loss: 9.0523 - val_acc: 0.4197\n",
      "Epoch 581/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3528 - acc: 0.8738 - val_loss: 9.0169 - val_acc: 0.4175\n",
      "Epoch 582/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.3560 - acc: 0.8729 - val_loss: 9.0837 - val_acc: 0.4197\n",
      "Epoch 583/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3431 - acc: 0.8768 - val_loss: 9.1149 - val_acc: 0.4214\n",
      "Epoch 584/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3425 - acc: 0.8780 - val_loss: 9.1071 - val_acc: 0.4205\n",
      "Epoch 585/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3547 - acc: 0.8746 - val_loss: 9.0842 - val_acc: 0.4190\n",
      "Epoch 586/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3595 - acc: 0.8715 - val_loss: 9.0847 - val_acc: 0.4197\n",
      "Epoch 587/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.3454 - acc: 0.8754 - val_loss: 9.0668 - val_acc: 0.4201\n",
      "Epoch 588/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3438 - acc: 0.8759 - val_loss: 9.1186 - val_acc: 0.4205\n",
      "Epoch 589/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3427 - acc: 0.8792 - val_loss: 9.1161 - val_acc: 0.4201\n",
      "Epoch 590/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3373 - acc: 0.8786 - val_loss: 9.0696 - val_acc: 0.4182\n",
      "Epoch 591/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3397 - acc: 0.8789 - val_loss: 9.0945 - val_acc: 0.4192\n",
      "Epoch 592/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3386 - acc: 0.8789 - val_loss: 9.0844 - val_acc: 0.4192\n",
      "Epoch 593/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3401 - acc: 0.8777 - val_loss: 9.0765 - val_acc: 0.4197\n",
      "Epoch 594/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3373 - acc: 0.8795 - val_loss: 9.0282 - val_acc: 0.4179\n",
      "Epoch 595/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3481 - acc: 0.8749 - val_loss: 9.0832 - val_acc: 0.4197\n",
      "Epoch 596/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3475 - acc: 0.8756 - val_loss: 9.1344 - val_acc: 0.4210\n",
      "Epoch 597/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.3406 - acc: 0.8777 - val_loss: 9.1353 - val_acc: 0.4219\n",
      "Epoch 598/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3488 - acc: 0.8739 - val_loss: 9.0814 - val_acc: 0.4197\n",
      "Epoch 599/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.3447 - acc: 0.8750 - val_loss: 9.1021 - val_acc: 0.4197\n",
      "Epoch 600/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3387 - acc: 0.8790 - val_loss: 9.1294 - val_acc: 0.4214\n",
      "Epoch 601/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3339 - acc: 0.8813 - val_loss: 9.0338 - val_acc: 0.4155\n",
      "Epoch 602/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3436 - acc: 0.8770 - val_loss: 8.9494 - val_acc: 0.4155\n",
      "Epoch 603/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3344 - acc: 0.8800 - val_loss: 9.1245 - val_acc: 0.4221\n",
      "Epoch 604/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.3280 - acc: 0.8825 - val_loss: 9.1163 - val_acc: 0.4210\n",
      "Epoch 605/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3429 - acc: 0.8786 - val_loss: 9.0612 - val_acc: 0.4188\n",
      "Epoch 606/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3234 - acc: 0.8831 - val_loss: 9.0651 - val_acc: 0.4186\n",
      "Epoch 607/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3369 - acc: 0.8814 - val_loss: 9.1066 - val_acc: 0.4208\n",
      "Epoch 608/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3402 - acc: 0.8783 - val_loss: 9.0940 - val_acc: 0.4184\n",
      "Epoch 609/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3326 - acc: 0.8821 - val_loss: 9.1335 - val_acc: 0.4223\n",
      "Epoch 610/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3329 - acc: 0.8811 - val_loss: 9.0852 - val_acc: 0.4188\n",
      "Epoch 611/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3227 - acc: 0.8846 - val_loss: 9.1128 - val_acc: 0.4208\n",
      "Epoch 612/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.3250 - acc: 0.8843 - val_loss: 9.0197 - val_acc: 0.4160\n",
      "Epoch 613/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3331 - acc: 0.8809 - val_loss: 9.1119 - val_acc: 0.4199\n",
      "Epoch 614/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3338 - acc: 0.8798 - val_loss: 9.1016 - val_acc: 0.4197\n",
      "Epoch 615/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3308 - acc: 0.8820 - val_loss: 9.1191 - val_acc: 0.4199\n",
      "Epoch 616/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3276 - acc: 0.8816 - val_loss: 9.0900 - val_acc: 0.4190\n",
      "Epoch 617/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3396 - acc: 0.8787 - val_loss: 9.0435 - val_acc: 0.4177\n",
      "Epoch 618/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3297 - acc: 0.8843 - val_loss: 8.9718 - val_acc: 0.4160\n",
      "Epoch 619/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3384 - acc: 0.8796 - val_loss: 9.0606 - val_acc: 0.4195\n",
      "Epoch 620/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.3359 - acc: 0.8789 - val_loss: 8.9942 - val_acc: 0.4184\n",
      "Epoch 621/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3452 - acc: 0.8773 - val_loss: 9.0844 - val_acc: 0.4203\n",
      "Epoch 622/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3318 - acc: 0.8817 - val_loss: 9.1532 - val_acc: 0.4227\n",
      "Epoch 623/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3317 - acc: 0.8814 - val_loss: 9.1141 - val_acc: 0.4216\n",
      "Epoch 624/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3435 - acc: 0.8776 - val_loss: 9.1117 - val_acc: 0.4201\n",
      "Epoch 625/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.3125 - acc: 0.8883 - val_loss: 9.0877 - val_acc: 0.4188\n",
      "Epoch 626/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3264 - acc: 0.8834 - val_loss: 9.0988 - val_acc: 0.4177\n",
      "Epoch 627/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.3145 - acc: 0.8891 - val_loss: 9.0994 - val_acc: 0.4186\n",
      "Epoch 628/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3271 - acc: 0.8827 - val_loss: 9.1514 - val_acc: 0.4190\n",
      "Epoch 629/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3180 - acc: 0.8863 - val_loss: 9.0941 - val_acc: 0.4188\n",
      "Epoch 630/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3205 - acc: 0.8856 - val_loss: 9.0674 - val_acc: 0.4160\n",
      "Epoch 631/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3230 - acc: 0.8851 - val_loss: 9.0986 - val_acc: 0.4186\n",
      "Epoch 632/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.3169 - acc: 0.8857 - val_loss: 9.1360 - val_acc: 0.4205\n",
      "Epoch 633/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3213 - acc: 0.8856 - val_loss: 9.0925 - val_acc: 0.4195\n",
      "Epoch 634/1000\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.3120 - acc: 0.8905 - val_loss: 9.1509 - val_acc: 0.4216\n",
      "Epoch 635/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3210 - acc: 0.8852 - val_loss: 9.1299 - val_acc: 0.4197\n",
      "Epoch 636/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3197 - acc: 0.8865 - val_loss: 9.1624 - val_acc: 0.4227\n",
      "Epoch 637/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3176 - acc: 0.8880 - val_loss: 9.1426 - val_acc: 0.4182\n",
      "Epoch 638/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.3233 - acc: 0.8843 - val_loss: 9.1394 - val_acc: 0.4199\n",
      "Epoch 639/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3170 - acc: 0.8876 - val_loss: 9.1189 - val_acc: 0.4182\n",
      "Epoch 640/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3186 - acc: 0.8859 - val_loss: 9.0820 - val_acc: 0.4166\n",
      "Epoch 641/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.3171 - acc: 0.8870 - val_loss: 9.1344 - val_acc: 0.4195\n",
      "Epoch 642/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3153 - acc: 0.8863 - val_loss: 9.1463 - val_acc: 0.4212\n",
      "Epoch 643/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3174 - acc: 0.8860 - val_loss: 9.1704 - val_acc: 0.4212\n",
      "Epoch 644/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3141 - acc: 0.8888 - val_loss: 9.1666 - val_acc: 0.4232\n",
      "Epoch 645/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.3088 - acc: 0.8887 - val_loss: 9.1437 - val_acc: 0.4216\n",
      "Epoch 646/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3107 - acc: 0.8890 - val_loss: 9.1608 - val_acc: 0.4210\n",
      "Epoch 647/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3072 - acc: 0.8897 - val_loss: 9.1236 - val_acc: 0.4197\n",
      "Epoch 648/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3162 - acc: 0.8881 - val_loss: 9.1208 - val_acc: 0.4201\n",
      "Epoch 649/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3166 - acc: 0.8867 - val_loss: 9.1364 - val_acc: 0.4216\n",
      "Epoch 650/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3085 - acc: 0.8894 - val_loss: 9.1413 - val_acc: 0.4195\n",
      "Epoch 651/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3119 - acc: 0.8898 - val_loss: 9.1556 - val_acc: 0.4203\n",
      "Epoch 652/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3056 - acc: 0.8917 - val_loss: 9.1208 - val_acc: 0.4182\n",
      "Epoch 653/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3087 - acc: 0.8908 - val_loss: 9.1570 - val_acc: 0.4208\n",
      "Epoch 654/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.3121 - acc: 0.8895 - val_loss: 9.1495 - val_acc: 0.4201\n",
      "Epoch 655/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3096 - acc: 0.8891 - val_loss: 9.1381 - val_acc: 0.4192\n",
      "Epoch 656/1000\n",
      "41259/41259 [==============================] - 41s 995us/step - loss: 0.3165 - acc: 0.8876 - val_loss: 9.0409 - val_acc: 0.4138\n",
      "Epoch 657/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3070 - acc: 0.8910 - val_loss: 9.1160 - val_acc: 0.4186\n",
      "Epoch 658/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3052 - acc: 0.8901 - val_loss: 9.1105 - val_acc: 0.4177\n",
      "Epoch 659/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.3080 - acc: 0.8911 - val_loss: 9.1143 - val_acc: 0.4205\n",
      "Epoch 660/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3104 - acc: 0.8910 - val_loss: 9.1425 - val_acc: 0.4208\n",
      "Epoch 661/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3078 - acc: 0.8897 - val_loss: 9.0884 - val_acc: 0.4162\n",
      "Epoch 662/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.3022 - acc: 0.8934 - val_loss: 9.1232 - val_acc: 0.4166\n",
      "Epoch 663/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2996 - acc: 0.8937 - val_loss: 9.0564 - val_acc: 0.4162\n",
      "Epoch 664/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2984 - acc: 0.8955 - val_loss: 9.1276 - val_acc: 0.4192\n",
      "Epoch 665/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2971 - acc: 0.8927 - val_loss: 9.1183 - val_acc: 0.4182\n",
      "Epoch 666/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2937 - acc: 0.8952 - val_loss: 9.1451 - val_acc: 0.4171\n",
      "Epoch 667/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3018 - acc: 0.8935 - val_loss: 9.1256 - val_acc: 0.4173\n",
      "Epoch 668/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.3054 - acc: 0.8928 - val_loss: 9.1675 - val_acc: 0.4192\n",
      "Epoch 669/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2951 - acc: 0.8948 - val_loss: 9.1451 - val_acc: 0.4184\n",
      "Epoch 670/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3044 - acc: 0.8921 - val_loss: 9.1222 - val_acc: 0.4162\n",
      "Epoch 671/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2955 - acc: 0.8971 - val_loss: 9.1489 - val_acc: 0.4195\n",
      "Epoch 672/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2971 - acc: 0.8930 - val_loss: 9.1086 - val_acc: 0.4186\n",
      "Epoch 673/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.3063 - acc: 0.8909 - val_loss: 9.1381 - val_acc: 0.4188\n",
      "Epoch 674/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.3043 - acc: 0.8920 - val_loss: 9.1245 - val_acc: 0.4197\n",
      "Epoch 675/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.3065 - acc: 0.8907 - val_loss: 9.1527 - val_acc: 0.4212\n",
      "Epoch 676/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3048 - acc: 0.8908 - val_loss: 9.0979 - val_acc: 0.4186\n",
      "Epoch 677/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2992 - acc: 0.8926 - val_loss: 9.0918 - val_acc: 0.4182\n",
      "Epoch 678/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.2987 - acc: 0.8930 - val_loss: 9.0797 - val_acc: 0.4192\n",
      "Epoch 679/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2982 - acc: 0.8937 - val_loss: 9.1245 - val_acc: 0.4190\n",
      "Epoch 680/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2998 - acc: 0.8937 - val_loss: 9.1234 - val_acc: 0.4208\n",
      "Epoch 681/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2943 - acc: 0.8953 - val_loss: 9.1071 - val_acc: 0.4199\n",
      "Epoch 682/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2905 - acc: 0.8972 - val_loss: 9.0788 - val_acc: 0.4153\n",
      "Epoch 683/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2975 - acc: 0.8955 - val_loss: 9.1074 - val_acc: 0.4175\n",
      "Epoch 684/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.2997 - acc: 0.8935 - val_loss: 9.0491 - val_acc: 0.4166\n",
      "Epoch 685/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.3033 - acc: 0.8912 - val_loss: 9.0941 - val_acc: 0.4175\n",
      "Epoch 686/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2890 - acc: 0.8983 - val_loss: 9.1127 - val_acc: 0.4182\n",
      "Epoch 687/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2930 - acc: 0.8972 - val_loss: 9.1363 - val_acc: 0.4197\n",
      "Epoch 688/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2897 - acc: 0.8978 - val_loss: 9.0940 - val_acc: 0.4199\n",
      "Epoch 689/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.3046 - acc: 0.8923 - val_loss: 9.1101 - val_acc: 0.4164\n",
      "Epoch 690/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2994 - acc: 0.8951 - val_loss: 9.0671 - val_acc: 0.4186\n",
      "Epoch 691/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2887 - acc: 0.8968 - val_loss: 9.1323 - val_acc: 0.4195\n",
      "Epoch 692/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2885 - acc: 0.8976 - val_loss: 9.1490 - val_acc: 0.4208\n",
      "Epoch 693/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2930 - acc: 0.8959 - val_loss: 9.1322 - val_acc: 0.4188\n",
      "Epoch 694/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2929 - acc: 0.8958 - val_loss: 9.1656 - val_acc: 0.4214\n",
      "Epoch 695/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2893 - acc: 0.8954 - val_loss: 9.1349 - val_acc: 0.4188\n",
      "Epoch 696/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2892 - acc: 0.8964 - val_loss: 9.1418 - val_acc: 0.4184\n",
      "Epoch 697/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2839 - acc: 0.8982 - val_loss: 9.1344 - val_acc: 0.4208\n",
      "Epoch 698/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2862 - acc: 0.8994 - val_loss: 9.1255 - val_acc: 0.4205\n",
      "Epoch 699/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2946 - acc: 0.8966 - val_loss: 9.1166 - val_acc: 0.4199\n",
      "Epoch 700/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2829 - acc: 0.9006 - val_loss: 9.1412 - val_acc: 0.4205\n",
      "Epoch 701/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2969 - acc: 0.8950 - val_loss: 9.1281 - val_acc: 0.4190\n",
      "Epoch 702/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2826 - acc: 0.8991 - val_loss: 9.1565 - val_acc: 0.4208\n",
      "Epoch 703/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2778 - acc: 0.9009 - val_loss: 9.1186 - val_acc: 0.4210\n",
      "Epoch 704/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2853 - acc: 0.8975 - val_loss: 9.1174 - val_acc: 0.4192\n",
      "Epoch 705/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2902 - acc: 0.8977 - val_loss: 9.1672 - val_acc: 0.4223\n",
      "Epoch 706/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2887 - acc: 0.8979 - val_loss: 9.1749 - val_acc: 0.4210\n",
      "Epoch 707/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2850 - acc: 0.8999 - val_loss: 9.1389 - val_acc: 0.4195\n",
      "Epoch 708/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2934 - acc: 0.8941 - val_loss: 9.1694 - val_acc: 0.4227\n",
      "Epoch 709/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.2858 - acc: 0.8992 - val_loss: 9.1150 - val_acc: 0.4199\n",
      "Epoch 710/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2909 - acc: 0.8964 - val_loss: 9.0792 - val_acc: 0.4173\n",
      "Epoch 711/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2794 - acc: 0.9005 - val_loss: 9.1398 - val_acc: 0.4219\n",
      "Epoch 712/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2827 - acc: 0.9002 - val_loss: 9.1203 - val_acc: 0.4214\n",
      "Epoch 713/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2840 - acc: 0.8987 - val_loss: 9.0974 - val_acc: 0.4177\n",
      "Epoch 714/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2704 - acc: 0.9036 - val_loss: 9.1323 - val_acc: 0.4210\n",
      "Epoch 715/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2774 - acc: 0.9006 - val_loss: 9.1464 - val_acc: 0.4216\n",
      "Epoch 716/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2860 - acc: 0.8989 - val_loss: 9.1358 - val_acc: 0.4212\n",
      "Epoch 717/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2815 - acc: 0.8997 - val_loss: 9.1415 - val_acc: 0.4177\n",
      "Epoch 718/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2769 - acc: 0.9013 - val_loss: 9.1304 - val_acc: 0.4195\n",
      "Epoch 719/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2829 - acc: 0.9017 - val_loss: 9.1598 - val_acc: 0.4212\n",
      "Epoch 720/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2811 - acc: 0.9004 - val_loss: 9.1359 - val_acc: 0.4203\n",
      "Epoch 721/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2810 - acc: 0.9003 - val_loss: 9.1636 - val_acc: 0.4212\n",
      "Epoch 722/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.2822 - acc: 0.9000 - val_loss: 9.1653 - val_acc: 0.4210\n",
      "Epoch 723/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2780 - acc: 0.9005 - val_loss: 9.1764 - val_acc: 0.4212\n",
      "Epoch 724/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2757 - acc: 0.9009 - val_loss: 9.1701 - val_acc: 0.4212\n",
      "Epoch 725/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2746 - acc: 0.9027 - val_loss: 9.1961 - val_acc: 0.4236\n",
      "Epoch 726/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2814 - acc: 0.9004 - val_loss: 9.1932 - val_acc: 0.4238\n",
      "Epoch 727/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2884 - acc: 0.8981 - val_loss: 9.1667 - val_acc: 0.4219\n",
      "Epoch 728/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.2816 - acc: 0.9005 - val_loss: 9.1483 - val_acc: 0.4203\n",
      "Epoch 729/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2831 - acc: 0.9006 - val_loss: 9.1616 - val_acc: 0.4221\n",
      "Epoch 730/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2699 - acc: 0.9043 - val_loss: 9.1421 - val_acc: 0.4188\n",
      "Epoch 731/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2739 - acc: 0.9033 - val_loss: 9.1559 - val_acc: 0.4221\n",
      "Epoch 732/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.2769 - acc: 0.9038 - val_loss: 9.1351 - val_acc: 0.4214\n",
      "Epoch 733/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.2752 - acc: 0.9045 - val_loss: 9.1578 - val_acc: 0.4219\n",
      "Epoch 734/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.2746 - acc: 0.9030 - val_loss: 9.1587 - val_acc: 0.4221\n",
      "Epoch 735/1000\n",
      "41259/41259 [==============================] - 41s 987us/step - loss: 0.2824 - acc: 0.9006 - val_loss: 9.1469 - val_acc: 0.4216\n",
      "Epoch 736/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2636 - acc: 0.9059 - val_loss: 9.1149 - val_acc: 0.4205\n",
      "Epoch 737/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2586 - acc: 0.9072 - val_loss: 9.1421 - val_acc: 0.4205\n",
      "Epoch 738/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2749 - acc: 0.9035 - val_loss: 9.1322 - val_acc: 0.4205\n",
      "Epoch 739/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.2700 - acc: 0.9038 - val_loss: 9.1038 - val_acc: 0.4186\n",
      "Epoch 740/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2653 - acc: 0.9056 - val_loss: 9.1401 - val_acc: 0.4195\n",
      "Epoch 741/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2813 - acc: 0.9013 - val_loss: 9.1191 - val_acc: 0.4188\n",
      "Epoch 742/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2600 - acc: 0.9074 - val_loss: 9.1333 - val_acc: 0.4177\n",
      "Epoch 743/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2759 - acc: 0.9028 - val_loss: 9.1429 - val_acc: 0.4205\n",
      "Epoch 744/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2732 - acc: 0.9039 - val_loss: 9.1384 - val_acc: 0.4208\n",
      "Epoch 745/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2816 - acc: 0.9008 - val_loss: 9.0893 - val_acc: 0.4168\n",
      "Epoch 746/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2694 - acc: 0.9028 - val_loss: 9.1453 - val_acc: 0.4203\n",
      "Epoch 747/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2735 - acc: 0.9033 - val_loss: 9.1548 - val_acc: 0.4214\n",
      "Epoch 748/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2665 - acc: 0.9069 - val_loss: 9.1308 - val_acc: 0.4205\n",
      "Epoch 749/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2707 - acc: 0.9071 - val_loss: 9.1526 - val_acc: 0.4210\n",
      "Epoch 750/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2663 - acc: 0.9058 - val_loss: 9.1453 - val_acc: 0.4201\n",
      "Epoch 751/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2622 - acc: 0.9085 - val_loss: 9.1713 - val_acc: 0.4227\n",
      "Epoch 752/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2650 - acc: 0.9065 - val_loss: 9.1453 - val_acc: 0.4208\n",
      "Epoch 753/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2644 - acc: 0.9081 - val_loss: 9.1686 - val_acc: 0.4221\n",
      "Epoch 754/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2584 - acc: 0.9082 - val_loss: 9.1746 - val_acc: 0.4234\n",
      "Epoch 755/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2663 - acc: 0.9062 - val_loss: 9.1551 - val_acc: 0.4205\n",
      "Epoch 756/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2634 - acc: 0.9066 - val_loss: 9.1609 - val_acc: 0.4205\n",
      "Epoch 757/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2623 - acc: 0.9063 - val_loss: 9.1750 - val_acc: 0.4225\n",
      "Epoch 758/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2665 - acc: 0.9072 - val_loss: 9.1253 - val_acc: 0.4190\n",
      "Epoch 759/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2683 - acc: 0.9065 - val_loss: 9.1604 - val_acc: 0.4219\n",
      "Epoch 760/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2548 - acc: 0.9116 - val_loss: 9.1856 - val_acc: 0.4236\n",
      "Epoch 761/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2631 - acc: 0.9079 - val_loss: 9.1588 - val_acc: 0.4227\n",
      "Epoch 762/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2702 - acc: 0.9035 - val_loss: 9.1230 - val_acc: 0.4188\n",
      "Epoch 763/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2675 - acc: 0.9044 - val_loss: 9.1389 - val_acc: 0.4210\n",
      "Epoch 764/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2623 - acc: 0.9066 - val_loss: 9.1707 - val_acc: 0.4225\n",
      "Epoch 765/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2565 - acc: 0.9089 - val_loss: 9.1775 - val_acc: 0.4223\n",
      "Epoch 766/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2630 - acc: 0.9077 - val_loss: 9.1456 - val_acc: 0.4197\n",
      "Epoch 767/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2660 - acc: 0.9059 - val_loss: 9.1583 - val_acc: 0.4221\n",
      "Epoch 768/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.2571 - acc: 0.9097 - val_loss: 9.1422 - val_acc: 0.4201\n",
      "Epoch 769/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2641 - acc: 0.9068 - val_loss: 9.1449 - val_acc: 0.4210\n",
      "Epoch 770/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2668 - acc: 0.9063 - val_loss: 9.1656 - val_acc: 0.4225\n",
      "Epoch 771/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2569 - acc: 0.9084 - val_loss: 9.1341 - val_acc: 0.4203\n",
      "Epoch 772/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.2604 - acc: 0.9079 - val_loss: 9.1410 - val_acc: 0.4199\n",
      "Epoch 773/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2608 - acc: 0.9088 - val_loss: 9.1002 - val_acc: 0.4173\n",
      "Epoch 774/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2601 - acc: 0.9078 - val_loss: 9.1425 - val_acc: 0.4205\n",
      "Epoch 775/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2615 - acc: 0.9090 - val_loss: 9.1667 - val_acc: 0.4219\n",
      "Epoch 776/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2571 - acc: 0.9091 - val_loss: 9.1509 - val_acc: 0.4212\n",
      "Epoch 777/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2508 - acc: 0.9115 - val_loss: 9.1619 - val_acc: 0.4214\n",
      "Epoch 778/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2575 - acc: 0.9085 - val_loss: 9.1634 - val_acc: 0.4212\n",
      "Epoch 779/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2584 - acc: 0.9079 - val_loss: 9.1589 - val_acc: 0.4203\n",
      "Epoch 780/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2606 - acc: 0.9095 - val_loss: 9.1543 - val_acc: 0.4208\n",
      "Epoch 781/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2685 - acc: 0.9058 - val_loss: 9.1690 - val_acc: 0.4210\n",
      "Epoch 782/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2530 - acc: 0.9122 - val_loss: 9.1627 - val_acc: 0.4210\n",
      "Epoch 783/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2483 - acc: 0.9133 - val_loss: 9.1835 - val_acc: 0.4221\n",
      "Epoch 784/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2501 - acc: 0.9121 - val_loss: 9.1419 - val_acc: 0.4205\n",
      "Epoch 785/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2487 - acc: 0.9122 - val_loss: 9.1856 - val_acc: 0.4223\n",
      "Epoch 786/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2441 - acc: 0.9141 - val_loss: 9.1421 - val_acc: 0.4205\n",
      "Epoch 787/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2513 - acc: 0.9104 - val_loss: 9.1605 - val_acc: 0.4203\n",
      "Epoch 788/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2516 - acc: 0.9123 - val_loss: 9.1847 - val_acc: 0.4219\n",
      "Epoch 789/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2554 - acc: 0.9105 - val_loss: 9.1616 - val_acc: 0.4208\n",
      "Epoch 790/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2445 - acc: 0.9151 - val_loss: 9.1739 - val_acc: 0.4216\n",
      "Epoch 791/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2516 - acc: 0.9127 - val_loss: 9.1871 - val_acc: 0.4208\n",
      "Epoch 792/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2613 - acc: 0.9086 - val_loss: 9.1710 - val_acc: 0.4214\n",
      "Epoch 793/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2495 - acc: 0.9110 - val_loss: 9.1598 - val_acc: 0.4199\n",
      "Epoch 794/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2421 - acc: 0.9135 - val_loss: 9.1756 - val_acc: 0.4214\n",
      "Epoch 795/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2524 - acc: 0.9121 - val_loss: 9.1814 - val_acc: 0.4210\n",
      "Epoch 796/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.2509 - acc: 0.9119 - val_loss: 9.1447 - val_acc: 0.4184\n",
      "Epoch 797/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2552 - acc: 0.9109 - val_loss: 9.1679 - val_acc: 0.4221\n",
      "Epoch 798/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2566 - acc: 0.9093 - val_loss: 9.1599 - val_acc: 0.4210\n",
      "Epoch 799/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.2455 - acc: 0.9141 - val_loss: 9.1837 - val_acc: 0.4234\n",
      "Epoch 800/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2494 - acc: 0.9123 - val_loss: 9.1578 - val_acc: 0.4208\n",
      "Epoch 801/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2479 - acc: 0.9131 - val_loss: 9.1850 - val_acc: 0.4232\n",
      "Epoch 802/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2488 - acc: 0.9132 - val_loss: 9.1639 - val_acc: 0.4216\n",
      "Epoch 803/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2396 - acc: 0.9157 - val_loss: 9.1912 - val_acc: 0.4240\n",
      "Epoch 804/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2464 - acc: 0.9130 - val_loss: 9.1903 - val_acc: 0.4232\n",
      "Epoch 805/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2447 - acc: 0.9145 - val_loss: 9.1659 - val_acc: 0.4221\n",
      "Epoch 806/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2440 - acc: 0.9143 - val_loss: 9.1726 - val_acc: 0.4242\n",
      "Epoch 807/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2492 - acc: 0.9134 - val_loss: 9.1547 - val_acc: 0.4221\n",
      "Epoch 808/1000\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.2537 - acc: 0.9100 - val_loss: 9.1621 - val_acc: 0.4227\n",
      "Epoch 809/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2561 - acc: 0.9106 - val_loss: 9.1673 - val_acc: 0.4223\n",
      "Epoch 810/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2513 - acc: 0.9131 - val_loss: 9.1919 - val_acc: 0.4212\n",
      "Epoch 811/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2476 - acc: 0.9121 - val_loss: 9.1690 - val_acc: 0.4223\n",
      "Epoch 812/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2437 - acc: 0.9143 - val_loss: 9.1880 - val_acc: 0.4251\n",
      "Epoch 813/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2424 - acc: 0.9147 - val_loss: 9.1811 - val_acc: 0.4227\n",
      "Epoch 814/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2504 - acc: 0.9121 - val_loss: 9.1536 - val_acc: 0.4221\n",
      "Epoch 815/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2516 - acc: 0.9111 - val_loss: 9.1833 - val_acc: 0.4234\n",
      "Epoch 816/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2470 - acc: 0.9115 - val_loss: 9.1349 - val_acc: 0.4214\n",
      "Epoch 817/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2435 - acc: 0.9152 - val_loss: 9.1770 - val_acc: 0.4227\n",
      "Epoch 818/1000\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.2463 - acc: 0.9148 - val_loss: 9.1899 - val_acc: 0.4240\n",
      "Epoch 819/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2495 - acc: 0.9122 - val_loss: 9.1708 - val_acc: 0.4225\n",
      "Epoch 820/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2553 - acc: 0.9091 - val_loss: 9.1627 - val_acc: 0.4223\n",
      "Epoch 821/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2510 - acc: 0.9125 - val_loss: 9.1439 - val_acc: 0.4227\n",
      "Epoch 822/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2429 - acc: 0.9138 - val_loss: 9.1562 - val_acc: 0.4219\n",
      "Epoch 823/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2516 - acc: 0.9111 - val_loss: 9.1594 - val_acc: 0.4229\n",
      "Epoch 824/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2557 - acc: 0.9109 - val_loss: 9.1481 - val_acc: 0.4203\n",
      "Epoch 825/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2492 - acc: 0.9118 - val_loss: 9.1633 - val_acc: 0.4201\n",
      "Epoch 826/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2453 - acc: 0.9138 - val_loss: 9.1856 - val_acc: 0.4236\n",
      "Epoch 827/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2479 - acc: 0.9124 - val_loss: 9.1665 - val_acc: 0.4223\n",
      "Epoch 828/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.2495 - acc: 0.9128 - val_loss: 9.1614 - val_acc: 0.4214\n",
      "Epoch 829/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2437 - acc: 0.9151 - val_loss: 9.1668 - val_acc: 0.4229\n",
      "Epoch 830/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2410 - acc: 0.9142 - val_loss: 9.1760 - val_acc: 0.4242\n",
      "Epoch 831/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2387 - acc: 0.9174 - val_loss: 9.1837 - val_acc: 0.4219\n",
      "Epoch 832/1000\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.2410 - acc: 0.9166 - val_loss: 9.1592 - val_acc: 0.4186\n",
      "Epoch 833/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2530 - acc: 0.9113 - val_loss: 9.1707 - val_acc: 0.4205\n",
      "Epoch 834/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2499 - acc: 0.9119 - val_loss: 9.1700 - val_acc: 0.4192\n",
      "Epoch 835/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2374 - acc: 0.9154 - val_loss: 9.1623 - val_acc: 0.4203\n",
      "Epoch 836/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2437 - acc: 0.9142 - val_loss: 9.1542 - val_acc: 0.4210\n",
      "Epoch 837/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2411 - acc: 0.9151 - val_loss: 9.1770 - val_acc: 0.4201\n",
      "Epoch 838/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2441 - acc: 0.9138 - val_loss: 9.1649 - val_acc: 0.4216\n",
      "Epoch 839/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.2387 - acc: 0.9172 - val_loss: 9.1599 - val_acc: 0.4205\n",
      "Epoch 840/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2392 - acc: 0.9164 - val_loss: 9.1408 - val_acc: 0.4184\n",
      "Epoch 841/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2389 - acc: 0.9160 - val_loss: 9.1835 - val_acc: 0.4223\n",
      "Epoch 842/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2299 - acc: 0.9191 - val_loss: 9.1804 - val_acc: 0.4208\n",
      "Epoch 843/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.2349 - acc: 0.9172 - val_loss: 9.1756 - val_acc: 0.4225\n",
      "Epoch 844/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2331 - acc: 0.9185 - val_loss: 9.1648 - val_acc: 0.4223\n",
      "Epoch 845/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.2437 - acc: 0.9152 - val_loss: 9.1694 - val_acc: 0.4229\n",
      "Epoch 846/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2446 - acc: 0.9137 - val_loss: 9.1700 - val_acc: 0.4238\n",
      "Epoch 847/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2394 - acc: 0.9161 - val_loss: 9.1565 - val_acc: 0.4210\n",
      "Epoch 848/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2305 - acc: 0.9201 - val_loss: 9.1823 - val_acc: 0.4225\n",
      "Epoch 849/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2353 - acc: 0.9180 - val_loss: 9.1721 - val_acc: 0.4216\n",
      "Epoch 850/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2370 - acc: 0.9159 - val_loss: 9.1833 - val_acc: 0.4210\n",
      "Epoch 851/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2406 - acc: 0.9166 - val_loss: 9.1613 - val_acc: 0.4203\n",
      "Epoch 852/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.2329 - acc: 0.9188 - val_loss: 9.1526 - val_acc: 0.4212\n",
      "Epoch 853/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2332 - acc: 0.9194 - val_loss: 9.1868 - val_acc: 0.4245\n",
      "Epoch 854/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2334 - acc: 0.9189 - val_loss: 9.1842 - val_acc: 0.4214\n",
      "Epoch 855/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2327 - acc: 0.9193 - val_loss: 9.1736 - val_acc: 0.4223\n",
      "Epoch 856/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2333 - acc: 0.9187 - val_loss: 9.1553 - val_acc: 0.4212\n",
      "Epoch 857/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2338 - acc: 0.9176 - val_loss: 9.1912 - val_acc: 0.4247\n",
      "Epoch 858/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2311 - acc: 0.9187 - val_loss: 9.1920 - val_acc: 0.4240\n",
      "Epoch 859/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.2414 - acc: 0.9159 - val_loss: 9.1834 - val_acc: 0.4240\n",
      "Epoch 860/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2303 - acc: 0.9194 - val_loss: 9.1611 - val_acc: 0.4208\n",
      "Epoch 861/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2314 - acc: 0.9197 - val_loss: 9.1763 - val_acc: 0.4221\n",
      "Epoch 862/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2278 - acc: 0.9210 - val_loss: 9.1543 - val_acc: 0.4210\n",
      "Epoch 863/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.2370 - acc: 0.9167 - val_loss: 9.1633 - val_acc: 0.4221\n",
      "Epoch 864/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2292 - acc: 0.9203 - val_loss: 9.1677 - val_acc: 0.4223\n",
      "Epoch 865/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2319 - acc: 0.9177 - val_loss: 9.1546 - val_acc: 0.4210\n",
      "Epoch 866/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2345 - acc: 0.9173 - val_loss: 9.1681 - val_acc: 0.4234\n",
      "Epoch 867/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2349 - acc: 0.9190 - val_loss: 9.1645 - val_acc: 0.4225\n",
      "Epoch 868/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.2391 - acc: 0.9174 - val_loss: 9.1717 - val_acc: 0.4232\n",
      "Epoch 869/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2370 - acc: 0.9176 - val_loss: 9.1598 - val_acc: 0.4234\n",
      "Epoch 870/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2383 - acc: 0.9169 - val_loss: 9.1586 - val_acc: 0.4219\n",
      "Epoch 871/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2299 - acc: 0.9197 - val_loss: 9.1762 - val_acc: 0.4234\n",
      "Epoch 872/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2299 - acc: 0.9208 - val_loss: 9.1378 - val_acc: 0.4182\n",
      "Epoch 873/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2306 - acc: 0.9193 - val_loss: 9.1782 - val_acc: 0.4221\n",
      "Epoch 874/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2329 - acc: 0.9191 - val_loss: 9.1618 - val_acc: 0.4214\n",
      "Epoch 875/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2328 - acc: 0.9187 - val_loss: 9.1592 - val_acc: 0.4216\n",
      "Epoch 876/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2311 - acc: 0.9183 - val_loss: 9.1719 - val_acc: 0.4219\n",
      "Epoch 877/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2272 - acc: 0.9201 - val_loss: 9.1821 - val_acc: 0.4225\n",
      "Epoch 878/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2340 - acc: 0.9191 - val_loss: 9.1766 - val_acc: 0.4238\n",
      "Epoch 879/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2342 - acc: 0.9175 - val_loss: 9.1680 - val_acc: 0.4214\n",
      "Epoch 880/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2369 - acc: 0.9183 - val_loss: 9.1746 - val_acc: 0.4227\n",
      "Epoch 881/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2318 - acc: 0.9183 - val_loss: 9.1463 - val_acc: 0.4199\n",
      "Epoch 882/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2330 - acc: 0.9186 - val_loss: 9.1602 - val_acc: 0.4221\n",
      "Epoch 883/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2311 - acc: 0.9196 - val_loss: 9.1521 - val_acc: 0.4205\n",
      "Epoch 884/1000\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.2300 - acc: 0.9194 - val_loss: 9.1615 - val_acc: 0.4229\n",
      "Epoch 885/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2295 - acc: 0.9198 - val_loss: 9.1786 - val_acc: 0.4236\n",
      "Epoch 886/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2260 - acc: 0.9213 - val_loss: 9.1760 - val_acc: 0.4223\n",
      "Epoch 887/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2317 - acc: 0.9197 - val_loss: 9.1485 - val_acc: 0.4210\n",
      "Epoch 888/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2309 - acc: 0.9193 - val_loss: 9.1446 - val_acc: 0.4195\n",
      "Epoch 889/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2301 - acc: 0.9201 - val_loss: 9.1422 - val_acc: 0.4223\n",
      "Epoch 890/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2295 - acc: 0.9181 - val_loss: 9.1518 - val_acc: 0.4216\n",
      "Epoch 891/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2230 - acc: 0.9222 - val_loss: 9.1717 - val_acc: 0.4234\n",
      "Epoch 892/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2215 - acc: 0.9239 - val_loss: 9.1618 - val_acc: 0.4214\n",
      "Epoch 893/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2276 - acc: 0.9191 - val_loss: 9.1372 - val_acc: 0.4186\n",
      "Epoch 894/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2221 - acc: 0.9224 - val_loss: 9.1647 - val_acc: 0.4214\n",
      "Epoch 895/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2241 - acc: 0.9211 - val_loss: 9.1739 - val_acc: 0.4234\n",
      "Epoch 896/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2324 - acc: 0.9191 - val_loss: 9.1701 - val_acc: 0.4212\n",
      "Epoch 897/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.2268 - acc: 0.9206 - val_loss: 9.1031 - val_acc: 0.4171\n",
      "Epoch 898/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2323 - acc: 0.9192 - val_loss: 9.1570 - val_acc: 0.4186\n",
      "Epoch 899/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2334 - acc: 0.9192 - val_loss: 9.1306 - val_acc: 0.4186\n",
      "Epoch 900/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2281 - acc: 0.9190 - val_loss: 9.1388 - val_acc: 0.4177\n",
      "Epoch 901/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2292 - acc: 0.9205 - val_loss: 9.1549 - val_acc: 0.4168\n",
      "Epoch 902/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2287 - acc: 0.9201 - val_loss: 9.1656 - val_acc: 0.4212\n",
      "Epoch 903/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2374 - acc: 0.9173 - val_loss: 9.1181 - val_acc: 0.4177\n",
      "Epoch 904/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2256 - acc: 0.9199 - val_loss: 9.1695 - val_acc: 0.4192\n",
      "Epoch 905/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2251 - acc: 0.9226 - val_loss: 9.1638 - val_acc: 0.4190\n",
      "Epoch 906/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2244 - acc: 0.9224 - val_loss: 9.1762 - val_acc: 0.4225\n",
      "Epoch 907/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2265 - acc: 0.9207 - val_loss: 9.1579 - val_acc: 0.4190\n",
      "Epoch 908/1000\n",
      "41259/41259 [==============================] - 41s 985us/step - loss: 0.2309 - acc: 0.9190 - val_loss: 9.1831 - val_acc: 0.4234\n",
      "Epoch 909/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2233 - acc: 0.9224 - val_loss: 9.1515 - val_acc: 0.4192\n",
      "Epoch 910/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2287 - acc: 0.9214 - val_loss: 9.1591 - val_acc: 0.4216\n",
      "Epoch 911/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2229 - acc: 0.9225 - val_loss: 9.1465 - val_acc: 0.4192\n",
      "Epoch 912/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.2204 - acc: 0.9237 - val_loss: 9.1537 - val_acc: 0.4190\n",
      "Epoch 913/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2273 - acc: 0.9200 - val_loss: 9.1664 - val_acc: 0.4216\n",
      "Epoch 914/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2207 - acc: 0.9240 - val_loss: 9.1733 - val_acc: 0.4195\n",
      "Epoch 915/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2244 - acc: 0.9222 - val_loss: 9.1736 - val_acc: 0.4190\n",
      "Epoch 916/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2281 - acc: 0.9208 - val_loss: 9.1724 - val_acc: 0.4219\n",
      "Epoch 917/1000\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.2243 - acc: 0.9210 - val_loss: 9.1785 - val_acc: 0.4229\n",
      "Epoch 918/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2222 - acc: 0.9240 - val_loss: 9.1691 - val_acc: 0.4190\n",
      "Epoch 919/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2267 - acc: 0.9215 - val_loss: 9.1787 - val_acc: 0.4221\n",
      "Epoch 920/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2230 - acc: 0.9226 - val_loss: 9.1861 - val_acc: 0.4229\n",
      "Epoch 921/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2269 - acc: 0.9208 - val_loss: 9.1559 - val_acc: 0.4195\n",
      "Epoch 922/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2307 - acc: 0.9199 - val_loss: 9.1696 - val_acc: 0.4240\n",
      "Epoch 923/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2293 - acc: 0.9186 - val_loss: 9.1748 - val_acc: 0.4240\n",
      "Epoch 924/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2286 - acc: 0.9189 - val_loss: 9.1362 - val_acc: 0.4175\n",
      "Epoch 925/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2252 - acc: 0.9214 - val_loss: 9.1748 - val_acc: 0.4232\n",
      "Epoch 926/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2213 - acc: 0.9225 - val_loss: 9.1613 - val_acc: 0.4238\n",
      "Epoch 927/1000\n",
      "41259/41259 [==============================] - 40s 982us/step - loss: 0.2259 - acc: 0.9209 - val_loss: 9.1525 - val_acc: 0.4221\n",
      "Epoch 928/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2237 - acc: 0.9219 - val_loss: 9.1705 - val_acc: 0.4238\n",
      "Epoch 929/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2203 - acc: 0.9237 - val_loss: 9.1635 - val_acc: 0.4216\n",
      "Epoch 930/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2209 - acc: 0.9226 - val_loss: 9.1413 - val_acc: 0.4221\n",
      "Epoch 931/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2197 - acc: 0.9234 - val_loss: 9.1510 - val_acc: 0.4188\n",
      "Epoch 932/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2149 - acc: 0.9236 - val_loss: 9.1729 - val_acc: 0.4232\n",
      "Epoch 933/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2187 - acc: 0.9231 - val_loss: 9.1567 - val_acc: 0.4190\n",
      "Epoch 934/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2163 - acc: 0.9248 - val_loss: 9.1685 - val_acc: 0.4227\n",
      "Epoch 935/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2153 - acc: 0.9255 - val_loss: 9.1643 - val_acc: 0.4234\n",
      "Epoch 936/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2196 - acc: 0.9239 - val_loss: 9.1684 - val_acc: 0.4229\n",
      "Epoch 937/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2196 - acc: 0.9257 - val_loss: 9.1568 - val_acc: 0.4223\n",
      "Epoch 938/1000\n",
      "41259/41259 [==============================] - 40s 968us/step - loss: 0.2182 - acc: 0.9238 - val_loss: 9.1601 - val_acc: 0.4221\n",
      "Epoch 939/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2167 - acc: 0.9235 - val_loss: 9.1563 - val_acc: 0.4236\n",
      "Epoch 940/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2172 - acc: 0.9236 - val_loss: 9.1550 - val_acc: 0.4195\n",
      "Epoch 941/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2197 - acc: 0.9232 - val_loss: 9.1690 - val_acc: 0.4236\n",
      "Epoch 942/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2171 - acc: 0.9230 - val_loss: 9.1695 - val_acc: 0.4238\n",
      "Epoch 943/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2233 - acc: 0.9228 - val_loss: 9.1517 - val_acc: 0.4197\n",
      "Epoch 944/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2214 - acc: 0.9223 - val_loss: 9.1606 - val_acc: 0.4210\n",
      "Epoch 945/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2163 - acc: 0.9251 - val_loss: 9.1620 - val_acc: 0.4201\n",
      "Epoch 946/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2241 - acc: 0.9215 - val_loss: 9.1707 - val_acc: 0.4238\n",
      "Epoch 947/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2283 - acc: 0.9208 - val_loss: 9.1788 - val_acc: 0.4232\n",
      "Epoch 948/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2181 - acc: 0.9244 - val_loss: 9.1539 - val_acc: 0.4205\n",
      "Epoch 949/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2151 - acc: 0.9254 - val_loss: 9.1541 - val_acc: 0.4227\n",
      "Epoch 950/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2107 - acc: 0.9255 - val_loss: 9.1654 - val_acc: 0.4245\n",
      "Epoch 951/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2149 - acc: 0.9243 - val_loss: 9.1573 - val_acc: 0.4247\n",
      "Epoch 952/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2213 - acc: 0.9216 - val_loss: 9.1479 - val_acc: 0.4227\n",
      "Epoch 953/1000\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.2190 - acc: 0.9251 - val_loss: 9.1090 - val_acc: 0.4197\n",
      "Epoch 954/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2159 - acc: 0.9246 - val_loss: 9.1436 - val_acc: 0.4214\n",
      "Epoch 955/1000\n",
      "41259/41259 [==============================] - 41s 986us/step - loss: 0.2140 - acc: 0.9265 - val_loss: 9.1384 - val_acc: 0.4223\n",
      "Epoch 956/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2066 - acc: 0.9270 - val_loss: 9.1587 - val_acc: 0.4232\n",
      "Epoch 957/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2143 - acc: 0.9258 - val_loss: 9.1381 - val_acc: 0.4221\n",
      "Epoch 958/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2160 - acc: 0.9247 - val_loss: 9.1451 - val_acc: 0.4212\n",
      "Epoch 959/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2120 - acc: 0.9269 - val_loss: 9.1658 - val_acc: 0.4227\n",
      "Epoch 960/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2221 - acc: 0.9212 - val_loss: 9.1529 - val_acc: 0.4227\n",
      "Epoch 961/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2120 - acc: 0.9256 - val_loss: 9.1475 - val_acc: 0.4223\n",
      "Epoch 962/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2117 - acc: 0.9267 - val_loss: 9.1744 - val_acc: 0.4234\n",
      "Epoch 963/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2069 - acc: 0.9279 - val_loss: 9.1766 - val_acc: 0.4258\n",
      "Epoch 964/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2097 - acc: 0.9271 - val_loss: 9.1680 - val_acc: 0.4234\n",
      "Epoch 965/1000\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.2180 - acc: 0.9229 - val_loss: 9.1289 - val_acc: 0.4223\n",
      "Epoch 966/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2167 - acc: 0.9244 - val_loss: 9.1758 - val_acc: 0.4240\n",
      "Epoch 967/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2165 - acc: 0.9244 - val_loss: 9.1617 - val_acc: 0.4221\n",
      "Epoch 968/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2120 - acc: 0.9267 - val_loss: 9.1718 - val_acc: 0.4234\n",
      "Epoch 969/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2195 - acc: 0.9234 - val_loss: 9.1437 - val_acc: 0.4221\n",
      "Epoch 970/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2169 - acc: 0.9255 - val_loss: 9.1464 - val_acc: 0.4234\n",
      "Epoch 971/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2069 - acc: 0.9289 - val_loss: 9.1400 - val_acc: 0.4205\n",
      "Epoch 972/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2054 - acc: 0.9291 - val_loss: 9.1381 - val_acc: 0.4219\n",
      "Epoch 973/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2144 - acc: 0.9276 - val_loss: 9.1540 - val_acc: 0.4223\n",
      "Epoch 974/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2084 - acc: 0.9270 - val_loss: 9.1562 - val_acc: 0.4227\n",
      "Epoch 975/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2129 - acc: 0.9258 - val_loss: 9.1223 - val_acc: 0.4197\n",
      "Epoch 976/1000\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.2137 - acc: 0.9255 - val_loss: 9.1419 - val_acc: 0.4203\n",
      "Epoch 977/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2048 - acc: 0.9289 - val_loss: 9.1543 - val_acc: 0.4229\n",
      "Epoch 978/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2044 - acc: 0.9284 - val_loss: 9.1698 - val_acc: 0.4240\n",
      "Epoch 979/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2009 - acc: 0.9312 - val_loss: 9.1564 - val_acc: 0.4221\n",
      "Epoch 980/1000\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.2137 - acc: 0.9255 - val_loss: 9.1506 - val_acc: 0.4219\n",
      "Epoch 981/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2075 - acc: 0.9288 - val_loss: 9.1540 - val_acc: 0.4195\n",
      "Epoch 982/1000\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.2035 - acc: 0.9287 - val_loss: 9.1445 - val_acc: 0.4214\n",
      "Epoch 983/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2067 - acc: 0.9273 - val_loss: 9.1509 - val_acc: 0.4223\n",
      "Epoch 984/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2113 - acc: 0.9259 - val_loss: 9.1546 - val_acc: 0.4238\n",
      "Epoch 985/1000\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.2097 - acc: 0.9261 - val_loss: 9.1684 - val_acc: 0.4229\n",
      "Epoch 986/1000\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.2064 - acc: 0.9289 - val_loss: 9.1494 - val_acc: 0.4210\n",
      "Epoch 987/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2104 - acc: 0.9271 - val_loss: 9.1660 - val_acc: 0.4225\n",
      "Epoch 988/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2051 - acc: 0.9302 - val_loss: 9.1717 - val_acc: 0.4223\n",
      "Epoch 989/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.1972 - acc: 0.9315 - val_loss: 9.1639 - val_acc: 0.4203\n",
      "Epoch 990/1000\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.2042 - acc: 0.9298 - val_loss: 9.1298 - val_acc: 0.4160\n",
      "Epoch 991/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2098 - acc: 0.9272 - val_loss: 9.1674 - val_acc: 0.4221\n",
      "Epoch 992/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.2020 - acc: 0.9296 - val_loss: 9.1716 - val_acc: 0.4227\n",
      "Epoch 993/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.2078 - acc: 0.9278 - val_loss: 9.1451 - val_acc: 0.4219\n",
      "Epoch 994/1000\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.2053 - acc: 0.9279 - val_loss: 9.1525 - val_acc: 0.4227\n",
      "Epoch 995/1000\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.2094 - acc: 0.9269 - val_loss: 9.1445 - val_acc: 0.4219\n",
      "Epoch 996/1000\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.1976 - acc: 0.9318 - val_loss: 9.1704 - val_acc: 0.4232\n",
      "Epoch 997/1000\n",
      "41259/41259 [==============================] - 41s 989us/step - loss: 0.2028 - acc: 0.9304 - val_loss: 9.1592 - val_acc: 0.4225\n",
      "Epoch 998/1000\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.1972 - acc: 0.9329 - val_loss: 9.1635 - val_acc: 0.4223\n",
      "Epoch 999/1000\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.2073 - acc: 0.9294 - val_loss: 9.1384 - val_acc: 0.4214\n",
      "Epoch 1000/1000\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.2096 - acc: 0.9262 - val_loss: 9.1544 - val_acc: 0.4208\n"
     ]
    }
   ],
   "source": [
    "history2 = model_.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21106857278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('model_sgd01.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41259 samples, validate on 4594 samples\n",
      "Epoch 1/400\n",
      "41259/41259 [==============================] - 40s 972us/step - loss: 0.1365 - acc: 0.9552 - val_loss: 9.1999 - val_acc: 0.4240\n",
      "Epoch 2/400\n",
      "41259/41259 [==============================] - 40s 974us/step - loss: 0.1402 - acc: 0.9534 - val_loss: 9.2008 - val_acc: 0.4232\n",
      "Epoch 3/400\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.1279 - acc: 0.9565 - val_loss: 9.1978 - val_acc: 0.4240\n",
      "Epoch 4/400\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.1324 - acc: 0.9555 - val_loss: 9.2059 - val_acc: 0.4236\n",
      "Epoch 5/400\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.1409 - acc: 0.9522 - val_loss: 9.1936 - val_acc: 0.4227\n",
      "Epoch 6/400\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.1361 - acc: 0.9544 - val_loss: 9.1929 - val_acc: 0.4249\n",
      "Epoch 7/400\n",
      "41259/41259 [==============================] - 40s 963us/step - loss: 0.1294 - acc: 0.9561 - val_loss: 9.1903 - val_acc: 0.4245\n",
      "Epoch 8/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1290 - acc: 0.9562 - val_loss: 9.1936 - val_acc: 0.4240\n",
      "Epoch 9/400\n",
      "41259/41259 [==============================] - 40s 968us/step - loss: 0.1412 - acc: 0.9528 - val_loss: 9.1957 - val_acc: 0.4229\n",
      "Epoch 10/400\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.1335 - acc: 0.9555 - val_loss: 9.1986 - val_acc: 0.4249\n",
      "Epoch 11/400\n",
      "41259/41259 [==============================] - 40s 968us/step - loss: 0.1241 - acc: 0.9580 - val_loss: 9.2039 - val_acc: 0.4245\n",
      "Epoch 12/400\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.1309 - acc: 0.9564 - val_loss: 9.1997 - val_acc: 0.4247\n",
      "Epoch 13/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1391 - acc: 0.9531 - val_loss: 9.1994 - val_acc: 0.4242\n",
      "Epoch 14/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1341 - acc: 0.9549 - val_loss: 9.2011 - val_acc: 0.4236\n",
      "Epoch 15/400\n",
      "41259/41259 [==============================] - 40s 963us/step - loss: 0.1341 - acc: 0.9544 - val_loss: 9.2076 - val_acc: 0.4236\n",
      "Epoch 16/400\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.1311 - acc: 0.9559 - val_loss: 9.2092 - val_acc: 0.4234\n",
      "Epoch 17/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1385 - acc: 0.9541 - val_loss: 9.2077 - val_acc: 0.4253\n",
      "Epoch 18/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1248 - acc: 0.9582 - val_loss: 9.2149 - val_acc: 0.4232\n",
      "Epoch 19/400\n",
      "41259/41259 [==============================] - 40s 967us/step - loss: 0.1267 - acc: 0.9575 - val_loss: 9.2089 - val_acc: 0.4240\n",
      "Epoch 20/400\n",
      "41259/41259 [==============================] - 40s 965us/step - loss: 0.1395 - acc: 0.9523 - val_loss: 9.2037 - val_acc: 0.4242\n",
      "Epoch 21/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1317 - acc: 0.9565 - val_loss: 9.2043 - val_acc: 0.4245\n",
      "Epoch 22/400\n",
      "41259/41259 [==============================] - 40s 959us/step - loss: 0.1271 - acc: 0.9578 - val_loss: 9.2030 - val_acc: 0.4247\n",
      "Epoch 23/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1338 - acc: 0.9556 - val_loss: 9.2015 - val_acc: 0.4236\n",
      "Epoch 24/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1264 - acc: 0.9577 - val_loss: 9.2013 - val_acc: 0.4236\n",
      "Epoch 25/400\n",
      "41259/41259 [==============================] - 40s 961us/step - loss: 0.1320 - acc: 0.9539 - val_loss: 9.1921 - val_acc: 0.4242\n",
      "Epoch 26/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1336 - acc: 0.9545 - val_loss: 9.1998 - val_acc: 0.4249\n",
      "Epoch 27/400\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.1300 - acc: 0.9565 - val_loss: 9.1963 - val_acc: 0.4245\n",
      "Epoch 28/400\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.1359 - acc: 0.9533 - val_loss: 9.2033 - val_acc: 0.4242\n",
      "Epoch 29/400\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.1374 - acc: 0.9554 - val_loss: 9.1990 - val_acc: 0.4260\n",
      "Epoch 30/400\n",
      "41259/41259 [==============================] - 40s 962us/step - loss: 0.1261 - acc: 0.9567 - val_loss: 9.1907 - val_acc: 0.4245\n",
      "Epoch 31/400\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.1301 - acc: 0.9556 - val_loss: 9.2012 - val_acc: 0.4249\n",
      "Epoch 32/400\n",
      "41259/41259 [==============================] - 40s 970us/step - loss: 0.1266 - acc: 0.9578 - val_loss: 9.2043 - val_acc: 0.4242\n",
      "Epoch 33/400\n",
      "41259/41259 [==============================] - 40s 969us/step - loss: 0.1301 - acc: 0.9563 - val_loss: 9.1964 - val_acc: 0.4238\n",
      "Epoch 34/400\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.1328 - acc: 0.9557 - val_loss: 9.1915 - val_acc: 0.4238\n",
      "Epoch 35/400\n",
      "41259/41259 [==============================] - 40s 973us/step - loss: 0.1266 - acc: 0.9569 - val_loss: 9.1954 - val_acc: 0.4240\n",
      "Epoch 36/400\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.1376 - acc: 0.9542 - val_loss: 9.1987 - val_acc: 0.4249\n",
      "Epoch 37/400\n",
      "41259/41259 [==============================] - 51s 1ms/step - loss: 0.1296 - acc: 0.9557 - val_loss: 9.2090 - val_acc: 0.4236\n",
      "Epoch 38/400\n",
      "41259/41259 [==============================] - 40s 971us/step - loss: 0.1424 - acc: 0.9530 - val_loss: 9.1911 - val_acc: 0.4247\n",
      "Epoch 39/400\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 0.1268 - acc: 0.9567 - val_loss: 9.2031 - val_acc: 0.4234\n",
      "Epoch 40/400\n",
      "41259/41259 [==============================] - 41s 998us/step - loss: 0.1318 - acc: 0.9556 - val_loss: 9.2071 - val_acc: 0.4234\n",
      "Epoch 41/400\n",
      "41259/41259 [==============================] - 41s 998us/step - loss: 0.1275 - acc: 0.9574 - val_loss: 9.2056 - val_acc: 0.4240\n",
      "Epoch 42/400\n",
      "41259/41259 [==============================] - 41s 997us/step - loss: 0.1337 - acc: 0.9547 - val_loss: 9.2026 - val_acc: 0.4240\n",
      "Epoch 43/400\n",
      "41259/41259 [==============================] - 41s 993us/step - loss: 0.1292 - acc: 0.9557 - val_loss: 9.1915 - val_acc: 0.4247\n",
      "Epoch 44/400\n",
      "41259/41259 [==============================] - 41s 999us/step - loss: 0.1242 - acc: 0.9581 - val_loss: 9.2040 - val_acc: 0.4247\n",
      "Epoch 45/400\n",
      "41259/41259 [==============================] - 41s 997us/step - loss: 0.1313 - acc: 0.9547 - val_loss: 9.1976 - val_acc: 0.4229\n",
      "Epoch 46/400\n",
      "41259/41259 [==============================] - 41s 1000us/step - loss: 0.1242 - acc: 0.9587 - val_loss: 9.2016 - val_acc: 0.4242\n",
      "Epoch 47/400\n",
      "41259/41259 [==============================] - 41s 997us/step - loss: 0.1296 - acc: 0.9572 - val_loss: 9.2040 - val_acc: 0.4245\n",
      "Epoch 48/400\n",
      "41259/41259 [==============================] - 41s 988us/step - loss: 0.1296 - acc: 0.9573 - val_loss: 9.2079 - val_acc: 0.4247\n",
      "Epoch 49/400\n",
      "41259/41259 [==============================] - 41s 996us/step - loss: 0.1270 - acc: 0.9578 - val_loss: 9.2129 - val_acc: 0.4236\n",
      "Epoch 50/400\n",
      "41259/41259 [==============================] - 41s 999us/step - loss: 0.1377 - acc: 0.9530 - val_loss: 9.2068 - val_acc: 0.4229\n",
      "Epoch 51/400\n",
      "41259/41259 [==============================] - 41s 997us/step - loss: 0.1317 - acc: 0.9555 - val_loss: 9.2067 - val_acc: 0.4242\n",
      "Epoch 52/400\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.1271 - acc: 0.9561 - val_loss: 9.2073 - val_acc: 0.4251\n",
      "Epoch 53/400\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.1247 - acc: 0.9579 - val_loss: 9.2029 - val_acc: 0.4247\n",
      "Epoch 54/400\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.1309 - acc: 0.9553 - val_loss: 9.2134 - val_acc: 0.4245\n",
      "Epoch 55/400\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.1302 - acc: 0.9570 - val_loss: 9.2082 - val_acc: 0.4242\n",
      "Epoch 56/400\n",
      "41259/41259 [==============================] - 41s 984us/step - loss: 0.1334 - acc: 0.9548 - val_loss: 9.2074 - val_acc: 0.4240\n",
      "Epoch 57/400\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.1364 - acc: 0.9540 - val_loss: 9.2061 - val_acc: 0.4245\n",
      "Epoch 58/400\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.1285 - acc: 0.9557 - val_loss: 9.2081 - val_acc: 0.4240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/400\n",
      "41259/41259 [==============================] - 40s 980us/step - loss: 0.1265 - acc: 0.9566 - val_loss: 9.2082 - val_acc: 0.4249\n",
      "Epoch 60/400\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.1279 - acc: 0.9576 - val_loss: 9.2099 - val_acc: 0.4240\n",
      "Epoch 61/400\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.1219 - acc: 0.9592 - val_loss: 9.2126 - val_acc: 0.4242\n",
      "Epoch 62/400\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.1295 - acc: 0.9562 - val_loss: 9.2095 - val_acc: 0.4240\n",
      "Epoch 63/400\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.1289 - acc: 0.9563 - val_loss: 9.2073 - val_acc: 0.4242\n",
      "Epoch 64/400\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.1291 - acc: 0.9575 - val_loss: 9.2015 - val_acc: 0.4242\n",
      "Epoch 65/400\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.1237 - acc: 0.9584 - val_loss: 9.2039 - val_acc: 0.4242\n",
      "Epoch 66/400\n",
      "41259/41259 [==============================] - 40s 978us/step - loss: 0.1260 - acc: 0.9583 - val_loss: 9.2075 - val_acc: 0.4247\n",
      "Epoch 67/400\n",
      "41259/41259 [==============================] - 40s 976us/step - loss: 0.1290 - acc: 0.9559 - val_loss: 9.2045 - val_acc: 0.4258\n",
      "Epoch 68/400\n",
      "41259/41259 [==============================] - 40s 979us/step - loss: 0.1333 - acc: 0.9560 - val_loss: 9.2008 - val_acc: 0.4236\n",
      "Epoch 69/400\n",
      "41259/41259 [==============================] - 41s 982us/step - loss: 0.1272 - acc: 0.9571 - val_loss: 9.2027 - val_acc: 0.4247\n",
      "Epoch 70/400\n",
      "41259/41259 [==============================] - 40s 977us/step - loss: 0.1251 - acc: 0.9584 - val_loss: 9.2109 - val_acc: 0.4245\n",
      "Epoch 71/400\n",
      "41259/41259 [==============================] - 40s 975us/step - loss: 0.1322 - acc: 0.9557 - val_loss: 9.2005 - val_acc: 0.4247\n",
      "Epoch 72/400\n",
      "41259/41259 [==============================] - 40s 981us/step - loss: 0.1337 - acc: 0.9558 - val_loss: 9.1966 - val_acc: 0.4247\n",
      "Epoch 73/400\n",
      "41259/41259 [==============================] - 41s 983us/step - loss: 0.1370 - acc: 0.9544 - val_loss: 9.1845 - val_acc: 0.4238\n",
      "Epoch 74/400\n",
      "27936/41259 [===================>..........] - ETA: 12s - loss: 0.1291 - acc: 0.9564"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-1431d78fc831>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2719\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2721\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2693\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history3 = model_.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred1 = model_.predict(X_train)\n",
    "pred2 = model_.predict(X_test)\n",
    "\n",
    "y_train_pred = pred1.argmax(axis=-1)\n",
    "y_test_pred = pred2.argmax(axis=-1)\n",
    "\n",
    "m1 = confusion_matrix(y_test.argmax(axis=-1), y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = confusion_matrix(y_train.argmax(axis=-1), y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17865</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8497</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3967</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2746</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2299</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1082</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>554</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2     3     4     5    6    7    8     9\n",
       "0  17865    46     7     2     0     0    0    0    0     0\n",
       "1      0  8497    38     1     1     0    0    0    1     0\n",
       "2      0     0  3967     3     0     0    0    0    0     0\n",
       "3      0     0     0  2746     1     0    0    1    2     0\n",
       "4      0     0     0     7  2299     1    0    0    0     0\n",
       "5      0     0     0     1     0  1082    0    1    3     0\n",
       "6      0     0     0    12     0     0  936    0    7     0\n",
       "7      0     0     0     1     1     7    1  554   16     1\n",
       "8      0     0     0     0     0     0    0    4  240     0\n",
       "9      0     0     0     0    13     1   13   14    0  2866"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1933</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>906</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>463</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1  2  3  4  5  6  7  8  9\n",
       "0  1933  33  0  0  0  0  0  0  0  0\n",
       "1   906   8  0  0  0  0  0  0  0  0\n",
       "2   463   3  0  0  0  0  0  0  0  0\n",
       "3   304   3  0  0  0  0  0  0  0  0\n",
       "4   268   3  0  0  0  0  0  0  0  0\n",
       "5   112   0  0  0  0  0  0  0  0  0\n",
       "6   122   1  0  0  0  0  0  0  0  0\n",
       "7    73   0  0  0  0  0  0  0  0  0\n",
       "8    28   0  0  0  0  0  0  0  0  0\n",
       "9   334   0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train.argmax(axis=-1)), y_train.argmax(axis=-1))\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.23023995535714287,\n",
       " 1: 0.48323963457484187,\n",
       " 2: 1.0392695214105794,\n",
       " 3: 1.5003272727272727,\n",
       " 4: 1.7884265279583875,\n",
       " 5: 3.795676172953082,\n",
       " 6: 4.3203141361256545,\n",
       " 7: 7.101376936316695,\n",
       " 8: 16.909426229508195,\n",
       " 9: 1.419298245614035}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_.save('model'+timestr+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optioncode u xceptiona sokup X_trainle uyusmasina bak//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = train_codes[317]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "speci_train = X_train[317,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token='0DQLcsXIojcAAAAAAAgybeB3DOmXKK7bRTLyYwkthbrAiGmpQR4AuGINQkjBXhif'\n",
    "dbx = dropbox.Dropbox(access_token)\n",
    "\n",
    "retrieved_image_df = pd.read_excel('C:/Users/Recep/Koton/Data/retrieved_image_list.xlsx', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "    bn_model = Xception(include_top=False, weights='imagenet', pooling='max')\n",
    "              \n",
    "            \n",
    "    im_index = retrieved_image_df.loc[code.upper()]['image1'+'_index']\n",
    "    if (math.isnan(float(im_index))==False):\n",
    "        filename = str(code.lower()) +'_'+'image1'+'_'+str(int(im_index))+'.jpg'\n",
    "        path = '/Koton_Image_Files/'+filename\n",
    "        \n",
    "        md, res = dbx.files_download(path)\n",
    "        temp_pic = cv2.cvtColor(cv2.resize(cv2.imdecode(np.frombuffer(res.content, dtype=np.uint8), -1), (299,299)), cv2.COLOR_BGR2RGB)\n",
    "        valid_pic1 = preprocess_input(np.array(temp_pic))\n",
    "        speci_y2 = bn_model.predict(valid_pic1.reshape(1, 299, 299, 3))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1920929e-07"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(speci_y2-speci_train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(speci_y-speci_y2).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
